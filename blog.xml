<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Lab Notebook - R</title>
 <link href="/blog.xml" rel="self"/>
 <link href="/"/>
 <updated>2014-10-14T21:51:46+00:00</updated>
 <id>http://www.carlboettiger.info</id>
 <author>
   <name>Carl Boettiger</name>
   <email>cboettig@gmail.com</email>
 </author>

 
 <entry>
   <title>Response To Software Discovery Index Report</title>
   <link href="/2014/10/08/response-to-software-discovery-index-report.html"/>
   <updated>2014-10-08T00:00:00+00:00</updated>
   <id>/2014/10/08/response-to-software-discovery-index-report</id>
   <content type="html">&lt;p&gt;The NIH has recently announced the &lt;a href=&quot;http://softwarediscoveryindex.org/report&quot;&gt;report&lt;/a&gt; of a landmark meeting which presents a vision for a &lt;em&gt;Software Discovery Index&lt;/em&gt; (SDI). The report is both timely and focused on the key issues of locating, citing, reusing software:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Software developers face challenges disseminating their software and measuring its adoption. Software users have difficulty identifying the most appropriate software for their work. Journal publishers lack a consistent way to handle software citations or to ensure reproducibility of published findings. Funding agencies struggle to make informed funding decisions about which software projects to support, while reviewers have a hard time understanding the relevancy and effectiveness of proposed software in the context of data management plans and proposed analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To address this, they propose an Index which would do three things:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;to assign standard and unambiguous identifiers to reference all software,&lt;/li&gt;
&lt;li&gt;to track specific metadata features that describe that software, and&lt;/li&gt;
&lt;li&gt;to enable robust querying of all relevant information for users.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The report is both timely and focused on key issues confronting our community, including the challenges of identifying, citing, and reusing software. The appendices do an excellent job in outlining key metadata, metrics, and use cases which help frame the discussion. The proposal does well to focus on the importance of identifiers and the creation of a query-able metadata index for research software, but leaves out an essential element necessary to make this work.&lt;/p&gt;
&lt;p&gt;This proposal sounds very much like the CrossRef and DataCite infrastructure already in place for academic literature and data, respectively; and indeed this is an excellent model to follow. However, a key piece of that infrastructure is missing from the present proposal – the social contract between repository or publisher and the index itself.&lt;/p&gt;
&lt;p&gt;CrossRef provides unique identifiers for the academic literature (CrossRef DOIs), but it also defines specific metadata that describe that literature (as well as metrics of its use), and embed that information into a robust, query-able, machine-readable format. DataCite does the same for scientific data. These are exactly the features that the authors of the report seek to emulate.&lt;/p&gt;
&lt;p&gt;Just as CrossRef itself does not host academic papers but only the metadata records, the SDI does not propose to host software itself. This introduces a substantial challenge in &lt;em&gt;maintaining the link&lt;/em&gt; between the metadata and the software itself. The authors have simply proposed that the metadata include “Links to the code repository.” If CrossRef or DataCite DOIs worked in this way, we would soon loose all ability to recover many of the papers or the data itself, and we would be left with only access to the metadata record and a broken link. DOIs were created explicitly to solve this problem, not through technology, but through a &lt;em&gt;social contract&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The scientific publishers who host the actual publications are responsible for ensuring that this link is always maintained when they change names, etc. Should the publisher go out of business, these links may be adjusted to point to a new home, such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;. This guarantees that the DOI always resolves to the resource in question, regardless of where it moves. Should a publisher fail to maintain these links, CrossRef may refuse to provide the publisher any additional DOIs, cutting it off from this key index. This is the social contract. Data repositories work in exactly the same way, purchasing their DOIs from DataCite. (While financial transaction isn’t strictly necessary for the financial contract, it provides a clear business model for maintaining the key organization responsible for the index).&lt;/p&gt;
&lt;p&gt;Without such a mechanism, links in the SDI would surely rot away, all the more rapidly in the fast-moving world of software. Without links to the software itself, the function of the index would be purely academic. Yet such a mechanism requires that the software repositories, not the individual software authors, would be willing to accept the same social contract, receiving (and possibly paying for) identifiers on the condition that they assume the responsibility of maintaining the links. It is unclear that the primary software repositories in use to day (Sourceforge, Github, Bitbucket, etc) would be willing to accept this.&lt;/p&gt;
&lt;p&gt;Data repositories already offer many of the compelling features of this proposal. Many data repositories accept a wide array of file formats including software packages, and would provide such software with a permanent unique identifier in the form of a DataCite DOI, as well as collecting much of the essential metadata listed in report’s Appendix 1, which would then already be accessible through the DataCite API in a nice machine-readable format. This strategy finds several aspects wanting.&lt;/p&gt;
&lt;p&gt;The primary barrier to using data repositories indexed by DataCite arises from the dynamic nature of software relative to data. Data repositories are designed to serve relatively static content with few versions. Software repositories, by contrast, are usually built upon explicit version control platforms such as Git or Subversion designed explicitly for handling continual changes, including branches and mergers, of software code. The report discusses the challenges of software versions as a reason for that citing a software paper as a proxy for citing software is not ideal: the citation to the paper does not convey what version was used. Rapid versioning creates other problems though, both in the number of identifiers that might be created (is each commit a new identifier?) and defining the relationship between different versions of the same software. Branches and merges exacerbate this problem. Existing approaches that provide the user a one-time way to import software from a software repository to a data repository such as those cited in the report (“One significant initiative is a collaboration between Mozilla, figshare, GitHub, and Zenodo”) do nothing to address this issues.&lt;/p&gt;
&lt;p&gt;Less challenging issues involve resolving differences between DataCite metadata and the proposed metadata records for software. Most obviously, the metadata would need a way to declare the object involved software instead of data per se, which would thus allow queries to restrict results to ‘software’ objects to avoid cluttering searches. Ideally, one would also create tools that can import such metadata from the format in which it is usually already defined in software, into the desired format of the index, rather than requiring manual double-entry of this information. These are important but more straight-forward problems which the report already seeks to address.&lt;/p&gt;
&lt;hr /&gt;
&lt;!-- comments
Ilya raises the question: Why not just use Github? I think it is important to note that:

a) Github isn't forever, repositories come and go all the time, or move to new links, etc

b) Re-creating and running an NIH-Github would be both expensive (Gitlab notwithstanding) and redundant -- researchers would continue to use Github etc.

c) Github provides somewhat limited query-able metadata, that doesn't capture even the minimal list of fields suggested by the report.
Leveraging existing scientific data repositories by linking them to versioned releases on software repositories addresses each of these problems.

--&gt;


&lt;!--
A. FRAMEWORK SUPPORTING THE SOFTWARE DISCOVERY INDEX
Unique identifiers
Connections to publishers
Use cases
Complementarity with the Data Discovery Index
B. CHALLENGES AND REMAINING QUESTIONS
Defining relevant software
Integrating with other repositories
Evaluating progress and distinguishing this from other efforts
C. IMPLEMENTATION ROADMAP
--&gt;


</content>
 </entry>
 
 <entry>
   <title>Docker Notes</title>
   <link href="/2014/08/14/docker-notes.html"/>
   <updated>2014-08-14T00:00:00+00:00</updated>
   <id>/2014/08/14/docker-notes</id>
   <content type="html">&lt;p&gt;Ticking through a few more of the challenges I raised in my &lt;a href=&quot;http://www.carlboettiger.info/2014/08/07/too-much-fun-with-docker.html&quot;&gt;first post on docker&lt;/a&gt;; here I explore some of the issues about facilitating interaction with a docker container so that a user’s experience is more similar to working in their own environment and less like working on a remote terminal over ssh. While technically minor, these issues are probably the first stumbling blocks in making this a valid platform for new users.&lt;/p&gt;
&lt;h2 id=&quot;sharing-a-local-directory&quot;&gt;Sharing a local directory&lt;/h2&gt;
&lt;p&gt;Launch a bash shell on the container that shares the current working directory of the host machine (from &lt;code&gt;pwd&lt;/code&gt;) with the &lt;code&gt;/host&lt;/code&gt; directory on the container (thanks to Dirk for this solution):&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;:/host cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows a user to move files on and off the container, use a familiar editor and even handle things like git commits / pulls / pushes in their working directory as before. Then the code can be executed in the containerized environment which handles all the dependencies. From the terminal docker opens, we just &lt;code&gt;cd /host&lt;/code&gt; where we find our working directory files, and can launch R and run the scripts. A rather clean way of maintaining the local development experience but containerizing execution.&lt;/p&gt;
&lt;p&gt;In particular, this frees us from having to pass our git credentials etc to the container, though is not so useful if we’re wanting to interact with the container via the RStudio server instead of R running in the terminal. (More on getting around this below).&lt;/p&gt;
&lt;p&gt;Unfortunately, Mac and Windows users have to run Docker inside an already-virualized environment such as provided by &lt;code&gt;boot2docker&lt;/code&gt; or &lt;code&gt;vagrant&lt;/code&gt;. This means that it is only the directories on the virtualized environment, not those on the native OS, can be shared in this way. While one could presumably keep a directory synced between this virtual environment and the native OS, (standard in in &lt;code&gt;vagrant&lt;/code&gt;), this is a problem for the easier-to-use &lt;code&gt;boot2docker&lt;/code&gt; at this time: (&lt;a href=&quot;https://github.com/docker/docker/issues/7249&quot;&gt;docker/issues/7249&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;a-docker-desktop&quot;&gt;A Docker Desktop&lt;/h2&gt;
&lt;p&gt;Dirk brought this &lt;a href=&quot;http://blog.docker.com/2013/07/docker-desktop-your-desktop-over-ssh-running-inside-of-a-docker-container&quot;&gt;docker-desktop&lt;/a&gt; to my attention; which uses Xpra (in place of X11 forwarding) to provide a window with fluxbox running on Ubuntu along with common applications like libreoffce, firefox, and rox file manager. Pretty clever, and worked just fine for me, but needs Xpra on the client machine and requires some extra steps (run the container, query for passwords and ports, run ssh to connect, then run Xpra to launch the window). The result is reasonably responsive but still slower than virtualbox, and probably too slow for real work.&lt;/p&gt;
&lt;h2 id=&quot;base-images&quot;&gt;Base images?&lt;/h2&gt;
&lt;p&gt;The basic Ubuntu:14.04 seems like a good lightweight base image (at 192 MB), but other images try to give more useful building blocks, like &lt;a href=&quot;https://github.com/phusion/baseimage-docker#contents&quot;&gt;phusion/baseimage&lt;/a&gt; (423 MB). Their &lt;code&gt;docker-bash&lt;/code&gt; script and other utilities provide some handy features for managing / debugging containers.&lt;/p&gt;
&lt;h2 id=&quot;other-ways-to-share-files&quot;&gt;Other ways to share files?&lt;/h2&gt;
&lt;p&gt;Took a quick look at this &lt;a href=&quot;https://github.com/gfjardim/docker-dropbox/blob/master/Dockerfile&quot;&gt;Dockerfile for running dropbox&lt;/a&gt;, which works rather well (at least on a linux machine, since it requires local directory sharing). Could probably be done without explicit linking to local directories to faciliate moving files on and off the container. Of course one can always scp/rsync files on and off containers if ssh is set up, but that is unlikely to be a popular solution for students.&lt;/p&gt;
&lt;p&gt;While we have rstudio server running nicely in a Docker container for local or cloud use, it’s still an issue getting Github ssh keys set up to be able to push changes to a repo. We can get around this by linking to our keys directory with the same &lt;code&gt;-v&lt;/code&gt; option shown above. We still need a few more steps: setting the Git username and email, and running &lt;code&gt;ssh-add&lt;/code&gt; for the key. Presumably we could do this with environmental variables and some adjustment to the Dockerfile:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v /path/to/keys:/home/rstudio/.ssh/ -e &lt;span class=&quot;st&quot;&gt;&amp;quot;USERNAME=Carl Boettiger&amp;quot;&lt;/span&gt; -e &lt;span class=&quot;st&quot;&gt;&amp;quot;EMAIL=cboettig@example.org&amp;quot;&lt;/span&gt; cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would prevent storing these secure values on the image itself.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An appropriate amount of fun with docker?</title>
   <link href="/2014/08/08/an-appropriate-amount-of-fun-with-docker.html"/>
   <updated>2014-08-08T00:00:00+00:00</updated>
   <id>/2014/08/08/an-appropriate-amount-of-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;em&gt;An update on my exploration with Docker. Title courtesy of &lt;a href=&quot;https://twitter.com/DistribEcology/status/497523435371638784&quot;&gt;Ted&lt;/a&gt;, with my hopes that this really does move us in a direction where we can spend less time thinking about the tools and computational environments. Not there yet though&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I’ve gotten RStudio Server working in the &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;ropensci-docker&lt;/a&gt; image (Issues/pull requests welcome!).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 8787:8787 cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will make an RStudio server instance available to you in your browser at localhost:8787. (Change the first number after the -p to have a different address). You can log in with username:pw rstudio:rstudio and have fun.&lt;/p&gt;
&lt;p&gt;One thing I like about this is the ease with which I can now get an RStudio server up and running in the cloud (e.g. I took this for sail on DigitalOcean.com today). This means in few minutes and 1 penny you have a URL that you and any collaborators could use to interact with R using the familiar RStudio interface, already provisioned with your data and dependencies in place.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For me this is a pretty key development. It replaces a lot of command-line only interaction with probably the most familiar R environment out there, online or off. For more widespread use or teaching this probably needs to get simpler still. I’m still skeptical that this will make it out beyond the crazies, but I’m less skeptical than I was when starting this out.&lt;/p&gt;
&lt;p&gt;The ropensci-docker image could no doubt be more modular (and better documented). I’d be curious to hear if anyone has had success or problems running docker on windows / mac platforms. Issues or pull requests on the repo would be welcome! https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile (maybe the repo needs to be renamed from it’s original fork now too…)&lt;/p&gt;
&lt;p&gt;Rich et al highlighted several “remaining challenges” in their original post. Here’s my take on where those stand in the Docker framework, though I’d welcome other impressions:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;dependencies could still be missed by incompletely documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think this one is largely addressed, at least assuming a user loads the Docker image. I’m still concerned that later builds of the docker image could simply break the build (though earlier images may still be available). Does anyone know how to roll back to earlier images in docker?&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The set of scripts for managing reproducibility are at least as complex as the analysis itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think a lot of that size is due to the lack of an R image for Travis and the need to install many common tools from scratch. Because docker is both modular and easily shared via docker hub, it’s much easier to write a really small script that builds on existing images, (as I show in cboettig/rnexml)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Travis.org CI constraints: public/open github repository with analyses that run in under 50 minutes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Docker has two advantages and also some weaknesses here: (1) it should be easy to run locally, while accomplishing much of the same thing as running on travis (though clearly that’s not as nice as running automatically &amp;amp; in the cloud on every push). (2) It’s easier to take advantage of caching – for instance, cboettig/rnexml provides the knitr cache files in the image so that a user can start exploring without waiting for all the data to download and code to run.&lt;/p&gt;
&lt;p&gt;It seems that Travis CI doesn’t currently support docker since the linux kernel they use is too old. (Presumably they’ll update one day. Anyone try Shippable CI? (which supports docker))&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The learning curve is still prohibitive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think that’s still true. But what surprised me is that I’m not sure that it’s gotten any worse by adding docker than it was to begin with using Travis CI. Because the approach can be used both locally and for scaling up in the cloud, I think it offers some more immediate payoffs to users than learning a Github+CI approach does. (Notably it doesn’t require any git just to deploy something ‘reproducible’, though of course it works nicely with git.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Too Much Fun With Docker</title>
   <link href="/2014/08/07/too-much-fun-with-docker.html"/>
   <updated>2014-08-07T00:00:00+00:00</updated>
   <id>/2014/08/07/too-much-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This post was originally drafted as a set of questions to the revived &lt;a href=&quot;https://groups.google.com/forum/#!forum/ropensci-discuss&quot;&gt;ropensci-discuss list&lt;/a&gt;, hopefully readers might join the discussion from there.&lt;/p&gt;
&lt;p&gt;Been thinking about Docker and the discussion about reproducible research in the comments of Rich et al’s recent post on the &lt;a href=&quot;ropensci.org/blog/2014/06/09/reproducibility/&quot;&gt;rOpenSci blog&lt;/a&gt; where quite a few of people mentioned the potential for Docker as a way to facilitate this.&lt;/p&gt;
&lt;p&gt;I’ve only just started playing around with Docker, and though I’m quite impressed, I’m still rather skeptical that non-crazies would ever use it productively. Nevertheless, I’ve worked up some Dockerfiles to explore how one might use this approach to transparently document and manage a computational environment, and I was hoping to get some feedback from all of you.&lt;/p&gt;
&lt;p&gt;For those of you who are already much more familiar with Docker than me (or are looking for an excuse to explore!), I’d love to get your feedback on some of the particulars. For everyone, I’d be curious what you think about the general concept.&lt;/p&gt;
&lt;p&gt;So far I’ve created a &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;dockerfile&lt;/a&gt; and &lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/ropensci-docker/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have docker up and running, perhaps you can give it a test drive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should find R installed with some common packages. This image builds on Dirk Eddelbuettel’s R docker images and serves as a starting point to test individual R packages or projects.&lt;/p&gt;
&lt;p&gt;For instance, my RNeXML manuscript draft is a bit more of a bear then usual to run, since it needs &lt;code&gt;rJava&lt;/code&gt; (requires external libs), &lt;code&gt;Sxslt&lt;/code&gt; (only available on Omegahat and requires extra libs) and latest &lt;code&gt;phytools&lt;/code&gt; (a tar.gz file from Liam’s website), along with the usual mess of pandoc/latex environment to compile the manuscript itself. By building on ropensci-docker, we need a &lt;a href=&quot;https://github.com/ropensci/RNeXML/tree/master/manuscripts/Dockerfile&quot;&gt;pretty minimal docker file&lt;/a&gt; to compile this environment:&lt;/p&gt;
&lt;p&gt;You can test drive it (&lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/rnexml&quot;&gt;docker image here&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/rnexml /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once in bash, launch R and run &lt;code&gt;rmarkdown::render(&amp;quot;manuscript.Rmd&amp;quot;)&lt;/code&gt;. This will recompile the manuscript from cache and leave you to interactively explore any of the R code shown.&lt;/p&gt;
&lt;h2 id=&quot;advantages-goals&quot;&gt;Advantages / Goals&lt;/h2&gt;
&lt;p&gt;Being able to download a pre-compiled image means a user can run the code without dependency hell (often not as much an R problem as it is in Python, but nevertheless one that I hit frequently, particularly as my projects age), and also without altering their personal R environment. Third (in principle) this makes it easy to run the code on a cloud server, scaling the computing resources appropriately.&lt;/p&gt;
&lt;p&gt;I think the real acid test for this is not merely that it recreates the results, but that others can build and extend on the work (with fewer rather than more barriers than usual). I believe most of that has nothing to do with this whole software image thing – providing the methods you use as general-purpose functions in an R package, or publishing the raw (&amp;amp; processed) data to Dryad with good documentation will always make work more modular and easier to re-use than cracking open someone’s virtual machine. But that is really a separate issue.&lt;/p&gt;
&lt;p&gt;In this context, we look for an easy way to package up whatever a researcher or group is already doing into something portable and extensible. So, is this really portable and extensible?&lt;/p&gt;
&lt;h2 id=&quot;concerns&quot;&gt;Concerns:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;This presupposes someone can run docker on their OS – and from the command line at that. Perhaps that’s the biggest barrier to entry right now, (though given docker’s virulent popularity, maybe something smart people with big money might soon solve).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The only way to interact with thing is through a bash shell running on the container. An RStudio server might be much nicer, but I haven’t been able to get that running. &lt;em&gt;Anyone know how to run RStudio server from docker?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(I tried &amp;amp; &lt;a href=&quot;https://github.com/mingfang/docker-druid/issues/2&quot;&gt;failed&lt;/a&gt;)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;I don’t see how users can move local files on and off the docker container. In some ways this is a great virtue – forcing all code to use fully resolved paths like pulling data from Dryad instead of their hard-drive, and pushing results to a (possibly private) online site to view them. But obviously a barrier to entry. &lt;em&gt;Is there a better way to do this?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;alternative-strategies&quot;&gt;Alternative strategies&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Docker is just one of many ways to do this (particularly if you’re not concerned about maximum performance speed), and quite probably not the easiest. Our friends at Berkeley D-Lab opted for a GUI-driven virtual machine instead, built with Packer and run in Virtualbox, after their experience proved that students were much more comfortable with the mouse-driven installation and a pixel-identical environment to the instructor’s (see their excellent &lt;a href=&quot;https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl&quot;&gt;paper&lt;/a&gt; on this).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Will/should researchers be willing to work and develop in virtual environments? In some cases, the virtual environment can be closely coupled to the native one – you use your own editors etc to do all the writing, and then execute in the virtual environment (seems this is easier in docker/vagrant approach than in the BCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--

I would like a way for a collaborator who knows a little R to be able to open my .Rmd manuscript on his/her own computer, edit some parameters, recompile and view the pdf. RStudio w/ rmarkdown has gone a long way to making that happen.
--&gt;




</content>
 </entry>
 
 <entry>
   <title>Is statistical software harmful?</title>
   <link href="/2014/06/04/is-statistical-software-harmful.html"/>
   <updated>2014-06-04T00:00:00+00:00</updated>
   <id>/2014/06/04/is-statistical-software-harmful</id>
   <content type="html">&lt;p&gt;Ben Bolker has an excellent post on this complex issue over &lt;a href=&quot;http://dynamicecology.wordpress.com/2014/06/04/guest-post-is-statistical-software-harmful&quot;&gt;at Dynamic Ecology&lt;/a&gt;, which got me thinking about writing my own thoughts on the topic in reply.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Google recently announced that it will be making it’s own self-driving cars, rather than modifying those of others. &lt;a href=&quot;http://www.automotive.com/news/1405-google-envisions-self-driving-cars-with-no-steering-wheel/&quot;&gt;Cars that won’t have steering wheels and pedals&lt;/a&gt;. Just a button that says “stop.” What does this tell us about the future of user-friendly complex statistical software?&lt;/p&gt;
&lt;p&gt;Ben quotes prominent statisticians voicing fears that echo common concerns about self-driving cars:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Andrew Gelman attributes to Brad Efron the idea that “recommending that scientists use Bayes’ theorem is like giving the neighbourhood kids the key to your F-16″.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think it is particularly interesting and instructive that the quote Gelman attributes to Efron is about a mathematical theorem rather than about software (e.g. Bayes Theorem, not WinBUGS). Even relatively simple statistical concepts like &lt;span class=&quot;math&quot;&gt;\(p\)&lt;/span&gt; values can cause plenty of confusion, statistical package or no. The concerns are not unique to software, so the solutions cannot come through limiting access to software.&lt;/p&gt;
&lt;p&gt;I am very wary of the suggestion that we should address concerns of appropriate application by raising barriers to access. Those arguments have been made about knowledge of all forms, from access to publications, to raw data, to things as basic as education and democratic voting.&lt;/p&gt;
&lt;p&gt;There are many good reasons for not creating a statistical software implementation of a new method, but I argue here that fear of misuse just is not one of them.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;em&gt;The barriers created by not having a convenient software implementation are not an appropriate filter to keep out people who can miss-interpret or miss-use the software. As you know, a fundamentally different skillset is required to program a published algorithm (say, MCMC), than to correctly interpret the statistical consequences.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We must be wary of a different kind of statistical machismo, in which we use the ability to implement a method by one’s self as a proxy for interpreting it correctly.&lt;/p&gt;
&lt;p&gt;1a) One immediate corollary of (1) is that: &lt;em&gt;Like it or not, someone is going to build a method that is “easy to use”, e.g. remove the programming barriers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1b) The second corollary is that: &lt;em&gt;individuals with excellent understanding of the proper interpretation / statistics will frequently make mistakes in the computational implementation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Both mistakes will happen. And both are much more formidable problems in the complex methodology of today than when computer was a job description.&lt;/p&gt;
&lt;p&gt;So, what do we do? I think we should abandon the &lt;a href=&quot;http://www.r-bloggers.com/what-is-correctness-for-statistical-software/&quot;&gt;false dichotomy between “usability” and “correctness.”&lt;/a&gt;. Just because software that is easy to use is easy to misuse, does not imply that decreasing usability increases correctness. I think that is a dangerous fallacy.&lt;/p&gt;
&lt;p&gt;A software implementation should aim first to remove the programming barriers rather than statistical knowledge barriers. Best practices such as modularity and documentation should make it easy for users and developers to understand and build upon it. I agree with Ben that software error messages are poor teachers. I agree that a tool cannot be foolproof, no tool ever has been.&lt;/p&gt;
&lt;p&gt;Someone does not misuse a piece of software merely because they do not understand it. Misuse comes from mistakenly thinking you understand it. The premise that most researchers will use something they do not understand just because it is easy to use is distasteful.&lt;/p&gt;
&lt;p&gt;Kevin Slavin gives &lt;a href=&quot;http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world&quot;&gt;a fantastic Ted talk&lt;/a&gt; on the ubiquitous role of algorithms in today’s world. His conclusion is neither one of panacea or doom, but rather that we seek to understand and characterize them, learn their strengths and weaknesses like a naturalist studies a new species.&lt;/p&gt;
&lt;p&gt;More widespread adoption of software such as BUGS &amp;amp; relatives has indeed increased the amount of misuse and false conclusions. But it has also dramatically increased awareness of issues ranging from computational aspects peculiar to particular implementations to general understanding and discourse about Bayesian methods. Like Kevin, I don’t think we can escape the algorithms, but I do think we can learn to understand and live with them.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Plos Data Sharing Policy Reflections</title>
   <link href="/2014/05/30/PLoS-data-sharing-policy-reflections.html"/>
   <updated>2014-05-30T00:00:00+00:00</updated>
   <id>/2014/05/30/PLoS-data-sharing-policy-reflections</id>
   <content type="html">&lt;p&gt;PLOS has posted an &lt;a href=&quot;http://blogs.plos.org/biologue/2014/05/30/plos-data-policy-update/&quot;&gt;excellent update&lt;/a&gt; reflecting on their experiences a few months in to their new data sharing policy, which requires authors to include a statement of where the data can be obtained rather than providing it upon request. They do a rather excellent job of highlighting common concerns and offering well justified and explained replies where appropriate.&lt;/p&gt;
&lt;p&gt;At the end of the piece they pose several excellent questions, which I reflect on here (mostly as a way of figuring out my own thoughts on these issues).&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;When should an author choose Supplementary Files vs a repository vs figures and tables?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To me, repositories should always be the default. Academic repositories provide robust permanent archiving (such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt; backup), independent DOIs to content, often tracking of use metrics, enhanced discoverability, clear and appropriate licenses, richer metadata, as well as frequently providing things like API access and easy-to-use interfaces. They are the Silicon Valley of publishing innovation today.&lt;/p&gt;
&lt;p&gt;Today I think it is much more likely that some material is not appropriate for a ‘journal supplement’ rather than not being able to find an appropriate repository (enough are free, subject agnostic and accept almost any file types). In my opinion the primary challenge is for publishers to tightly integrate the repository contents with their own website, something that the repositories themselves can support with good APIs and embedding tools (many do, PLOS’s coordination with figshare for individual figures being a great example).&lt;/p&gt;
&lt;p&gt;I’m not clear on “vs figures and tables”, as this seems like a content question of “What” should be archived rather than “Where” (unless it is referring to separately archiving the figures and tables of the main text, which sounds like a great idea to me).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Should software/code be treated any differently from ‘data’? How should materials-sharing differ?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the highest level I think it is possible to see software as a ‘type’ of data. Like other data, it is in need of appropriate licensing, a management plan, documentation/metadata, and conforming to appropriate standards and placed in appropriate repositories. Of course what is meant by “appropriate” differs, but that is also true between other types of data. The same motivations for requiring data sharing (understanding and replicating the work, facilitating future work, increasing impact) apply.&lt;/p&gt;
&lt;p&gt;I think we as a scientific community (or rather, many loosely federated communities) are still working out just how best to share scientific code and the unique challenges that it raises. Traditional scientific data repositories are well ahead in establishing best practices for other data, but are rapidly working out approaches to code. The &lt;a href=&quot;http://openresearchsoftware.metajnl.com/about/editorialPolicies&quot;&gt;guidelines&lt;/a&gt; from the Journal of Open Research Software from the UK Software Sustainability Institute are a great example. (I’ve written on this topic before, such as &lt;a href=&quot;http://www.carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;what I look for in software papers&lt;/a&gt; and on the topic of the &lt;a href=&quot;www.carlboettiger.info/2013/09/25/mozilla-software-review.html&quot;&gt;Mozilla Science Code review pilot&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I’m not informed enough to speak to sharing of non-digital material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What does peer review of data mean, and should reviewers and editors be paying more attention to data than they did previously, now that they can do so?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In as much as we are satisfied with the current definition of peer review for journal articles I think this is a false dichotomy. Landmark papers, at least in my field, five or six decades ago (e.g. about as old as the current peer review system) frequently contained all the data in the paper (papers were longer and data was smaller). Somehow the data outgrew the paper and it just became okay to omit it, just as methods have gotten more complex and papers today frequently gloss over methodological details. The problem, then, is not one of type but one of scale: how do you review data when it takes up more than half a page of printed text.&lt;/p&gt;
&lt;p&gt;The problem of scale is of course not limited to data. Papers frequently have many more authors than reviewers, often representing disparate and highly specialized expertise over possibly years of work, depend upon more than 100 citations and be accompanied by as many pages of supplemental material. To the extent that we’re satisfied with how reviewers and editors have coped with these trends, we can hope for the same for data.&lt;/p&gt;
&lt;p&gt;Meanwhile, data transparency and data reuse may be more effective safe guards. Yes, errors in the data may cause trouble before they can be brought to light, just like bugs in software. But in this way they do eventually come to light, and that is somewhat less worrying if we view data the way we currently build publications (e.g. as fundamental building blocks of research) and publications as we currently view data (e.g. as a means to an ends, illustrated in the idea that it is okay to have mistakes in the data as long as they don’t change the conclusions). Jonathan Eisen has some &lt;a href=&quot;http://www.slideshare.net/phylogenomics/jonathan-eisen-talk-on-open-science-at-bosc2012-ismb&quot; title=&quot;see slide 13&quot;&gt;excellent&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=oWZzUe3Kxeo&quot;&gt;examples&lt;/a&gt; in which openly sharing the data led to rapid discovery and correction of errors that might have been difficult to detect otherwise.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And getting at the reason why we encourage data sharing: how much data, metadata, and explanation is necessary for replication?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I agree that the “What” question is a crux issue, and one we are still figuring out by community. There are really two issues here: what data to include, and what metadata (which to me includes any explanation or other documentation of the data) to provide for whatever data is included.&lt;/p&gt;
&lt;p&gt;On appropriate metadata, we’ll never have a one-size-fits-all answer, but I think the key is to at least uphold current community best-practices (best != mode), whatever they may be. Parts of this are easy: scholarly archives everywhere include basic &lt;a href=&quot;http://en.wikipedia.org/wiki/Dublin_Core&quot;&gt;Dublin Core Elements&lt;/a&gt; metadata like title, author, date, subject and unique identifier, and most data repositories will attach this information in a machine-readable metadata format with minimal burden on the author (e.g. &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt;, or to lesser extent, &lt;a href=&quot;http://figshare.org&quot;&gt;figshare&lt;/a&gt;). Many fields already have well-established and tested standards for data documentation, such as the [Ecological Metadata Langauge], which helps ecologists document things like column names and units in an appropriate and consistent way without constraining how the data is collected or structured.&lt;/p&gt;
&lt;p&gt;What data we include in the first place is more challenging, particularly as there is no good definition of ‘raw data’ (one person’s raw data being another person’s highly processed data). I think a useful minimum might be to provide any data shown in a figure or used in a statistical test that appears in the paper.&lt;/p&gt;
&lt;p&gt;Journal policies can help most in each of these cases by pointing authors to the policies of repositories and to subject-specific publications on these best practices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A crucial issue that is much wider than PLOS is how to cite data and give academic credit for data reuse, to encourage researchers to make data sharing part of their everyday routine.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again I agree that credit for data reuse is an important and largely cultural issue. Certainly editors can play there part as they already do in encouraging authors to cite the corresponding papers on the methods used, etc.&lt;/p&gt;
&lt;p&gt;I think the cultural challenge is much greater for the “long tail” content than it is for the most impactful data. I think most of the top-cited papers over the last two decades have been methods papers (or are cited for the use of a method that has become the standard of a field; often as software). As with any citation, there’s a positive feedback as more people are aware of it. I suspect that the papers announcing the first full genomes of commonly studied organisms (essentially data papers, though published by the most exclusive journals) did not lack citations. For data (or methods for that matter) that do not anticipate that level of reuse, the concern of appropriate credit is more real. Even if a researcher can assume they will be cited by future reuse of their data, they may not feel that sufficient compensation if it means one less paper to their name.&lt;/p&gt;
&lt;p&gt;Unfortunately I think these are not issues unique to data publication but germane to academic credit in general. Citations, journal names, and so forth are not meaningless metrics, but very noisy ones. I think it is too easy to fool ourselves by looking only at cases where statistical averages are large enough to see the signal – datasets like the human genome and algorithms like BLAST we know are impactful, and the citation record bears this out. Really well cited papers or well-cited journals tend to coincide with our notions of impact, so it is easy to overestimate the fidelity of citation statistics when the sample size is much smaller. Besides, academic work is a high-dimensional creature not easily reduced to a few scalar metrics. &lt;!--(I think that is why, at least in the US, we tend
to place more trust in the opinions of people over current metrics.)--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And for long-term preservation, we must ask who funds the costs of data sharing? What file formats should be acceptable and what will happen in the future with data in obsolete file formats? Is there likely to be universal agreement on how long researchers should store data, given the different current requirements of institutions and funders?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think these are questions for the scientific data repositories and the fields they serve, rather than the journals, and for the most part they are handling them well.&lt;/p&gt;
&lt;p&gt;Repositories like &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt; have clear pricing schemes closely integrated with other publication costs, and standing at least an order of magnitude less than most journal publication fees look like a bargain. (Not so if you publish in subscription journals I hear you say. Well, I would not be surprised if we start seeing such repositories negotiate institutional subscriptions to cover the costs of their authors).&lt;/p&gt;
&lt;p&gt;I think the question of data formats is closely tied to that of metadata, as they are all topics of best-practices in archiving. Many scientific data repositories have usually put a lot of thought into these issues and also weigh them against the needs and ease-of-use of the communities they serve. Journal data archiving policies can play their part by encouraging best practices by pointing authors to repository guidelines as well as published articles from their community (such as the &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4608&quot;&gt;Nine Simple Ways&lt;/a&gt; paper by White et al.)&lt;/p&gt;
&lt;p&gt;I feel the almost rhetorical question about ‘universal agreement’ is unnecessarily pessimistic. I suspect that much of the variance in recommendations for the duration a researcher should archive their own work predates the widespread emergence of data repositories, which have vastly simplified the issue from when it was left up to each individual lab. Do we ask this question of the scientific literature? No, largely because many major journals have already provided robust long term archiving with &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;/LOCKSS backup agreements. Likewise scientific data repositories seem to have settled for indefinite archiving. It seems both reasonable and practical that data archiving can be held to the same standard as the journal article itself. (Sure there are lots of challenging issues to be worked out here, the key is only to leave it in the hands of those already leading the way and not re-invent the wheel).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Integrating Github Project Repos Into The Notebook</title>
   <link href="/2014/05/07/integrating-github-project-repos-into-the-notebook.html"/>
   <updated>2014-05-07T00:00:00+00:00</updated>
   <id>/2014/05/07/integrating-github-project-repos-into-the-notebook</id>
   <content type="html">&lt;p&gt;For a while now most of my active research is developed through &lt;code&gt;.Rmd&lt;/code&gt; scripts connected to a particular project repository (something I discuss at length in &lt;a href=&quot;http://www.carlboettiger.info/2014/05/05/knitr-workflow-challenges.html&quot;&gt;deep challenges with knitr workflows&lt;/a&gt;). In the &lt;a href=&quot;http://www.carlboettiger.info/2014/05/06/steps-to-a-more-portable-workflow.html&quot;&gt;previous post&lt;/a&gt; I discuss creating a &lt;code&gt;template&lt;/code&gt; package with a more transparent organization of files, such as moving manuscripts from &lt;code&gt;inst/doc/&lt;/code&gt; to simply &lt;code&gt;manuscripts/&lt;/code&gt;. This left these exploratory analysis scripts in &lt;code&gt;inst/examples&lt;/code&gt; in a similarly unintuitive place. Though I like having these scripts as part of the repository (which keeps everything for a project in one place, as it were), like the manuscript they aren’t really part of the R package, particularly as I have gotten better at creating proper unit tests in place of just rerunning dynamic scripts occasionally.&lt;/p&gt;
&lt;p&gt;I’ve also been nagged by the idea of having to always just link to these nice dynamic documents from my lab notebook. Sure Github renders the markdown so that it’s easy enough to see highlighted code and figures etc., but it still makes them seem rather external. Occasionally I would copy the complete &lt;code&gt;.md&lt;/code&gt; file into a notebook post, but this divorced it of it’s original version history and associated &lt;code&gt;.Rmd&lt;/code&gt; source.&lt;/p&gt;
&lt;p&gt;One option would be to move them all directly into my lab notebook, &lt;code&gt;.Rmd&lt;/code&gt; files and all. This would integrate the scripts more nicely than Github’s own rendering, matching the URL and look and feel of my notebook. It would also allow for javascript elements such as MathJax equations, Google Analytics, and Disqus that are not possible when only linking to an &lt;code&gt;.md&lt;/code&gt; file on Github.&lt;/p&gt;
&lt;p&gt;In the recent &lt;a href=&quot;https://github.com/ropensci/docs&quot;&gt;ropensci/docs&lt;/a&gt; project we are exploring a way to have Jekyll automatically compile (potentially with caching) a site that uses &lt;code&gt;.Rmd&lt;/code&gt; posts and deploy to Github all using &lt;code&gt;travis&lt;/code&gt;, but we’re not quite finished and this is potentially fragile particularly with the hundreds of posts in this notebook. Besides this, the notebook structure is rather temporally oriented, (posts are chronological and reflected in my URL structure) while these scripts are largely project-oriented. (Consistent use of categories and tags would ameliorate this).&lt;/p&gt;
&lt;h3 id=&quot;embedding-images-in-.rmd-outputs&quot;&gt;Embedding images in &lt;code&gt;.Rmd&lt;/code&gt; outputs&lt;/h3&gt;
&lt;p&gt;A persistent challenge has been how best to deal with images created by these scripts, some of which I may run many times. By default &lt;code&gt;knitr&lt;/code&gt; creates &lt;code&gt;png&lt;/code&gt; images, which as binary files are ill suited for committing to Github, and which could bloat a repository rather quickly. For a long while I have used custom hooks to push these images to &lt;code&gt;flickr&lt;/code&gt;, (see &lt;a href=&quot;http://flickr.com/cboettig&quot;&gt;flickr.com/cboettig&lt;/a&gt;), inserting the permanent flickr URL into the output markdown.&lt;/p&gt;
&lt;p&gt;Recently Martin Fenner convinced me that &lt;code&gt;svg&lt;/code&gt; files would both render more nicely across a range of devices (being vector graphics), and could be easily committed to Github as they are text-based (XML) files, so that reproducing the same image in repeated runs wouldn’t take up any more space. We can then browse a nice version history of the any particular figure, and this also keeps all the output material together, making it easier to archive permanently (certainly nicer than my old archiving solution using data URIs.). Lastly, &lt;code&gt;svg&lt;/code&gt; is both web native, being a standard namespace of HTML5, and potentially interactive, as the &lt;a href=&quot;http://www.omegahat.org/SVGAnnotation/&quot;&gt;SVGAnnotation&lt;/a&gt; R package illustrates. So, lots of advantages in using &lt;code&gt;svg&lt;/code&gt; graphics.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;svg&lt;/code&gt; files also bring some unique challenges. Unlike when &lt;code&gt;png&lt;/code&gt; files are added to Github, webpages cannot directly link them since Github enforces rendering them as text instead of an image through its choice of HTML header, for security reasons. This means the only way to link to an &lt;code&gt;svg&lt;/code&gt; file on Github is to have that file on a &lt;code&gt;gh-pages&lt;/code&gt; branch, where it can be rendered as a website. A distinct disadvantage of this approach is that while we can link to a specific version of any file on Github, we see only the most recent version rendered on the website created by a &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;On the other hand, having the &lt;code&gt;svg&lt;/code&gt; files on the &lt;code&gt;gh-pages&lt;/code&gt; branch further keeps down the footprint of the project &lt;code&gt;master&lt;/code&gt; branch. This leads rather naturally to the idea that the &lt;code&gt;.Rmd&lt;/code&gt; files and their &lt;code&gt;.md&lt;/code&gt; outputs should also appear on the &lt;code&gt;gh-pages&lt;/code&gt; branch. This removes them from their awkward home in &lt;code&gt;inst/examples/&lt;/code&gt;, and enables all the benefits of custom CSS, custom javascript, and custom URLs that we don’t have on Github’s rendering.&lt;/p&gt;
&lt;p&gt;To provide a consistent look and feel, I merely copied over the &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; from my lab notebook, tweaking them slightly to use the assets already hosted there. I add custom domain name for the all my &lt;code&gt;gh-pages&lt;/code&gt; as a sub-domain, &lt;code&gt;io.carlboettiger.info&lt;/code&gt; &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, and now instead of having script output appear like so:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/7dd8fc444cb9d20d839286eac8068b3099ea9b6a/inst/examples/gaussian-process-basics.md&quot;&gt;nonparametric-bayes/inst/examples/gaussian-process-basics.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have the same page rendered on my &lt;code&gt;io&lt;/code&gt; sub-domain:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&quot;&gt;io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;with its mathjax, disqus, matching css, URL and nav elements.&lt;/p&gt;
&lt;h2 id=&quot;landing-pages&quot;&gt;Landing pages&lt;/h2&gt;
&lt;p&gt;An obvious extension of this approach is to grab a copy of the repository README and rename it &lt;code&gt;index.md&lt;/code&gt; and add a yaml header such that it serves as a landing page for the repository. A few lines of Liquid code can then generate the links to the other output scripts, as in this example:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/&quot;&gt;io.carlboettiger.info/nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;template&quot;&gt;Template&lt;/h2&gt;
&lt;p&gt;I have added a &lt;code&gt;gh-pages&lt;/code&gt; branch with this set up to my new &lt;code&gt;template&lt;/code&gt; repository, with some more &lt;a href=&quot;http://io.carlboettiger.info/template/README&quot;&gt;basic documentation and examples&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;There’s no need to use a different sub-domain than the rest of my website, other than that it would require my notebook be hosted on the &lt;a href=&quot;https://github.com/cboettig/cboettig.github.com&quot;&gt;cboettig.github.com&lt;/a&gt; repo instead of &lt;a href=&quot;https://github.com/cboettig/labnotebook&quot;&gt;labnotebook&lt;/a&gt;. However I prefer keeping my hosting on the repository I already have, and it also seems a bit unorthodox to host all my repositories on my main domain. In particular, it increases the chance for URL collisions if I create a repository with the same name as a page or directory on my website. Having gh-pages on the &lt;code&gt;io&lt;/code&gt; sub-domain feels like just the right amount of separation to me.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Steps To A More Portable Workflow</title>
   <link href="/2014/05/06/steps-to-a-more-portable-workflow.html"/>
   <updated>2014-05-06T00:00:00+00:00</updated>
   <id>/2014/05/06/steps-to-a-more-portable-workflow</id>
   <content type="html">&lt;p&gt;While I have made &lt;a href=&quot;http://www.carlboettiger.info/2012/05/06/research-workflow.html&quot;&gt;my workflow&lt;/a&gt; for most of my ongoing projects available on Github for some time, this does not mean that it has been particularly easy to follow. Further, as I move from project to project I have slowly improved how I handle projects. For instance, I have since added unit tests (with &lt;code&gt;testthat&lt;/code&gt;) and continuous integration (with &lt;a href=&quot;http://travis-ci.org&quot;&gt;travis-ci&lt;/a&gt;) to my repositories, and my handling of manuscripts has gotten more automated, with richer latex templates, yaml metadata, and simpler and more powerful makefiles.&lt;/p&gt;
&lt;p&gt;Though I have typically used my most recent project as a template for my next one (not so trivial as I work on several at a time), I realized it would make sense to just maintain a general template repo with all the latest goodies. I have now launched my &lt;a href=&quot;https://github.com/cboettig/template&quot;&gt;template&lt;/a&gt; on Github.&lt;/p&gt;
&lt;p&gt;I toyed with the idea of just treating the manuscript as a standard vignette, but this would make &lt;code&gt;pandoc&lt;/code&gt; an external dependency for the package, putting an unecessary burden on &lt;code&gt;travis&lt;/code&gt; and users. I settled on creating a &lt;code&gt;manuscripts&lt;/code&gt; directory in the project root folder as the most semantically obvious place. This is added to &lt;code&gt;.Rbuildignore&lt;/code&gt; as it doesn’t fit the standard structure of an R package, but since it is not a vignette and cannot be built with the package dependencies anyhow, this seems to make sense to me.&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The manuscript itslef is written in &lt;code&gt;.Rmd&lt;/code&gt;, with a &lt;code&gt;yaml&lt;/code&gt; header for the usual metadata of authors, affiliations, and so forth. Pandoc’s recent support for &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/manuscript.Rmd#L1-27&quot;&gt;yaml metadata&lt;/a&gt; makes it much easier to use &lt;code&gt;.Rmd&lt;/code&gt; with a LaTeX template, making &lt;code&gt;.Rnw&lt;/code&gt; rather unnecessary. &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/components/elsarticle.latex&quot;&gt;My template&lt;/a&gt; includes a custom &lt;code&gt;LaTeX&lt;/code&gt; template that includes pandoc’s macros for inserting authors, affiliations, and so forth in the correct LaTeX elements, though pandoc’s &lt;a href=&quot;https://github.com/jgm/pandoc-templates/blob/master/default.latex&quot;&gt;default template&lt;/a&gt; is rather good and already has macros for most things in place (meaning you can merely declare the layout or font in the yaml header and magically see the tex interpret it).&lt;/p&gt;
&lt;p&gt;I have tried to keep the &lt;code&gt;manuscripts&lt;/code&gt; directory relatively clean, placing &lt;code&gt;csl&lt;/code&gt;, &lt;code&gt;bibtex&lt;/code&gt;, &lt;code&gt;figures/&lt;/code&gt;, &lt;code&gt;cache/&lt;/code&gt; and other such files in a &lt;code&gt;components/&lt;/code&gt; sub-directory. I have also tried to keep the &lt;code&gt;Makefile&lt;/code&gt; as platform-independent as possible by having it call little Rscripts (also housed in &lt;code&gt;components/&lt;/code&gt;) rather than commandline utilities like &lt;code&gt;sed -i&lt;/code&gt; and &lt;code&gt;wget&lt;/code&gt; that may not behave the same way on all platforms.&lt;/p&gt;
&lt;p&gt;Lastly, Ryan Batts recently convinced me that providing binary cache files of results was an important way to allow a reader to quickly engage in exploring an analysis without having to first let potentially long-running code execute. &lt;code&gt;knitr&lt;/code&gt; provides an excellent way to create and manage this caching on a code chunk by chunk level, which is also crucial when editing a dynamic document with intensive code (no one wants to rerun your MCMC just to rebuild the pdf). Since git/Github seems like a poor option for distributing binaries, I have for the moment just archived the cache on a (university) web server and added a Make/Rscript line to that can restore it from that location. Upon publication this cache could be permanently archived (along with plain text tables of the graphs) and then installed from that archive instead.&lt;/p&gt;
&lt;p&gt;I have also added a separate &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/README.md&quot;&gt;README&lt;/a&gt; in the manuscripts directory to provide some guidance to a user seeking to build the manuscript.&lt;/p&gt;
&lt;p&gt;Examples of an active projects currently using this layout for manuscripts, etc include &lt;a href=&quot;https://github.com/ropensci/RNeXML&quot;&gt;RNeXML&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/&quot;&gt;nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Perhaps I should not have the manuscript on the master branch at all, but putting it on another branch would defeat the purpose of having it in an obviously-named directory of the repository home page where it is most easy to discover.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Deep challenges to dynamic documentation in daily workflows</title>
   <link href="/2014/05/05/knitr-workflow-challenges.html"/>
   <updated>2014-05-05T00:00:00+00:00</updated>
   <id>/2014/05/05/knitr-workflow-challenges</id>
   <content type="html">&lt;p&gt;We often discuss dynamic documents such as &lt;code&gt;Sweave&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt; in reference to final products such as publications or software package vignettes. In this case, all the elements involved are already fixed: external functions, code, text, and so forth. The dynamic documentation engine is really just a tool to combine them (knit them together). Using dynamic documentation on a day-to-day basis on ongoing research presents a compelling opportunity but a rather more complex challenge as well. The code base grows, some of it gets turned into external custom functions where it continues to change. One analysis script branches into multiple that vary this or that. The text and figures are likewise subject to the same revision as the code, expanding and contracting, or being removed or shunted off into an appendix.&lt;/p&gt;
&lt;p&gt;Structuring a dynamic document when all the parts are morphing and moving is one of the major opportunities for the dynamic approach, but also the most challenging. Here I describe some of those challenges along with various tricks I have adopted to deal with them, mostly in hopes that someone with a better strategy might be inspired to fill me in.&lt;/p&gt;
&lt;h2 id=&quot;the-old-way&quot;&gt;The old way&lt;/h2&gt;
&lt;p&gt;For a while now I have been using the &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; dynamic documentation/reproducible research software for my project workflow. Most discussion of dynamic documentation focuses on ‘finished’ products such as journal articles or reports. Over the past year, I have found the dynamic documentation framework to be particularly useful as I develop ideas, and remarkably more challenging to then integrate into a final paper in a way that really takes advantage of its features. I explain both in some detail here.&lt;/p&gt;
&lt;p&gt;My former workflow followed a pattern no doubt familiar to many:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash away in an R terminal, paste useful bits into an R script…&lt;/li&gt;
&lt;li&gt;Write manuscript separately, pasting in figures, tables, and in-line values returned from R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This doesn’t leave much of a record of what I did or why, which is particularly frustrating when some discussion reminds me of an earlier idea.&lt;/p&gt;
&lt;h2 id=&quot;dynamic-docs-.rmd-files&quot;&gt;Dynamic docs: &lt;code&gt;.Rmd&lt;/code&gt; files&lt;/h2&gt;
&lt;p&gt;When I begin a new project, I now start off writing a &lt;code&gt;.Rmd&lt;/code&gt; file, intermixing notes to myself and code chunks. Chunks break up the code into conceptual elements, markdown gives me a more expressive way to write notes than comment lines do. Output figures, tables, and in-line values inserted. So far so good. I version manage this creature in git/Github. Great, now I have a trackable history of what is going on, and all is well:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Document my thinking and code as I go along on a single file scratch-pad&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Version-stamped history of what I put in and what I got out on each step of the way&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rich markup with equations, figures, tables, embedded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching of script chunks, allowing me to tweak and rerun an analysis without having to execute the whole script. While we can of course duplicate that behavior with careful save and load commands in a script, in knitr this comes for free.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;limitations-to-.rmd-alone&quot;&gt;Limitations to .Rmd alone&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;As I go along, the &lt;code&gt;.Rmd&lt;/code&gt; files starts getting too big and cluttered to easily follow the big picture of what I’m trying to do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Before long, my investigation branches. Having followed one &lt;code&gt;.Rmd&lt;/code&gt; script to some interesting results, I start a new &lt;code&gt;.Rmd&lt;/code&gt; script representing a new line of investigation. This new direction will nevertheless want to re-use large amounts of code from the first file.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;a-solution-the-r-package-research-compendium-approach&quot;&gt;A solution? The R package “research compendium” approach&lt;/h2&gt;
&lt;p&gt;I start abstracting tasks performed in chunks into functions, so I can re-use these things elsewhere, loop over them, and document them carefully somewhere I can reference that won’t be in the way of what I’m thinking. I start to move these functions into &lt;code&gt;R/&lt;/code&gt; directory of an R package structure, documenting with &lt;code&gt;Roxygen&lt;/code&gt;. I write unit tests for these functions (in &lt;code&gt;inst/tests&lt;/code&gt;) to have quick tests to check their sanity without running my big scripts (recent habit). The package structure helps me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reuse the same code between two analyses without copy-paste or getting our of sync&lt;/li&gt;
&lt;li&gt;Document complicated algorithms outside of my working scripts&lt;/li&gt;
&lt;li&gt;Test complicated algorithms outside of my working scripts (&lt;code&gt;devtools::check&lt;/code&gt; and/or unit tests)&lt;/li&gt;
&lt;li&gt;Manage dependencies on other packages (DESCRIPTION, NAMESPACE), including other projects of mine&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This runs into trouble in several ways.&lt;/p&gt;
&lt;h2 id=&quot;problem-1-reuse-of-code-chunks&quot;&gt;Problem 1: Reuse of code chunks&lt;/h2&gt;
&lt;p&gt;What to do with code I want to reuse across blocks but do not want to write as a function, document, or test?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Perhaps this category of problem doesn’t exist, except in my laziness.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This situation arises all the time, usually through the following mechanism: almost any script performs several steps that are best represented as chunks calling different functions, such as &lt;code&gt;load_data&lt;/code&gt;, &lt;code&gt;set_fixed_parameters&lt;/code&gt;, &lt;code&gt;fit_model&lt;/code&gt;, &lt;code&gt;plot_fits&lt;/code&gt;, etc. I then want to re-run almost the same script, but with a slightly different configuration (such as a different data set or extra iterations in the fixed parameters). For just a few such cases, it doesn’t make sense to write these into a single function,&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; instead, I copy this script to a new file and make the changes there.&lt;/p&gt;
&lt;p&gt;This is great until I want to change something in about the way both scripts behave that cannot be handled just by changing the &lt;code&gt;R/&lt;/code&gt; functions they share. Plotting options are a good example of this (I tend to avoid wrapping &lt;code&gt;ggplot&lt;/code&gt; calls as separate functions, as it seems to obfuscate what is otherwise a rather semantic and widely recognized, if sometimes verbose, function call).&lt;/p&gt;
&lt;p&gt;I have explored using &lt;code&gt;knitr&lt;/code&gt;’s support for external chunk inclusion, which allows me to maintain a single R script with all commonly used chunks, and then import these chunks into multiple &lt;code&gt;.Rmd&lt;/code&gt; files. An example of this can be seen in my &lt;code&gt;nonparametric-bayes&lt;/code&gt; repo, where several files (in the same directory) draw most of their code from &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/9232dfd814c40e3c48c5a837be110a870d8639da/inst/examples/BUGS/external-chunks.R&quot;&gt;external-chunks.R&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;problem-2-package-level-reproducibility&quot;&gt;Problem 2: package-level reproducibility&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Minor/relatively easy to fix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separate files can frustrate reproducibility of a given commit. As I change the functions in &lt;code&gt;R/&lt;/code&gt;, the &lt;code&gt;.Rmd&lt;/code&gt; file can give different results despite being unchanged. (Or fail to reflect changes because it is caching chunks and does not recognize the function definitions have changed underneath it). Git provides a solution to this: since the &lt;code&gt;.Rmd&lt;/code&gt; file lives in the same git repository (&lt;code&gt;inst/examples&lt;/code&gt;) as the package, I can make sure the whole repository matches the hash of the &lt;code&gt;.Rmd&lt;/code&gt; file: &lt;code&gt;install_github(&amp;quot;packagename&amp;quot;, &amp;quot;cboettig&amp;quot;, &amp;quot;hash&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This solution is not fail-safe: the installed version, the potentially uncommitted (but possibly installed) version of the R functions in the working directory, and the R functions present at the commit of the &lt;code&gt;.Rmd&lt;/code&gt; file (and thus matching the hash) could all be different. If we commit and install before every &lt;code&gt;knit&lt;/code&gt;, we can avoid these potential errors (at the cost of some computational overhead), restoring reproducibility to the chain.&lt;/p&gt;
&lt;h2 id=&quot;problem-3-synthesizing-results-into-a-manuscript&quot;&gt;Problem 3: Synthesizing results into a manuscript&lt;/h2&gt;
&lt;p&gt;In some ways this is the easiest part, since the code-base is relatively static and it is just a matter of selecting which results and figures to include and what code is necessary to generate it. A few organizational challenges remain:&lt;/p&gt;
&lt;p&gt;While we generally want &lt;code&gt;knitr&lt;/code&gt; code chunks for the figures and tables that will appear, we usually aren’t interested in displaying much, if any, of the actual code in the document text (unlike the examples until this point, where this was a major advantage of the knitr approach). In principle, this is as simple as setting &lt;code&gt;echo=FALSE&lt;/code&gt; in the global chunk options. In practice, it means there is little benefit to having the chunks interwoven in the document. What I tend to want is having all the chunks run at the beginning, such that any variables or results can easily be added (and their appearance tweaked by editing the code) as figure chunks or in-line expressions. The only purpose of maintaining chunks instead of a simple script is the piecewise caching of chunk dependencies which can help debugging.&lt;/p&gt;
&lt;p&gt;Since displaying the code is suppressed, we are then left with the somewhat ironic challenge of how best to present code as a supplement. One option is simply to point to the source &lt;code&gt;.Rmd&lt;/code&gt;, another is to use the &lt;code&gt;tangle()&lt;/code&gt; option to extract all the code as a separate &lt;code&gt;.R&lt;/code&gt; file. In either case, the user must also identify the correct version of the R package itself for the external &lt;code&gt;R/&lt;/code&gt; functions.&lt;/p&gt;
&lt;h2 id=&quot;problem-4-branching-into-other-projects&quot;&gt;Problem 4: Branching into other projects&lt;/h2&gt;
&lt;p&gt;Things get most complicated when projects begin to branch into other projects. In an ideal world this is simple: a new idea can be explored on a new branch of the version control system and merged back in when necessary, and an entirely new project can be built as a new R package in a different repo that depends on the existing project. After several examples of each, I have learned that it is not so simple. Despite the nice tools, I’ve learned I still need to be careful in managing my workflows in order to leave behind material that is understandable, reproducible, and reflects clear provenance. So far, I’ve learned this the hard way. I use this last section of the post to reflect on two of my own examples, as writing this helps me work through what I should have done differently.&lt;/p&gt;
&lt;h3 id=&quot;example-warning-signals-project&quot;&gt;example: warning-signals project&lt;/h3&gt;
&lt;p&gt;For instance, my work on early warning signals dates back to the start of my &lt;a href=&quot;http://openwetware.org/wiki/User:Carl_Boettiger/Notebook/Stochastic_Population_Dynamics/2010/02/09&quot;&gt;open notebook on openwetware&lt;/a&gt;, when my code lived on a Google code page which seems to have disappeared. (At the time it was part of my ‘stochastic population dynamics’ project). When I moved to Github, this project got it’s own repository, &lt;a href=&quot;https://github.com/cboettig/warningsignals&quot;&gt;warningsignals&lt;/a&gt;, though after a major re-factorization of the code I moved to a new repository, &lt;a href=&quot;https://github.com/cboettig/earlywarning&quot;&gt;earlywarning&lt;/a&gt;. Okay, so far that was due to me not really knowing what I was doing.&lt;/p&gt;
&lt;p&gt;My first paper on this topic was based on the master branch of that repository, which still contains the code required. When one of the R dependencies was moved from CRAN I was able to update the codebase to reflect the replacement package (see issue &lt;a href=&quot;https://github.com/cboettig/earlywarning/issues/10&quot;&gt;#10&lt;/a&gt;). Even before that paper appeared I started exploring other issues on different &lt;a href=&quot;https://github.com/cboettig/earlywarning/network&quot;&gt;branches&lt;/a&gt;, with the &lt;code&gt;prosecutor&lt;/code&gt; branch eventually becoming it’s own paper, and then it’s &lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/&quot;&gt;own repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That paper sparked a comment letter in response to it, and the analysis involved in my reply piece was just developed on the same master branch of the prosecutor-fallacy repository. This leaves me with a total of three repositories across four branches, with one repo that corresponds more-or-less directly to a paper, one to two papers, and one to no papers.&lt;/p&gt;
&lt;p&gt;All four branches have diverged and unmerge-able code. Despite sharing and reusing functions across these projects, I often found it better to simply change the function on the new branch or new repo as I desired for the new work. These changes could not be easily merged back as they broke the original function calls of the earlier work.&lt;/p&gt;
&lt;p&gt;Hindsight being 20-20, it would have been preferable that I had maintained one repository, perhaps developed each paper on a different branch and clearly tagged the commit corresponding to the submission of each publication. Ideally these could be merged back where possible to a master branch. Tagged commits provide a more natural solution than unmerged branches to deal with changes to the package that would break methods from earlier publications.&lt;/p&gt;
&lt;h3 id=&quot;example-optimal-control-projects&quot;&gt;example: optimal control projects&lt;/h3&gt;
&lt;p&gt;A different line of research began through a NIMBioS working group called “Pretty Darn Good Control”, beginning it’s digital life in my &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt; repository. Working in different break-out groups as well as further investigation on my own soon created several different projects. Some of these have continue running towards publication, others terminating in dead ends, and still others becoming completely separate lines of work. Later work I have done in optimal control, such &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes&quot;&gt;nonparametric-bayes&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty&quot;&gt;multiple_uncertainty&lt;/a&gt; depend on this package for certain basic functions, though both also contain their own diverged versions of functions that first appeared in &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because the topics are rather different and the shared code footprint is quite small, separate repositories probably makes more sense here. Still, managing the code dependencies in separate repositories requires extra care, as checking out the right version of the focal repository does not guarantee that one will also have the right version of the [pdg_control] repository. Ideally I should note the hash of [pdg_control] on which I depend, and preferably install that package at that hash (easy enough thanks to &lt;code&gt;devtools&lt;/code&gt;), since depending on a separate project that is also still changing can be troublesome. Alternatively it might make more sense to just duplicate the original code and remove this potentially frail dependency. After all, documenting the provenance need not rely on the dependency, and it is more natural to think of these separate repos as divergent forks.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;If I have a lot of different configurations, it may make sense to wrap up all these steps into a single function that takes input data and/or parameters as it’s argument and outputs a data frame with the results and inputs.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>why I sign my reviews</title>
   <link href="/2014/05/04/why-I-sign-my-reviews.html"/>
   <updated>2014-05-04T00:00:00+00:00</updated>
   <id>/2014/05/04/why-I-sign-my-reviews</id>
   <content type="html">&lt;p&gt;For the past four years I have made an effort to sign all my reviews (which I try to keep to about one a month). It isn’t because I believe in radical openness or something crazy like that. Its really just my self interest involved – at least mostly. Writing a review is an incredibly time consuming, and largely thankless task. Supposedly anonymous peer review is supposed to protect the reviewer, particularly the scenario of the less established scientist critiquing the work of the more established. I am sure it occasionally serves that purpose. On the other hand, that very scenario can be the &lt;em&gt;most&lt;/em&gt; profitable time to sign a review. Really, when are you more likely to get an esteemed colleague to closely read your every argument than when you’re holding up their publication?&lt;/p&gt;
&lt;p&gt;While the possibility of a vindictive and powerful author sounds daunting, but rather inconsistent with my impression of most scientists, who are more apt to be impressed by an intelligent even if flawed critique than by simple praise. I find it hardest to sign a review that I have found very little constructive criticism to offer, though after a decade of being trained to critique science one can always find something. (Of course signing can be hard on the occasional terrible paper for which it is hard to offer much constructive criticism, but fortunately that has been very rare). Both authors and other reviewers (who are sometimes sent the other reviews, a practice I find very educational as a reviewer) have on occasion commented or complemented me on reviews or acknowledged me in the papers, suggesting that the practice does indeed provide for some simple recognition. At times, it may sow seeds for future collaboration.&lt;/p&gt;
&lt;p&gt;Signing my reviews has on occasion given the author a chance to follow up with me directly. While I’m not certain about journal policies in this regard, I suspect we can assume that we’re all adults capable of civil discussion. In any event, a phone call or even a few back-and-forth emails can be immensely more efficient in allowing an author to clarify elements that I have sometimes misunderstood or been unable to follow from the text, as well as making it easier to communicate my difficulties with the paper. In my experience this has resulted in both a faster and more satisfactory resolution to issues that have led to see some papers published more quickly and without as many tedious multiple rounds of revision. Given that many competitive journals simply cut off papers that might otherwise be successful with a bit more dialog between reviewer and author, because multiple “Revise and resubmits” put too much demand on editors, this seems like a desirable outcome for all involved. I’m not suggesting that such direct dialog is always desirable, but that no doubt many of us have been in the position in which a little dialog might have resolved issues more satisfactorily.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Software Sustainability Issues In R</title>
   <link href="/2014/03/20/software-sustainability-issues-in-R.html"/>
   <updated>2014-03-20T00:00:00+00:00</updated>
   <id>/2014/03/20/software-sustainability-issues-in-R</id>
   <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Editorial note:&lt;/strong&gt; The following is slightly edited text from my post to R-devel discussing this issue, which I first drafted here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There seems to be some question of how frequently changes to software packages result in irreproducible results. I am sure that research using functions like &lt;code&gt;glm&lt;/code&gt; and other functions that are shipped with base R are quite reliable; and after all they already benefit from being versioned with R releases (as Jeroen has argued).&lt;/p&gt;
&lt;p&gt;In my field of ecology and evolution, the situation is quite different. Packages are frequently developed by scientists without any background in programming and become widely used, such as &lt;a href=&quot;http://cran.r-project.org/web/packages/geiger/&quot;&gt;geiger&lt;/a&gt;, with 463 papers citing it and probably many more using it that do not cite it (both because it is sometimes used only as a dependency of another package or just because our community isn’t great at citing packages). The package has changed substantially over the time it has been on CRAN and many functions that would once run based on older versions could no longer run on newer ones. It’s dependencies, notably the phylogenetics package ape, has changed continually over that interval with both bug fixes and substantial changes to the basic data structure. The ape package has 1,276 citations (again a lower bound). I suspect that correctly identifying the right version of the software used in any of these thousands of papers would prove difficult and for a large fraction the results would simply not execute successfully. It would be much harder to track down cases where the bug fixes would have any impact on the result. I have certainly seen both problems in the hundreds of Sweave/knitr files I have produced over the years that use these packages.&lt;/p&gt;
&lt;p&gt;Even work that simply relies on a package that has been archived becomes a substantial challenge to reproducibility by other scientists even when an expert familiar with the packages (e.g. the original author) would not have a problem, as the informatics team at the Evolutionary Synthesis center recently concluded in an exercise trying to reproduce several papers including my own that used a package that had been archived (odesolve, whose replacement, deSolve, does not use quite the same function call for the same &lt;code&gt;lsoda&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;New methods are being published all the time, and I think it is excellent that in ecology and evolution it is increasingly standard to publish R packages implementing those methods, as a scan of any table of contents in “methods in Ecology and Evolution”, for instance, will quickly show. But unlike &lt;code&gt;glm&lt;/code&gt;, these methods have a long way to go before they are fully tested and debugged, and reproducing any work based on them requires a close eye to the versions (particularly when unit tests and even detailed changelogs are not common). The methods are invariably built by “user-developers”, researchers developing the code for their own needs, and thus these packages can themselves fall afoul of changes as they depend and build upon work of other nascent ecology and evolution packages.&lt;/p&gt;
&lt;p&gt;Detailed reproducibility studies of published work in this area are still hard to come by, not least because the actual code used by the researchers is seldom published (other than when it is published as it’s own R package). But incompatibilities between successive versions of the 100s of packages in our domain, along with the interdependencies of those packages might provide some window into the difficulties of computational reproducibility. I suspect changes in these fast-moving packages are far more culprit than differences in compilers and operating systems.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Startup Culture And Platforms For The Academy</title>
   <link href="/2014/01/23/Startup-Culture-and-Platforms-for-the-Academy.html"/>
   <updated>2014-01-23T00:00:00+00:00</updated>
   <id>/2014/01/23/Startup-Culture-and-Platforms-for-the-Academy</id>
   <content type="html">&lt;h2 id=&quot;scientific-research-moving-beyond-bubbles-to-platforms.&quot;&gt;Scientific Research: Moving beyond bubbles to platforms.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.economist.com/news/special-report/21593580-cheap-and-ubiquitous-building-blocks-digital-products-and-services-have-caused&quot;&gt;The Economist&lt;/a&gt; contrasts the current start-up culture to that of the dotcom bubble. Their special report argues that while the dotcom bubble was characterized by heavy investment in a single idea with a “Build it and they will come” attitude, the current generation of startups is characterized by tinkering and rapid experimentation. They argue that this is possible thanks to the cheaply available &lt;em&gt;“platforms”&lt;/em&gt; of open source software, cloud computing, APIs, and social media dissemination. And they argue that this time, it is no bubble. That it will eat the world. That these lessons apply far beyond tech startups, to businesses in general and even governments. The special report is well worth reading in full.&lt;/p&gt;
&lt;h2 id=&quot;so-how-about-science&quot;&gt;So how about science?&lt;/h2&gt;
&lt;p&gt;After all, experimentation is what science is all about. Yet the platforms that run that world are largely absent from scientific research. Research looks more like the dotcom model, where each research team must make an immense investment in infrastructure and data gathering up front. Must hire expertise in-house for each step of the process, from experimental design to data collection to analysis to writing. Such large teams resemble more the companies of previous decades with their own development, marketing, and research departments, then the agile start-ups currently building the future. The resulting products are monolithic, “build it and they will come”.&lt;/p&gt;
&lt;p&gt;I believe that this is the cornerstone of the open science and reproducible research movement.&lt;/p&gt;
&lt;h2 id=&quot;update-2014-07-24&quot;&gt;Update (2014-07-24)&lt;/h2&gt;
&lt;p&gt;This seems to be exactly what Martin Fenner is talking about in “&lt;a href=&quot;http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/&quot;&gt;Roads, not Stagecoaches&lt;/a&gt;” – the value of transformative infrastructure over individual software contributions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>So, you're active on Research Gate?</title>
   <link href="/2013/11/14/research-gate.html"/>
   <updated>2013-11-14T00:00:00+00:00</updated>
   <id>/2013/11/14/research-gate</id>
   <content type="html">&lt;p&gt;I have occassionally been getting this question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, you’re active on ResearchGate?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sounds like being accused of some scandal, doesn’t it?&lt;/p&gt;
&lt;p&gt;I’m not generally active on it - my impression is that the open science community is mostly skeptical about ResearchGate and any other “Social Network” for scientists, largely on the grounds that “we already use the same social networks everyone else uses.” Some object on more philosophical grounds (profit, Mendeley, etc), but heck I publish in Elsevier/Springer/Wiley so I won’t preach. That’s perhaps a US/elite institution centric view though; it seems more popular with a more international audience where things like basic access to pdfs may be more of an issue. I present no data to back any if that up.&lt;/p&gt;
&lt;p&gt;Personally, it hasn’t added any value for me, in contrast to the value I get from interacting with other researchers on Github, G+ or Twitter. Still, as RG recently got $35 million from Bill Gates, they might actually build something useful. Certainly traditional publishers have left plenty of room for innovation in the space of sharing data, networking, etc. So I have a profile there to wait and see, next to a disclaimer that says “please see my website for updated information.”&lt;/p&gt;
&lt;p&gt;However, I was actually impressed by ResearchGate this morning. While I thought I had successfully blocked most of their email notifications, one this morning had successfully found the full text of a recent paper of mine (albeit a few months after it had appeared). Instead of asking me to upload something, RG was able to obtain the full text from the publisher (Springer). On so doing, it also asks me if I would like to “follow” several of the researchers I cited who are also on ResearchGate.&lt;/p&gt;
&lt;p&gt;Why is that impressive? Mendeley, for all it’s much more natural fit into most researcher’s workflows, never automatically discovers papers I publish. If I want them in my Mendeley profile, I have to add them manually. Manually maintaining profiles across different networks is so entirely a waste of time and the antithesis of a linked data web where I have already made this information machine readable that I find it the most annoying feature by far in any of these sites. Here, ResearchGate is actually doing the intelligent thing, whether by connecting my RG identity to my ORCID ID, or something more heuristic. (Google Scholar automatically adds things to my profile, but in a far less selective algorithm that can be easily gamed, see &lt;a href=&quot;http://doi.org/10.1002/asi.23056&quot;&gt;10.1002/asi.23056&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;By obtaining the full-text directly from the publisher, they show the considerable advantage of a well-funded network. Presumably this indicates that access was negotiated directly with the publisher, who agrees and even facilitates me sharing the full-text of my otherwise paywalled article on my RG profile. That’s a non-trivial contribution towards open access: Contrast this to Mendeley’s more murky policy which encourages me to provide full text access through my user profile but places the legal responsibility directly on me to confirm that this permissible, or an organization like ORCID which despite (because of?) it’s more non-profit and utilitarian values does not have permission to distribute my paywalled pdfs on my profile. (Sure, my papers on arXiv already, but that isn’t the point).&lt;/p&gt;
&lt;p&gt;Likewise, using the citation data against the RG data on which researchers have profiles shows a vaguely intelligent use of data other platforms mostly ignore (providing useful suggestions using some understanding of the academic process rather than mindless application of some friend-of-a-friend network algorithm).&lt;/p&gt;
&lt;p&gt;(The fact that RG pings unfortunate souls who might have signed up once but have no desire to see my “Activity” on RG is one of it’s potentially effective marketing but more pernicious decisions. Use does not necessarily imply trust).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Do we need a culture of Data Science in Academia?</title>
   <link href="/2013/11/13/Data-Science-Center.html"/>
   <updated>2013-11-13T00:00:00+00:00</updated>
   <id>/2013/11/13/Data-Science-Center</id>
   <content type="html">&lt;p&gt;Just my draft copy of a &lt;a href=&quot;http://dynamicecology.wordpress.com/2013/11/25/do-we-need-a-culture-of-data-science-in-academia-guest-post/&quot;&gt;Guest blog post&lt;/a&gt; I wrote for Dynamic Ecology.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;On Tuesday the &lt;a href=&quot;http://www.nitrd.gov/nitrdgroups/index.php?title=Data_to_Knowledge_to_Action&quot;&gt;Whitehouse Office of Science and Technology Policy&lt;/a&gt; announced the creation of a $37.8 million dollar initiative to promote a “Data Science Culture” in academic institutions, funded by the &lt;a href=&quot;http://www.moore.org/newsroom/press-releases/2013/11/12/%20bold_new_partnership_launches_to_harness_potential_of_data_scientists_and_big_data&quot;&gt;Gordon and Betty Moore Foundation&lt;/a&gt;, &lt;a href=&quot;http://www.sloan.org/fileadmin/media/files/press_releases/datascience.pdf&quot;&gt;Alfred P. Sloan Foundation&lt;/a&gt;, and hosted in centers at the universities UC Berkeley, University of Washington, and New York University. Sadly, these announcements give little description of just what such a center would do, beyond repeating the usual the hype of “Big Data.”&lt;/p&gt;
&lt;p&gt;Fernando Perez, a research scientist at UC Berkeley closely involved with the process, paints a rather more provocative picture in his own perspective on &lt;a href=&quot;http://blog.fperez.org/2013/11/an-ambitious-experiment-in-data-science.html&quot;&gt;what this initiative might mean by a “Data Science Culture.”&lt;/a&gt; Rather than motivating the need for such a Center merely by expressing terabytes in scientific notation, Perez focuses on something not mentioned in the press releases. In his view, the objective of such a center stems from the observation that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the incentive mechanisms of academic research are at sharp odds with the rising need for highly collaborative interdisciplinary research, where computation and data are first-class citizens&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;His list of problems to be tackled by this Data Science Initiative includes some particularly catching references to issues that have raised themselves on Dynamic Ecology before:&lt;/p&gt;
&lt;!--
&gt; - An incentive structure that favors individualism, hyper-specialization and &quot;novelty&quot; to a toxic extreme
--&gt; 

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;people grab methods like shirts from a rack, to see if they work with the pants they are wearing that day&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;methodologists tend to only offer proof-of-concept, synthetic examples, staying largely shielded from real-world concerns&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well that’s a different tune than the usual big data hype&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. While it is easy to find anecdotes that support each of these charges, it is more difficult to assess just how rare or pervasive they really are. Though these are not new complaints among ecologists, the solutions (or at least antidotes) proposed in a Data Science Culture given a rather different emphasis. At first glance, the Data Science Culture sounds like the more familiar call for an interdisciplinary culture, emphasizing that the world would be a better place if only domain scientists learned more mathematics, statistics and computer science. It is not.&lt;/p&gt;
&lt;h5 id=&quot;the-problem-part-1-statistical-machismo&quot;&gt;the problem, part 1: &lt;a href=&quot;http://dynamicecology.wordpress.com/2012/09/11/statistical-machismo/&quot;&gt;statistical machismo&lt;/a&gt;?&lt;/h5&gt;
&lt;p&gt;As to whether ecologists choose methods to match their pants, we have at least some data beyond anecdote. A survey earlier this year by Joppa et al. &lt;a href=&quot;http://doi.org/10.1126/science.1231535&quot;&gt;(2013) &lt;em&gt;Science&lt;/em&gt;&lt;/a&gt;) has indeed shown that most ecologists select methods software guided primarily by concerns of fashion (in other words, whatever everybody else uses). The recent expansion of readily available statistical software has greatly increased the number of shirts on the rack. Titles in &lt;em&gt;Ecology&lt;/em&gt; reflect the trend of rising complexity in ecological models, such as &lt;a href=&quot;http://doi.org/10.1890/10-1124.1&quot;&gt;Living Dangerously with big fancy models&lt;/a&gt; and &lt;a href=&quot;http://doi.org/10.1890/10-0052.1&quot;&gt;Are exercises like this a good use of anybody’s time?&lt;/a&gt;). Because software enables researchers to make use of methods without the statistical knowledge of how to implement them from the ground up, many echo the position so memorably &lt;a href=&quot;http://press.princeton.edu/titles/8348.html&quot;&gt;articulated by Jim Clark&lt;/a&gt; that we “handing guns to children.” This belittling position usually leads to a call for improved education and training in mathematical and statistical underpinnings (see each of the 9 articles in another &lt;a href=&quot;http://doi.org/10.1890/08-1402.1&quot;&gt;&lt;em&gt;Ecology&lt;/em&gt; Forum&lt;/a&gt; on this topic), or the occassional wistful longing for a simpler time.&lt;/p&gt;
&lt;h5 id=&quot;the-solution-part-1-data-publication&quot;&gt;the solution, part 1: data publication?&lt;/h5&gt;
&lt;p&gt;What is most interesting to me in Perez’s perspective on the Data Science Institute in an emphasis on changing &lt;em&gt;incentives&lt;/em&gt; more than changing &lt;em&gt;educational&lt;/em&gt; practices. Perez characterizes the fundamental objective of the initiative as a &lt;em&gt;cultural shift in which&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The creation of usable, robust computational tools, and the work of data acquisition and analysis must be treated as equal partners to methodological advances or domain-specific results”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While this does not tackle the problem of misuse or misinterpretation of statistical methodology head-on, I believe it is a rather thought-provoking approach to mitigate the consequences of mistakes or limiting assumptions. By atomizing the traditional publication into such component parts: data, text, and software implementation, it becomes easier to recognize each for it’s own contributions. A brilliantly executed experimental manipulation need not live or die on some minor flaw in a routine statistical analysis when the data is a product in its own right. Programmatic access to raw data and computational libraries of statistical tools could make it easy to repeat or alter the methods chosen by the original authors, allowing the consequences of these mistakes to be both understood and corrected. In the current system in which access to the raw data is rare, statistical mistakes can be difficult to detect and even harder to remedy. This in turn places a high premium on the selection of appropriate statistical methods, while putting little selective pressure on the details of the data management or implementation of those methods. Allowing the data to stand by itself places a higher premium on careful collection and annotation of data (e.g. the adoption of metadata standards). To the extent that misapplication of statistical and modeling approaches could place a substantial error rate on the literature (&lt;a href=&quot;http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble?fsrc=scn/tw_ec/trouble_at_the_lab&quot;&gt;Economist&lt;/a&gt;, &lt;a href=&quot;http://doi.org/10.1371/journal.pmed.0020124&quot;&gt;Ioannidis 2005&lt;/a&gt;), independent data publication might be an intriguing antidote.&lt;/p&gt;
&lt;!--
Data scales in a way that publications do not, even (or rather, especially) within a particular sub-domain.  Given a single paper on a topic, it is much easier to read the conclusion than replicate it from the data.  Given 1000 papers on the topic vs having the underlying data in a standardized form, it becomes easier to replicate the analyses. Surely that would just lead to nonsense, given the idiosyncrasies of each different experiment that were never meant to be compared in that way?  Surely only by carefully reading not only each paper, but each supplement, can we really understand what was done well enough to replicate or re-analyze it?  Yet we cite papers as if they have demonstrated something in far more generality than all that.   

--&gt;

&lt;h5 id=&quot;the-problem-part-2-junk-software&quot;&gt;the problem, part 2: junk software&lt;/h5&gt;
&lt;p&gt;As Perez is careful to point out, those implementing and publishing methods aren’t helping either. Unreliable, inextensible and opaque computational implementations act both as barriers to adoption and validation. Trouble with scientific software has been well recognized by the literature (e.g. &lt;a href=&quot;http://doi.org/10.1038/467775a&quot;&gt;Merali (2010), &lt;em&gt;Nature&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;http://doi.org/10.1038/nature10836&quot;&gt;Inces et al. (2012), &lt;em&gt;Nature&lt;/em&gt;&lt;/a&gt;), the news (&lt;a href=&quot;http://www.timeshighereducation.co.uk/news/save-your-work-give-software-engineers-a-career-track/2006431.article&quot;&gt;Times Higher Education&lt;/a&gt;) and funding agencies (&lt;a href=&quot;http://www.nsf.gov/pubs/2013/nsf13525/nsf13525.htm&quot;&gt;National Science Foundation&lt;/a&gt;). While it is difficult to assess the frequency of software bugs that may really alter the results (though see Inces et al.), designs that will make software challenging or impossible to maintain, scale to larger tasks or extend as methods evolve are more readily apparent. Cultural challenges around software run as deep as they do around data. When Mozilla’s Science Lab &lt;a href=&quot;http://doi.org/10.1038/501472a&quot;&gt;undertook a review of code&lt;/a&gt; associated with scientific publications, they took some criticism from other &lt;a href=&quot;http://simplystatistics.org/2013/09/26/how-could-code-review-discourage-code-disclosure-reviewers-with-motivation/&quot;&gt;advocates&lt;/a&gt; of publishing code. I encountered this first hand in replies from authors, editors and reviewers on my own blog post &lt;a href=&quot;http://carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;suggesting we raise the bar&lt;/a&gt; on the review of methodological implementations. Despite disagreement about where that bar should be, I think we all felt the community could benefit from clearer guidance or consensus on how to review papers in which the software implementation plays an essential part and contribution.&lt;/p&gt;
&lt;h5 id=&quot;the-solution-part-2-software-publication&quot;&gt;the solution, part 2: software publication?&lt;/h5&gt;
&lt;p&gt;As in the case of data, educational practices are the route usually suggested to address better programming practices, and no doubt these are important. Once again though, it is interesting to think how a higher incentive on such research products might also improve their quality, or at least facilitate distilling the good from the bad from the ugly, more easily. Yet in this case, I think there is a potential downside as well.&lt;/p&gt;
&lt;h6 id=&quot;or-not&quot;&gt;Or not?&lt;/h6&gt;
&lt;p&gt;While widespread recognition of its importance will no doubt help bring us faster software, fewer bugs and more user-friendly interfaces, it may do more harm than good. Promotion of software as a product can lead to empire-building, for which ESRI’s ArcGIS might be a poster child. The scientific concepts become increasingly opaque, while training in a conceptually rich academic field gives way to more mindless training in the user interface of a single giant software tool. I believe that good scientific software should be modular – small code bases that can be easily understood, inter-operable, and perform a single task well (the Unix model). This lets us build more robust computational infrastructure tailored to the problem at hand, just as individual Lego bricks may be assembled and reassembled. Unfortunately, I do not see how recognition for software products would promote small modules over vast software platforms, or interoperability with other software instead of an exclusive walled garden.&lt;/p&gt;
&lt;!-- include image of lego brick tower?--&gt;


&lt;h4 id=&quot;so-change-incentives-how&quot;&gt;So, change incentives &lt;em&gt;how&lt;/em&gt;?&lt;/h4&gt;
&lt;p&gt;If this provides some argument as to why one might want to change incentives around data and software publication, I have said nothing to suggest how. After all, as ecologists we’re trained to reflect on the impact a policy would have, not advocate for what should be done about it. If the decision-makers agree about the effects of the given incentives, then choosing what to reward should be easier.&lt;/p&gt;
&lt;!--
#### Data Science Culture is Data Sharing Culture? 

In raising these questions, Perez brings the role of a Data Science Institute down from the clouds of superficial pattern-finding in astronomically large data sets that bear no resemblance to what most of us work on, and places front and center the challenges of designing, selecting, implementing and interpreting the richer statistical analyses we face each day. In the process of bolstering the data and methodology upon which we base research today, we may open up the doors to better, faster, and bigger science tomorrow.  

--&gt;


&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Probably for reasons discussed &lt;a href=&quot;http://dynamicecology.wordpress.com/2013/11/07/the-one-true-route-to-good-science-is/comment-page-1/#comment-20373&quot;&gt;recently on Dynamic Ecology&lt;/a&gt; about politicians and dirty laundry.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Is it time to retire Pagel's lambda?</title>
   <link href="/2013/10/11/is-it-time-to-retire-pagels-lambda.html"/>
   <updated>2013-10-11T00:00:00+00:00</updated>
   <id>/2013/10/11/is-it-time-to-retire-pagels-lambda</id>
   <content type="html">&lt;p&gt;Pagel’s &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; (lambda), introduced in &lt;a href=&quot;http://doi.org/10.1038/44766&quot; title=&quot;Inferring the historical patterns of biological evolution. in Nature.&quot;&gt;Pagel 1999&lt;/a&gt; as a potential measure of “phylogenetic signal,” the extent to which correlations in traits reflect their shared evolutionary history (as approximated by Brownian motion).&lt;/p&gt;
&lt;p&gt;Numerous critiques and ready alternatives have not appeared to decrease it’s popularity. There are many issues with the statistic, some of which I attempt to summarise below.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; statistic is defined by the Brownian motion model together with a transformation of the branch lengths: multiply all internal branches by &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt;. The motivation for the definition is obvious: &lt;span class=&quot;math&quot;&gt;\(\lambda = 1\)&lt;/span&gt; the tree is unchanged and the model equivalent to Brownian motion, while for &lt;span class=&quot;math&quot;&gt;\(\lambda = 0\)&lt;/span&gt; the tree becomes a star phylogeny and the model is equivalent to completely independent random walks. &lt;span class=&quot;math&quot;&gt;\(0 &amp;lt; \lambda &amp;lt; 1\)&lt;/span&gt; provides an intermediate range where the correlations are weaker than expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem 1: It is biological nonsense to treat tips different from other edges.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All other problems arise from this. While it is okay that a statistic does not have a corresponding evolutionary model, being part of an explicit model might have helped avoid this sillyness. Technically &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; is a model, but one that treats evolution along “tips” as special, as if evolution should follow completely different rules for a species alive today relative to it’s former evolutionary history. Sounds almost creationist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem 2: The statistic doesn’t measure what is says it measures.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To demonstrate this, we can consider two cases in which phylogeny has the identical effect of explaining trait correlations, and yet have very different lambdas. Consider that Researcher 1 examines the phylogeny in Figure 1 and estimates very little phylogenetic signal, &lt;span class=&quot;math&quot;&gt;\(\lambda = 0.1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(ape)
&lt;span class=&quot;kw&quot;&gt;cat&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;(((A_sp:10,B_sp:10):1,C_sp:11):1,D_sp:12);&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;file =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;ex.tre&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sep =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;)
ex &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;read.tree&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;ex.tre&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(ex)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm4.staticflickr.com/3708/10715190003_f2f21044be_o.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;figcaption&gt;Figure 1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now Researcher 2 discovers closely related sister species of some of the taxa originally studied, as in Figure 2.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;cat&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;((((A_sp:1, A2_sp:1):9,(B_sp:1, B2_sp:1):9):1,(C_sp:1, C2_sp:1):10):1,(D_sp:1, D2_sp:1):11);&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;file =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;ex2.tre&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sep =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;)
ex2 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;read.tree&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;ex2.tre&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm6.staticflickr.com/5482/10715001046_f914f6ecee_o.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;figcaption&gt;Figure 2&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;There traits of sister taxa are very similar (indeed let us assume the sister species are hard to distinguish morphologically - perhaps why they were overlooked by Researcher 1). The OU or BM model estimates made by researcher 1 will closely agree with with those of Researcher 1, since the sister taxa have quite similar traits. Yet the &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; estimates differs greatly – all of a sudden the phylogenetic signal must be quite high!&lt;/p&gt;
&lt;p&gt;And yet the underlying evolutionary process by which we have simulated the data has been unchanged! The difference arises because what formerly appeared as long tips have become short tips. How do we intepret a metric that depends so heavily on whether or not all sister species are present in the data? As noted, this problem does not impact other phylogenetic comparative methods to nearly the same extent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem 3&lt;/strong&gt; The statistic has no notion of timescale or depth in the phylogeny.&lt;/p&gt;
&lt;p&gt;In &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; (and other definitions such as Blomberg’s K), phylogenetic signal is an all-or-nothing proposition. If we find that really recently diverged species that happen to resemble each-other, while species that have diverged for longer than, say, a couple million years show no correlation – is this phylogenetic signal or not? This ‘extinction’ of phylogenetic signal as we go far enough back in time seems like a biologically reasonable concept that is perfectly well expressed in the &lt;span class=&quot;math&quot;&gt;\(\alpha\)&lt;/span&gt; parameter of the OU model, but is lost in the consideration of &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt;. If folks really want to estimate a continuous quantity to measure phylogenetic signal, I suggest &lt;span class=&quot;math&quot;&gt;\(\alpha\)&lt;/span&gt; is a far more meaningful number (note that it has units! (1/time or 1/branch length)).&lt;/p&gt;
&lt;p&gt;Consider the returning force alpha in the OU model (i.e. stabilizing selection). When alpha is near zero, the model is essentially Brownian, (i.e. ‘strong phylogenetic signal,’ where more recently diverged species are more similar on average than distantly related ones). When alpha is very large, traits reflect the selective constraint of the environment rather than their history, and so recently diverged species are no more or less likely to be similar than distant ones (provided all species in question are under the same OU model / same selection strength for the trait in question). The size of alpha gives the timescale over which ‘phylogenetic signal’ is lost (in units of the branch length). Two very recently diverged sister-taxa may thus show some phylogenetic correlation because their divergence time is of order 1/alpha, while those with longer divergence times act phylogeneticly independent, such as in our Figure 2 above. I find this an imperfect but reasonable meaning of phylogenetic signal.&lt;/p&gt;
&lt;p&gt;If we restrict &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt; to be strictly 1 or 0 these problems are alliviated, though then it is unnessary to define the statistic as such as we may instead consider a star tree (sometimes called the “white noise” model of evolution).&lt;/p&gt;
&lt;h4 id=&quot;other-such-statistics&quot;&gt;other such statistics&lt;/h4&gt;
&lt;p&gt;Pagel’s &lt;span class=&quot;math&quot;&gt;\(\delta\)&lt;/span&gt; is a transformation on node depth, which is again problematic as there is no meaningfully consistent way to describe what is a node (think about deep speciation events with no present day ancestor.) I believe &lt;span class=&quot;math&quot;&gt;\(\kappa\)&lt;/span&gt; would also be problematic to interpret as it is a nonlinear transform of branch length – raises branch length to a power – and thus would have a rather different effect depending on the units in which branch length were measured. (For instance, consider the case where the tree is scaled to length unity, so all branch lengths are less than one and thus become shorter with large exponents, vs one in which lengths are all larger than one). Fortunately these statistics are far less popular than &lt;span class=&quot;math&quot;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reflections on the Mozilla Science Code Review Pilot</title>
   <link href="/2013/09/25/mozilla-software-review.html"/>
   <updated>2013-09-25T00:00:00+00:00</updated>
   <id>/2013/09/25/mozilla-software-review</id>
   <content type="html">&lt;p&gt;I was recently interviewed a &lt;em&gt;Nature&lt;/em&gt; senior reporter &lt;a href=&quot;http://twitter.com/erica_check&quot;&gt;Erika Check Hayden&lt;/a&gt; on the subject of the scientific &lt;a href=&quot;http://kaythaney.com/2013/08/08/experiment-exploring-code-review-for-science/&quot;&gt;code review project&lt;/a&gt; being conducted by &lt;a href=&quot;https://wiki.mozilla.org/ScienceLab&quot;&gt;Mozilla Science Lab&lt;/a&gt;. The piece appears in this week’s issue, &lt;a href=&quot;http://doi.org/10.1038/501472a&quot; title=&quot;Mozilla plan seeks to debug scientific code&quot;&gt;Hayden 2013&lt;/a&gt;. &lt;a href=&quot;http://carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;My blog post&lt;/a&gt; sharing my own approach to code review is mentioned at the beginning of the article, though it is rather Roger Peng’s comments at the end that have stirred &lt;a href=&quot;https://twitter.com/Erika_Check/status/382911015358181376/photo/1&quot;&gt;some interesting discussion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Roger raises two concerns. First, that increased scrutiny will discourage researchers from sharing code, (which, right or wrong, remains a voluntary choice in most journals):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One worry I have is that, with reviews like this, scientists will be even more discouraged from publishing their code&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and second, that code review does not focus on what matters mosts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We need to get more code out there, not improve how it looks&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Erika provides a bit more context to Roger’s comments below).&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
&lt;a href=&quot;https://twitter.com/ctitusbrown&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/cboettig&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/nickbarnes&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;nickbarnes&quot;&gt;@nickbarnes&lt;/span&gt;&lt;/a&gt; see whole &lt;a href=&quot;https://twitter.com/simplystats&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;simplystats&quot;&gt;@simplystats&lt;/span&gt;&lt;/a&gt; quote on prof. code review discouraging sharing &lt;a href=&quot;http://t.co/pNQWT9Safz&quot;&gt;pic.twitter.com/pNQWT9Safz&lt;/a&gt;
&lt;/p&gt;
— Erika Check Hayden (&lt;span class=&quot;citation&quot; data-cites=&quot;Erika_Check&quot;&gt;@Erika_Check&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/Erika_Check/statuses/382911015358181376&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;Nature&lt;/em&gt; News piece thus nails a central tension in the community between promoting higher standards for code (A position exemplified in an earlier &lt;em&gt;Nature&lt;/em&gt; column titled &lt;a href=&quot;http://doi.org/10.1038/467775a&quot; title=&quot;Merali 2010, Nature&quot;&gt;“Computational Science … Error: Why scientific programming does not compute”&lt;/a&gt;, and more recently in &lt;em&gt;Science&lt;/em&gt; by &lt;a href=&quot;http://doi.org/10.1126/science.1231535&quot; title=&quot;Joppa et al. 2013, &amp;#39;Troubling Trends in Scientific Software Use&amp;#39;&quot;&gt;Joppa &lt;em&gt;et al.&lt;/em&gt; 2013&lt;/a&gt;, which explicitly calls for peer review of scientific software) vs promoting more widespread sharing of software (as exemplified by Nick Barnes piece in the same issue, &lt;a href=&quot;http://doi.org/10.1038/467753a&quot; title=&quot;Barnes 2010, Nature&quot;&gt;“Publish your computer code: it is good enough”&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The arguments made in each of the perspectives are excellent, and should be required reading for anyone interested in the subject. In addition to the shorter comment by Barnes, I also recommend the more recent &lt;em&gt;Nature&lt;/em&gt; perspective, &lt;a href=&quot;http://doi.org/10.1038/nature10836&quot; title=&quot;Ince et al. 2012, Nature&quot;&gt;The Case for Open Computer Programs&lt;/a&gt;, which lays out the argument and modest practical recommendations (that have largely been ignored as far as I can tell) just brilliantly. In particular, I think they nail the issue of why describing the algorithm or providing pseudo-code is not a satisfactory description of the method.&lt;/p&gt;
&lt;p&gt;However, I also think the tension between review and sharing is somewhat artificial. While each of these positions emphasizes the need to share source-code, the call for code review by Merali, Joppa et al (or in my own blog post mentioned earlier), focus on &lt;em&gt;scientific software&lt;/em&gt; aimed at reuse by others. The concerns voiced in Roger Peng’s comments and echoed by Nick Barnes focus on another class of code entirely – code associated with a particular research publication that would primarily serve only to document and support those results, rather than be readily adapted to other uses.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
&lt;a href=&quot;https://twitter.com/cboettig&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/ctitusbrown&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; but far more important to get code out than to get it “right”, IYSWIM.
&lt;/p&gt;
— Nick Barnes (&lt;span class=&quot;citation&quot; data-cites=&quot;nickbarnes&quot;&gt;@nickbarnes&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/nickbarnes/statuses/382746748135174144&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;



&lt;h3 id=&quot;classes-of-code-snippets-vs-software&quot;&gt;Classes of Code: Snippets vs Software&lt;/h3&gt;
&lt;p&gt;For me, the crux of these concerns lies in the difference between “software papers”&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and papers which merely use code in some element of the methodology. The Mozilla study focused exclusively on code appearing in the full-text of publications in PLoS Computational Biology. Though I am pleased to see the Science Lab tackle the issue of software review and bring the expertise of their professional software developers to bear on scientific code, this is perhaps not the kind of code I would have chosen to focus on (something I shared with the team early on in seeing the announcement).&lt;/p&gt;
&lt;p&gt;Without knowing which papers are included it is of course difficult to say to much. But knowing that the code appears in the full text of the papers themselves, we can assume that it is not a complete software package intended for reuse by other researchers. Using code within the body of a manuscript implies the intent to communicate methodology more concisely and precisely than might be done in prose; in much the same manner that we use equations in place of prose. This is an important development in scientific communication, but is also rather distinct from the use of code in other contexts, in which the code itself is meant to be read primarily by machines. It is code that is already intended to help &lt;em&gt;explain&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Code included in appendices to scientific papers is meant rather to &lt;em&gt;document&lt;/em&gt; exactly what has been done, in a manner that assists replication, and may require considerable effort to decipher exactly what is being done. Instead, it merely supports the more readable but less precise description and potentially the pseudo-code that would appear in the body text.&lt;/p&gt;
&lt;p&gt;Code intended for reuse as research software (in software papers) is another class entirely. Ostensibly, the user never needs to see the code itself, but only interact with the user interface or end-user functions (API) provided. Code that is written clearly and concisely still has value – helping identify bugs and facilitating future researcher-developers extending the software, but most of it’s functionality can be accessed and assessed without looking at the source. I think it is in this kind of review that we as a researcher-developer community could learn the most from the Mozilla software engineering experts.&lt;/p&gt;
&lt;p&gt;I believe the most important focus of code review is in scientific software rather than in code snippets. And in reviewing software, I think all of the most important elements do not actually involve reading the source code at all (as I discuss in my &lt;a href=&quot;http://carlboettiger.info/2013/07/09/reviewing-software-revisited.html&quot;&gt;revised position on reviewing software papers&lt;/a&gt;), but rather in establishing that the software behaves as expected and follows software development practices that make it more sustainable, such as hosting in a software repository, version control, or example input and output.&lt;/p&gt;
&lt;h3 id=&quot;code-vanity&quot;&gt;Code vanity?&lt;/h3&gt;
&lt;p&gt;Roger’s second comment appears more dismissive of code review than I think it actually is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“We need to get more code out there, not improve how it looks.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
&lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/ctitusbrown&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;&lt;/a&gt; yup, ‘pretty’ is dismissive terminology, though possibly short-hand for ‘human-readable’ not just ‘machine-readable’
&lt;/p&gt;
— Carl Boettiger (&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/cboettig/statuses/382905327013728256&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Most modern languages include &lt;a href=&quot;http://en.wikipedia.org/wiki/Syntactic_sugar&quot;&gt;syntactic sugar&lt;/a&gt;: ways of expressing commands that are more easily interpretable to human readers. For instance, in C, &lt;code&gt;a[i]&lt;/code&gt; is syntactic sugar for &lt;code&gt;*(a+i)&lt;/code&gt;. Higher-level languages are in some ways all sugar around existing lower-level libraries.&lt;a href=&quot;#fn2&quot; class=&quot;footnoteRef&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Like good mathematical notation or good prose, this is not just about being ‘pretty’, but being more effective in communicating with humans. Certainly this is something we can improve upon as researchers, but it is perhaps not the best starting point.&lt;/p&gt;
&lt;h3 id=&quot;share-first-fix-later&quot;&gt;Share first, fix later&lt;/h3&gt;
&lt;p&gt;If code review should apply to all levels of code or be reserved for scientific software may still be an open question. What we should be able to agree on is in the publishing of code in the first place:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
&lt;a href=&quot;https://twitter.com/cboettig&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/nickbarnes&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;nickbarnes&quot;&gt;@nickbarnes&lt;/span&gt;&lt;/a&gt; I think code sharing should be mandatory &lt;em&gt;shrug&lt;/em&gt;. It's part of the methods. I reject papers w/o it.
&lt;/p&gt;
— Titus Brown (&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/ctitusbrown/statuses/382903924253929472&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;


&lt;p&gt;It continues to surprise me how few journals require code deposition. &lt;em&gt;Science&lt;/em&gt; explicitly &lt;a href=&quot;http://doi.org/10.1126/science.1203354&quot;&gt;adopted a new policy in 2011&lt;/a&gt; stating&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Science&lt;/em&gt; is extending our data access requirement listed above to include computer codes involved in the creation or analysis of data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which asks that the data be placed in a appropriate permanent repository or otherwise placed in the supplementary materials (see &lt;a href=&quot;http://www.sciencemag.org/site/feature/contribinfo/prep/gen_info.xhtml#dataavail&quot;&gt;information for authors&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Yet I have not seen code provided for any analysis I have read in Science since 2011. Either we have a very different understanding of what it means to use computer codes in the analysis of data or &lt;em&gt;Science&lt;/em&gt; grossly neglects its own policy.&lt;a href=&quot;#fn3&quot; class=&quot;footnoteRef&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Not to pick on them of course, few other journals have explicitly adopted such a policy.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
.&lt;a href=&quot;https://twitter.com/cboettig&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/nickbarnes&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;nickbarnes&quot;&gt;@nickbarnes&lt;/span&gt;&lt;/a&gt; As one of my grad students said to me, “I don't understand why ‘must share code’ is a radical opinion.”
&lt;/p&gt;
— Titus Brown (&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/ctitusbrown/statuses/382904483102982145&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;


&lt;p&gt;Nick Barnes suggests that this alone may be enough to improve code quality:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;
&lt;a href=&quot;https://twitter.com/cboettig&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cboettig&quot;&gt;@cboettig&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/ctitusbrown&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ctitusbrown&quot;&gt;@ctitusbrown&lt;/span&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/kaythaney&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;kaythaney&quot;&gt;@kaythaney&lt;/span&gt;&lt;/a&gt; require sharing. Pride will then rapidly lead to review and other improvement techniques.
&lt;/p&gt;
— Nick Barnes (&lt;span class=&quot;citation&quot; data-cites=&quot;nickbarnes&quot;&gt;@nickbarnes&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/nickbarnes/statuses/382755320143282176&quot;&gt;September 25, 2013&lt;/a&gt;
&lt;/blockquote&gt;

&lt;p&gt;Certainly it will help, though not enough if the state of open source scientific software is any indication (Nick does acknowledge a rather geological notion of ‘rapid’). Smaller codes used in particular analyses will certainly feel this pressure more, as they will be easier to scrutinize.&lt;/p&gt;
&lt;h3 id=&quot;so-what-might-we-learn-from-the-mozilla-code-review&quot;&gt;So what might we learn from the Mozilla Code review?&lt;/h3&gt;
&lt;p&gt;Focusing on code appearing in-line in papers certainly addresses a different beast than large scientific software packages intended for reuse. As Roger observed, we will likely learn that scientists aren’t software engineers. We may learn how to use code to communicate more effectively. We may learn some lessons that apply for larger codebases involved in scientific software, but I think there the problem are often outside of the individual lines of code themselves and arise from other development practices.&lt;/p&gt;
&lt;p&gt;Still, learning how to do code review at all would be an invaluable start. As the discussion on my own post on the subject made clear, we researchers have no training in this practice. The Mozilla study would give the first taste. I only hope they turn there attention next to larger scientific software that lives outside of the publications themselves and is intended at re-purposing and reuse. Meanwhile, I also hope journals will become more serious about recognizing code as methods, as they have started to do with data.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;which I define as papers primarily aimed at promoting the reuse of a particular codebase and providing an indexed citation target to credit the work.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;It is frequently argued that ideal code should be ‘self-documenting’, with syntax so precise that no other explanation of the function is necessary. While I think this is an admirable ideal, it should never be a substitute for actually providing prose documentation as well. I find we too easily decieve ourselves as to just how self-documenting our own code really is (it’s all so obvious at the time, right?).&lt;a href=&quot;#fnref2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;From my reading it appears that &lt;em&gt;Science&lt;/em&gt; requirements are intentionally too vague on this issue, stating: “All computer codes involved in the creation or analysis of data must also be available to any reader of Science.” It is unclear if ‘available on request’ is considered appropriate for code, though it is explicitly not acceptable for data: “we have therefore required authors to enter into an archiving agreement, in which the author commits to archive the data on an institutional Web site, with a copy of the data held at Science”, and the editorial makes it clear that “the data access requirement … includes computer codes”. Meanwhile all studies involving ‘requests for data’ have shown a return rate of substantially less than a 100% (usually less than 20%, e.g. &lt;a href=&quot;http://doi.org/10.1371/journal.pbio.1001636&quot;&gt;this recent study&lt;/a&gt;). If that was a viable option we could just publish abstracts and have papers available on request too. If you need to know who wants your data, why not do the same for papers?&lt;a href=&quot;#fnref3&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Forking The R Journal</title>
   <link href="/2013/07/10/forking-the-R-journal.html"/>
   <updated>2013-07-10T00:00:00+00:00</updated>
   <id>/2013/07/10/forking-the-R-journal</id>
   <content type="html">&lt;p&gt;All I really wanted to do is cite a paper in &lt;em&gt;the R Journal&lt;/em&gt;, which is peer-reviewed, open access (CC-BY), LaTeX based and without author charges. Sure, I could do that already, but I like being able to programmatically generate citation metadata from a URL – we do have this thing called the internet now, and citations are just links, right? Unfortunately, nice as it is, the R journal doesn’t have HTML landing pages for articles that embed the metadata.&lt;/p&gt;
&lt;p&gt;This makes it harder for Google Scholar to index the articles, and means that we cannot extract citation metadata from the URL using a tool like &lt;a href=&quot;http://greycite.knowledgeblog.org&quot;&gt;greycite&lt;/a&gt;. Until now.&lt;/p&gt;
&lt;p&gt;I wrote to Editor-in-Chief Hadley Wickam about this, who responded in the best way possible: making the &lt;a href=&quot;ihttps://github.com/rjournal/rjournal.github.io&quot;&gt;journal website’s Github repository&lt;/a&gt; public. A fork and a little hacking later, and voila, we have html landing pages for &lt;em&gt;the R Journal&lt;/em&gt; with embedded metadata (pending a &lt;a href=&quot;https://github.com/rjournal/rjournal.github.io/pull/1&quot;&gt;pull request&lt;/a&gt;). Check out &lt;a href=&quot;https://github.com/cboettig/rjournal.github.io/commit/e70e84e6e53e6c04ec9864af162d7ba58439d4d5&quot;&gt;the source code&lt;/a&gt; for how this works – it’s really quite straight forward since the metadata is already available in &lt;code&gt;_config.yaml&lt;/code&gt;. The main step involves a &lt;a href=&quot;https://github.com/cboettig/rjournal.github.io/blob/e70e84e6e53e6c04ec9864af162d7ba58439d4d5/_plugins/article_html_pages.rb&quot;&gt;Generator plugin&lt;/a&gt; which builds a page for each article and makes the relevant article metadata available to the page. Then we can write a &lt;a href=&quot;https://github.com/cboettig/rjournal.github.io/blob/e70e84e6e53e6c04ec9864af162d7ba58439d4d5/_layouts/article.html&quot;&gt;page template&lt;/a&gt; using Liquid code to import the metadata.&lt;/p&gt;
&lt;p&gt;The really exciting thing about this is the basic idea of forking a journal website and improving it. There’s a lot we could do to improve the R journal. I think that ideally we’d have HTML5 versions of the articles as well, something that would be straight forward if authors used knitr of course, but I realize that’s a bigger shift. One could also automatically enhance the HTML with quite a bit more semantic content, we could have animations, links the Rnw files, etc etc. I’d be happy to help and I’m sure others would too. R-journal is great but it could be so much more as an example and test-bed of innovation in statistical publishing.&lt;/p&gt;
&lt;p&gt;Well, now that the journal is open for pull requests, hack away!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reviewing Software Revisited</title>
   <link href="/2013/07/09/reviewing-software-revisited.html"/>
   <updated>2013-07-09T00:00:00+00:00</updated>
   <id>/2013/07/09/reviewing-software-revisited</id>
   <content type="html">&lt;p&gt;Several weeks ago I wrote a &lt;a href=&quot;/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;post&lt;/a&gt; describing issues I commonly raise when reviewing software papers. This provoked a lively discussion among authors, editors, and other reviewers of software papers (most of us having been on at least both reviewing and authoring end), with quite varied perspectives as to whether such criteria were appropriate in this context. As I describe there, I view the concept of dedicated software papers as a somewhat necessary “hack” of the publishing system, and hope a more mature system will eventually come into place, as is now happening for data through efforts such as &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt; and associated journal archiving requirements.&lt;/p&gt;
&lt;p&gt;Sustainability of the software is important for the same reason archiving the literature is important – without this, we cannot build on existing work. Building on “researcher/developer” software (e.g. the kind of software typically covered in ‘software papers’) is currently an uncommon and risky, as eloquently argued by Brian O’Meara in the comments of &lt;a href=&quot;http://carlboettiger.info/2013/04/23/we-need-more-object-oriented-design-in-comparative-methods.html&quot;&gt;an earlier post&lt;/a&gt; describing just such a failure.&lt;/p&gt;
&lt;p&gt;Through that discussion, I’ve come to see the goal of reviewing software papers as one of software sustainability. After all, the publication becomes a part of the researcher’s permanent record and part of the permanent scholarly archive, designed to outlive the journal itself. It seems reasonable that the software it describes should at least exist and do something five years later. Though this sentiment underlies most of my criteria, my phrasing is often too specific and a little too grounded in language explicit to software development.&lt;/p&gt;
&lt;p&gt;As I learned from Neil Chue Hong (&lt;a href=&quot;http://www.software.ac.uk/&quot;&gt;Software Sustainability Institute&lt;/a&gt;) in a follow up from that discussion, The Journal of Open Research Software has recently revised its guidelines making much of this more explicit and much better worded than I did. They provide separate lists for the reviewer to comment on both the paper and the software – making it clear that the review goes beyond the paper. The questions for the paper are every bit as valuable as those from the software section: for instance, reference to sections on “Reuse”, “Quality Control”, and “Implementation and Architecture”. While these things might not get their own section headings in an MEE paper, the ideas should certainly be addressed. I think these guidelines provide an excellent checklist for promoting sustainable software without putting undo emphasis on questions of performance, scalability, extensibility, etc.&lt;/p&gt;
&lt;h2 id=&quot;journal-of-open-research-softwares-guidelines&quot;&gt;Journal of Open Research Software’s Guidelines&lt;/h2&gt;
&lt;p&gt;(content below &lt;a href=&quot;http://creativecommons.org/licenses/by/3.0/&quot;&gt;CC-BY&lt;/a&gt;, quoted from &lt;a href=&quot;http://openresearchsoftware.metajnl.com/&quot;&gt;Journal of Open Research Software&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please provide a list of your recommendations, indicating which are compulsory for acceptance and which are optional but would improve the quality of the paper or the reusability of the software.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-paper&quot;&gt;The paper&lt;/h3&gt;
&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;Is the title of the paper descriptive and objective?&lt;/li&gt;
&lt;li&gt;Does the Abstract give an indication of the software’s functionality, and where it would be used?&lt;/li&gt;
&lt;li&gt;Do the keywords enable a reader to search for the software?&lt;/li&gt;
&lt;li&gt;Does the Introduction give enough background information to understand the context of the software’s development and use?&lt;/li&gt;
&lt;li&gt;Does the Implementation and Architecture section give enough information to get an idea of how the software is designed, and any constraints that may be placed on its use?&lt;/li&gt;
&lt;li&gt;Does the Quality Control section adequately explain how the software results can be trusted?&lt;/li&gt;
&lt;li&gt;Does the Reuse section provide concrete and useful suggestions for reuse of the software, for instance: other potential applications, ways of extending or modifying the software, integration with other software?&lt;/li&gt;
&lt;li&gt;Are figures and diagrams used to enhance the description? Are they clear and meaningful?&lt;/li&gt;
&lt;li&gt;Do you believe that another researcher could take the software and use it, or take the software and build on it?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;the-software&quot;&gt;The software&lt;/h3&gt;
&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;Is the software in a suitable repository? (see &lt;a href=&quot;http://openresearchsoftware.metajnl.com/about/repositories/&quot;&gt;http://openresearchsoftware.metajnl.com/about/repositories/&lt;/a&gt; for more information)&lt;/li&gt;
&lt;li&gt;Does the software have a suitable open licence? (see &lt;a href=&quot;http://openresearchsoftware.metajnl.com/faq/#q5&quot;&gt;http://openresearchsoftware.metajnl.com/faq/#q5&lt;/a&gt; for more information)&lt;/li&gt;
&lt;li&gt;If the Archive section is filled out, is the link in the form of a persistent identifier, e.g. a DOI? Can you download the software from this link?&lt;/li&gt;
&lt;li&gt;If the Code repository section is filled out, does the identifier link to the appropriate place to download the source code? Can you download the source code from this link?&lt;/li&gt;
&lt;li&gt;Is the software license included in the software in the repository? Is it included in the source code?&lt;/li&gt;
&lt;li&gt;Is sample input and output data provided with the software?&lt;/li&gt;
&lt;li&gt;Is the code adequately documented? Can a reader understand how to build/deploy/install/run the software, and identify whether the software is operating as expected?&lt;/li&gt;
&lt;li&gt;Does the software run on the systems specified? (if you do not have access to a system with the prerequisite requirements, let us know)&lt;/li&gt;
&lt;li&gt;Is it obvious what the support mechanisms for the software are?&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Like my own list, a repository, a license, and sample input-output data are all mentioned explicitly. Software review section suggests there should be a mechanism in place to “identify whether the software is operating as expected” – such as basic unit tests. Extensibility is also mentioned, but without any reference to features that may or may not assist in that – a fault of my own list, which was probably over-specific on that account. I think these are reasonable and solid guidelines, and would love to see other journals that frequently publish software papers (Looking at you, MEE, JSS, Bioinformatics, R Journal, PLoS Comp Bio) adopt similar criteria.&lt;/p&gt;
&lt;p&gt;Of course I’d love to hear what others think about this.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>What I look for in 'Software Papers'</title>
   <link href="/2013/06/13/what-I-look-for-in-software-papers.html"/>
   <updated>2013-06-13T00:00:00+00:00</updated>
   <id>/2013/06/13/what-I-look-for-in-software-papers</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; Thanks to the rich discussion in the comments and beyond, I’ve revised my thoughts on this somewhat, as I discuss in &lt;a href=&quot;/2013/07/09/reviewing-software-revisited.html&quot;&gt;this more recent post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am more and more frequently reviewing ‘software papers:’ which I define as publications whose primary purpose is to publicize a piece of scientific software and provide a traditional research product with hopes that it will receive citations and recognition from other researchers in grant and job reviews. To me this feels very much like hacking the publication recognition system rather than the ideal way to recognize and track the role of software in research communities, but a very practical one in the current climate. I have written two myself, so I have been on both ends of this issue. In this post, I share what I look for in such papers and what omissions I see most frequently.&lt;/p&gt;
&lt;h2 id=&quot;reviewing-software&quot;&gt;Reviewing software&lt;/h2&gt;
&lt;p&gt;If we are going to employ this hack of the publication system for software, we should at least use it to maximal advantage. As a reviewer, I feel that means reviewing not just the submitted manuscript but the software itself. If we can agree on nothing else, we as a community should at least be able to say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;my review of a software paper is a review of the software&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I assume most other authors, reviewers and editors on this content share this implicit assumption, but I’d love to hear first hand from anyone else. For instance: as an editor, what would you do if your reviewers only commented on the paper directly and not on the software distributed?&lt;/p&gt;
&lt;p&gt;We are not really taught to review software, any more than we are taught to write it in the first place. Most journals offer little guidance on this (though see the Journal of Open Research Software &lt;a href=&quot;http://openresearchsoftware.metajnl.com/peer-review/&quot;&gt;guidelines for peer review of software&lt;/a&gt;, all though they are still rather minimal.) In the absence of a culture on software reviewing, I thought I would lay out my own perspective with the hope of hearing feedback and push back from others. Perhaps through such dialogs we can develop clearer expectations for this rapidly expanding genre.&lt;/p&gt;
&lt;h2 id=&quot;reviewing-the-software-paper&quot;&gt;Reviewing the software paper&lt;/h2&gt;
&lt;p&gt;I don’t include “to document” the software as a purpose, since none do so very comprehensively, and besides, documentation belongs in the software, not in a journal. “Publicize” usually includes some motivating examples that could convince many readers that the software does something useful for them without too much effort. As such, I expect the paper to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;provide the journal’s audience with a clear motivation for why the package is useful, * and have at least one functioning “wow” example that I can run (by copy-paste) and understand without difficulty (e.g. without referring to code comments or the package manual to understand the function calls and their arguments).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an intentionally low bar that I hope helps promote these kinds of contributions. Despite this, papers frequently fail the copy-paste test or the plain text explanations of the function calls. Come on people. Meanwhile, I try to focus the rest of my review on the software itself.&lt;/p&gt;
&lt;h2 id=&quot;my-partial-list-of-criteria&quot;&gt;My partial list of criteria&lt;/h2&gt;
&lt;p&gt;As I am almost always reviewing R packages, the software already meets some very basic standards required by submission to CRAN: dependencies and license stated, built-in documentation, passing some minimal automatic checks, etc. (See the &lt;a href=&quot;http://cran.r-project.org/web/packages/policies.html&quot;&gt;CRAN Policies&lt;/a&gt; and the &lt;a href=&quot;http://cran.r-project.org/doc/manuals/R-exts.html&quot;&gt;Writing R Extensions Manual&lt;/a&gt; for details). This is great, as it clears the first few hurdles of installation, etc. without much fuss, but still provides a bar that is by itself unacceptably low for published scientific software. Here is a list of the things I see that most often frustrate me. This isn’t intended as a style-guide or a comprehensive list of best practices, just my own pet peeves. I have somewhat tongue-in-cheek labeled them by severity of the review I might give; which like any other use of these terms is more of a measure of how annoyed I am then anything else. Critiques and suggestions welcome.&lt;/p&gt;
&lt;h3 id=&quot;edit&quot;&gt;Edit:&lt;/h3&gt;
&lt;p&gt;The comments from other reviewers, authors, and editors have been fantastic, thank you all. I particularly appreciate the opportunity to have reviewing styles critiqued, something that does not happen in normal peer review.&lt;/p&gt;
&lt;p&gt;Just a note on my headings here. I do not see any of these things as “gatekeeping requirements” and have intentionally omitted the option of “&lt;em&gt;Reject&lt;/em&gt;”. I would reject such a paper for methodological flaws, etc., but not for any of the reasons on my list below. The list is intended only to improve, not prevent, software publication.&lt;/p&gt;
&lt;p&gt;I believe any of the decisions below typically result in a revision to the same journal, that authors judiciously choose how to respond to reviewer comments guided by the editor’s own feedback, and that it is ultimately the editor’s decision whether any of this is relevant. &lt;code&gt;&amp;lt;/edit&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;reject-and-resubmit&quot;&gt;“Reject and resubmit”&lt;/h2&gt;
&lt;h3 id=&quot;automatic-tests&quot;&gt;Automatic tests&lt;/h3&gt;
&lt;p&gt;A scientific R package &lt;em&gt;must must must&lt;/em&gt; have some automated tests that are run by &lt;code&gt;R CMD check&lt;/code&gt;. Even if further development of the package doesn’t break anything (most likely only if further development doesn’t happen), changes to the package dependencies could still break things, and so it is crucial to have a mechanism in place to detect these problems when they arise. For code that runs quickly, the simplest way to do this is through the examples in the documentation. I don’t expect all scientific software to have a complete test suite with 100% coverage covering all the weird things that can happen if a user passes in a matrix when the function expects a data frame or has some unanticipated missing values, etc. Just some tests to make sure the basic examples execute and I’ll be happy. Longer running functions or those that need calls to external web resources shouldn’t be run in the examples (too much of a burden for CRAN’s automatic testing) so they should be marked &lt;code&gt;dontrun&lt;/code&gt; and put in a separate test suite or vignette as it says in the manual.&lt;/p&gt;
&lt;h3 id=&quot;passing-optional-arguments&quot;&gt;Passing optional arguments&lt;/h3&gt;
&lt;p&gt;I see authors write functions like this all the time:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;f &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;myfunction&lt;/span&gt;(f, p){ 
  &lt;span class=&quot;co&quot;&gt;#  stuff&lt;/span&gt;
  o &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;optim&lt;/span&gt;(f, p)
  &lt;span class=&quot;co&quot;&gt;#  stuff&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;calling an existing library function like &lt;code&gt;optim&lt;/code&gt; that has a whole host of very useful optional arguments that have a significant impact on how the algorithm functions. Whenever you a rich function like &lt;code&gt;optim&lt;/code&gt;, please have the courtesy to make it’s arguments available to future users and developers through your function call. Yes, most users will just want the default arguments, (or your default arguments, if different), and that can be handled just fine by providing default values as optional arguments. R has a fantastic mechanism for this exact issue: the &lt;code&gt;...&lt;/code&gt; argument. The above code could be fixed simply by using:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;f &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;myfunction&lt;/span&gt;(f, p, ...){ 
  &lt;span class=&quot;co&quot;&gt;#  stuff&lt;/span&gt;
  o &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;optim&lt;/span&gt;(f, p, ...)
  &lt;span class=&quot;co&quot;&gt;#  stuff&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which works just they way you think it would. If you have more than one such function (ask yourself if you can write shorter functions first and then) pass optional arguments as lists,&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;f &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;myfunction&lt;/span&gt;(f, p, optim_options, fn2_options){
  &lt;span class=&quot;co&quot;&gt;# stuff&lt;/span&gt;
  o &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;do.call&lt;/span&gt;(optim, &lt;span class=&quot;kw&quot;&gt;as.list&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(f, p, optim_options)))
  &lt;span class=&quot;co&quot;&gt;# stuff&lt;/span&gt;
  b &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;do.call&lt;/span&gt;(fn2, fn2_options)
  &lt;span class=&quot;co&quot;&gt;# stuff &lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;arguments can also be extracted with &lt;code&gt;list(...)$arg1&lt;/code&gt; etc.&lt;/p&gt;
&lt;p&gt;A converse of this issue is not providing default arguments where it might be natural to do so. This does not bother me so much, as it is probably useful to force the user to think how many iterations &lt;code&gt;n&lt;/code&gt; are appropriate for their problem rather than just assuming that &lt;code&gt;100&lt;/code&gt; is good because it is the default. The only time this case is annoying is when the argument will not be changing – such as a user’s authentication token to access a web resource. Don’t make me manually pass the token to every function in the library please.&lt;/p&gt;
&lt;h3 id=&quot;development-site-and-bug-tracker&quot;&gt;Development site and bug tracker&lt;/h3&gt;
&lt;p&gt;I would really like to see a link to the software development page, such as r-forge or Github. The primary asset in this context is pointing reviewers to an address with a bug tracking system where issues can be assigned ticket numbers and readers can transparently see if a package is being actively maintained. A reader who comes across the paper years later who has only an email address that may or may not work has little way to determine what the latest version of the code is, whether it is actively maintained, or whether earlier versions that may have been in used in previous publications suffered from any significant bugs.&lt;/p&gt;
&lt;h3 id=&quot;cite-your-dependencies&quot;&gt;Cite your dependencies!&lt;/h3&gt;
&lt;p&gt;We write software papers with the sometimes vain hope that they will be cited by users, so authors of such papers should at least follow these best practices themselves. R includes a native mechanism for providing citations to packages, &lt;code&gt;citation(packagename)&lt;/code&gt;, including the information for any software paper published along with it. Be sure to add your own software papers to the &lt;code&gt;CITATION&lt;/code&gt; file. More information can be found in my post on &lt;a href=&quot;http://purl.org/cboettig/2012/03/20/citing-r-packages.html&quot;&gt;Citing R packages&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;major-revisions&quot;&gt;“Major Revisions”&lt;/h2&gt;
&lt;p&gt;These are other things that commonly frustrate me, but fall on a bit more of a continuum of style rather than gross oversights. As such I’m not sure that any one of these things would justify rejection.&lt;/p&gt;
&lt;h3 id=&quot;functionalize-the-code&quot;&gt;Functionalize the code&lt;/h3&gt;
&lt;p&gt;Style guides will tell you to keep functions short, not more than a screen or 20 lines. Breaking large functions into a series of smaller functions and documenting those smaller functions – even if they are only used internally – is a great help to a reviewer trying to understand what a function is supposed to do and also test that it does what it says. Anyone building the code base later (most often yourself) will appreciate the reusable modules.&lt;/p&gt;
&lt;h3 id=&quot;stable-clean-and-complete-return-objects&quot;&gt;Stable, clean, and complete return objects&lt;/h3&gt;
&lt;p&gt;An extension of providing optional arguments to functions is to also provide access to all of their return information. To extend the example from wrapping &lt;code&gt;optim&lt;/code&gt;, this would involve returning the convergence information. Using object classes and helper functions for return objects helps keep code stable and lets users leverage existing code for similar objects, such as fitting or plotting routines. More discussion on this topic based on my own experiences in the post, &lt;a href=&quot;http://carlboettiger.info/2013/04/23/we-need-more-object-oriented-design-in-comparative-methods.html#comment-878249659&quot;&gt;we need more object oriented design in comparative methods&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;state-a-license&quot;&gt;State a license&lt;/h3&gt;
&lt;p&gt;Because CRAN requires this through the DESCRIPTION file, R package authors rarely neglect this entirely. A sometimes misconception is that because R itself is primarily dual-licensed under GPL-2 and GPL-3 that R packages must use a GPL license due to the “viral” clause of the GPL. This clause only applies if you are modifying existing GPL functions directly and is not a requirement for R packages, which recognize a large array of licenses. My own recommendation for authors seeking to maximize the impact of their work is to use MIT, BSD (2 clause), or CC0 license for the package. CC0 has the advantage of being suitable for and data or documentation included, but authors should do there homework and decide what is best for them.&lt;/p&gt;
&lt;h2 id=&quot;minor-revisions&quot;&gt;“Minor Revisions”&lt;/h2&gt;
&lt;p&gt;Consistent use of coding style, good documentation, clear examples, intelligent reuse of code, and other best practices are all areas in which any work could improve. While we can all become better developers by highlighting these issues in our reviews, they are probably best developed over time and in dialog with the user community. I also put anything extending the scope of functionality into this category – I do not have any concept of minimal contribution as long as the code meets the criteria above. Meanwhile, there’s always a few pet peeves I just cannot help mentioning. Here’s one which is particular to R packages and so commonly overlooked.&lt;/p&gt;
&lt;h3 id=&quot;imports-not-depends&quot;&gt;IMPORTS not DEPENDS&lt;/h3&gt;
&lt;p&gt;Many developers overlook that package dependencies that provide functions your functions will use internally should be listed as under IMPORTS rather than DEPENDS. This keeps the users namespace cleaner and avoids collisions of functions having the same name. Use DEPENDS only for those packages whose functions will be used by the end user as well.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If you are an author, editor, or reviewer of R software packages, what are your pet peeves?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>manuscript reviews on github?</title>
   <link href="/2013/06/10/mansucript-reviews-on-github.html"/>
   <updated>2013-06-10T00:00:00+00:00</updated>
   <id>/2013/06/10/mansucript-reviews-on-github</id>
   <content type="html">&lt;p&gt;I was recently impressed to learn of Trevor Bedford’s strategy of seeking &lt;a href=&quot;https://twitter.com/trvrb/status/334310856982671361&quot;&gt;pre-approval&lt;/a&gt; for posting his reviewer’s comments as Github issues. Beyond providing links to the data and source-code, I generally don’t advertise the open science nature of papers I submit – I guess I assume that if the reader or reviewers care, it should be easy enough for them to discover it. Consequently I am usually immediately frustrated to realize that upon receiving my reviews I have to create a second, private repository for the review material, our replies to reviewers, etc., as I don’t have permission to disclose that information. &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; I have recently stumbled across &lt;a href=&quot;http://www.steinsaltz.me.uk/pnas.html&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;http://theseamonster.net/2013/05/are-unreasonably-harsh-reviewers-retarding-the-pace-of-coral-reef-science/&quot;&gt;examples&lt;/a&gt; of authors publishing to the web anonymous reviews they have received. Though anonymous, I feel the practice potentially murky without explicit permission, so I would appreciate any insight others have on this.&lt;/p&gt;
&lt;h3 id=&quot;asking-permission&quot;&gt;Asking permission&lt;/h3&gt;
&lt;p&gt;Trevor’s approach suggests I should consider broaching this question when first submitting my review, so I am puzzling over the best way to do so. One option would be to include such a request in the cover letter. For example,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The authors of this manuscript would like to request that the editor and reviewers indicate in their replies if they consent or decline to have their comments posted anonymously in the public &lt;a href=&quot;#&quot;&gt;Issues Tracker&lt;/a&gt; of this paper.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Does that need more explanation? A link to examples like &lt;a href=&quot;https://github.com/trvrb/flux/issues?labels=reviewer+1&quot;&gt;Trevor’s&lt;/a&gt; that have done this before? Do I need to explain the value of having this kind of transparent provenance for the paper? Should I mention how this could give the reviewer more transparent credit? Encourage them to comment directly on Github from their own account?&lt;/p&gt;
&lt;p&gt;Does this need the blessing of the journal? How would you feel about such a clause as a reviewer or editor? For a recent review I had done of a paper that was similarly written on Github, I obtained the Journal’s permission to post my review as an &lt;a href=&quot;https://github.com/weecology/data-sharing-paper/issues/71&quot;&gt;issue&lt;/a&gt; in their repository. I would love to see more examples of this kind of thing, if anyone has come across them.&lt;/p&gt;
&lt;h2 id=&quot;cover-letters-for-open-science-manuscripts&quot;&gt;Cover letters for open science manuscripts?&lt;/h2&gt;
&lt;p&gt;While I lean towards a minimal statement such as the one above, perhaps a cover letter would be a good place to document some of the other open and reproducible features of the manuscript? Or perhaps such statements should be added to the manuscript itself? Among the options, I might point out:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The manuscript has been written on Github. Consequently the full drafting and &lt;strong&gt;revision history&lt;/strong&gt; is available, along with graphs of author contributions (which omit authors without Github accounts and may be distorted by trivial line changes)&lt;/li&gt;
&lt;li&gt;The manuscript has been written with all the code necessary to repoduce the results embedded as a &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; &lt;strong&gt;dynamic document&lt;/strong&gt;. This helps ensure the analysis is always in synch with the results presented in the manuscript and the that the research is reproducible. The analysis, figures, and manuscript can be reassembled from scratch by typing &lt;code&gt;make pdf&lt;/code&gt; in the repository directory.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt; to replicate the analysis and produce each of the figures shown can be found at: (Version-stable lnk to the appropriate Github pages? Deposit in Figshare/Dryad first?)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt; to replicate the analysis and data shown in each of the figures can be found at: (Easiest to link to Github, since the code and data already reside there.&lt;br /&gt;&lt;em&gt;Alternatively I could deposit these in &lt;a href=&quot;http://figshare.com&quot;&gt;Figshare&lt;/a&gt; or &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt; first…)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The manuscript, code, data, and documentation are available &lt;strong&gt;as an R package in the Github repository&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;issues tracker&lt;/strong&gt; associated with the manuscript’s repository provides a record of this research, including lines of investigation that were resolved into the results presented here, lines that were closed as dead-ends or null results, and outstanding issues for further investigation.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;daily lab notebook entries&lt;/strong&gt; accompanying this research can be found under the &lt;a href=&quot;/tags&quot;&gt;project-tag&lt;/a&gt; between dates of XX and XX.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Listing all of these would make for a somewhat lengthy cover letter, which might be a bit overwhelming to be useful (or seem more promotional than valuable). Are any of the seven things above worth highlighting in particular?&lt;/p&gt;
&lt;p&gt;Perhaps these details could be deferred to a README file in the project’s Github repo, and the cover letter could simply provide a link to the project repository? What, if anything, would appear most useful and accessible to a reviewer unfamiliar with this approach or its potential value? Though elements of this approach have been discussed in the published literature, e.g. &lt;span class=&quot;showtooltip&quot; title=&quot;Gentleman R and Temple Lang D (2007). Statistical Analyses And
Reproducible Research. _Journal of Computational And Graphical
Statistics_, *16*. ISSN 1061-8600, 
http://dx.doi.org/10.1198/106186007X178663.&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1198/106186007X178663&quot; rel=&quot;http://purl.org/spar/cito/citesAsEvidence&quot; &gt;Gentleman &amp;amp; Temple Lang (2007)&lt;/a&gt;&lt;/span&gt; ‘compendium’ concept, &lt;span class=&quot;showtooltip&quot; title=&quot;Stodden V (2009). Enabling Reproducible Research: Open Licensing
for Scientific Innovation. 
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1362040. 
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1362040.&quot;&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1362040&quot; rel=&quot;http://purl.org/spar/cito/citesAsEvidence&quot; &gt;Stodden (2009)&lt;/a&gt;&lt;/span&gt; RRS concept, or &lt;span class=&quot;showtooltip&quot; title=&quot;Peng R (2011). Reproducible Research in Computational Science.
_Science_, *334*. ISSN 0036-8075, 
http://dx.doi.org/10.1126/science.1213847.&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1126/science.1213847&quot; rel=&quot;http://purl.org/spar/cito/citesAsEvidence&quot; &gt;Peng (2011)&lt;/a&gt;&lt;/span&gt; reproducible papers in the Journal of Biostatistics, I’m unsure if pointing a reviewer to these references would be more valuable or more confusing. Let me know what you think.&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Robert Gentleman, Duncan Temple Lang, (2007) Statistical Analyses And Reproducible Research. &lt;em&gt;Journal of Computational And Graphical Statistics&lt;/em&gt; &lt;strong&gt;16&lt;/strong&gt; &lt;a href=&quot;http://dx.doi.org/10.1198/106186007X178663&quot;&gt;10.1198/106186007X178663&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;R. D. Peng, (2011) Reproducible Research in Computational Science. &lt;em&gt;Science&lt;/em&gt; &lt;strong&gt;334&lt;/strong&gt; &lt;a href=&quot;http://dx.doi.org/10.1126/science.1213847&quot;&gt;10.1126/science.1213847&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Victoria Stodden, (2009) Enabling Reproducible Research: Open Licensing for Scientific Innovation. &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1362040&quot;&gt;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1362040&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Strictly speaking the edits to the manuscript in the open repository could also be considered confidential, though at that stage I haven’t yet signed the copyright agreements that come with publication, which tend to be quite reasonable even for the traditional subscription based journals I work with&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>DOI != citable</title>
   <link href="/2013/06/03/DOI-citable.html"/>
   <updated>2013-06-03T00:00:00+00:00</updated>
   <id>/2013/06/03/DOI-citable</id>
   <content type="html">&lt;p&gt;I feel I see this kind of comment almost daily:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;&lt;p&gt;
Is there a way to obtain DOI for a &lt;a href=&quot;https://twitter.com/github&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;github&quot;&gt;@github&lt;/span&gt;&lt;/a&gt; repository? (for citing &lt;a href=&quot;https://twitter.com/search?q=%23opensource&amp;amp;src=hash&quot;&gt;#opensource&lt;/a&gt; software packages, similar to &lt;a href=&quot;https://twitter.com/figshare&quot;&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;figshare&quot;&gt;@figshare&lt;/span&gt;&lt;/a&gt; objects) &lt;a href=&quot;https://twitter.com/search?q=%23git&amp;amp;src=hash&quot;&gt;#git&lt;/a&gt;
&lt;/p&gt;
— Ahmed Moustafa (&lt;span class=&quot;citation&quot; data-cites=&quot;AhmedMoustafa&quot;&gt;@AhmedMoustafa&lt;/span&gt;) &lt;a href=&quot;https://twitter.com/AhmedMoustafa/statuses/339727912896954369&quot;&gt;May 29, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Again and again, researchers suggest that DOI to makes something “citable”. And this &lt;a href=&quot;https://twitter.com/cboettig/status/337986074624282624&quot;&gt;frustrates me&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Don’t get me wrong. I love DOIs, and I love CrossRef. And I bang on the table when I have some old journal article that doesn’t yet have a DOI. I use DOIs every day in many ways. I use CrossRef’s APIs all the time to draw in metadata for citations in my notebook (through my &lt;a href=&quot;http://github.com/cboettig/knitcitations&quot;&gt;knitcitations&lt;/a&gt; package), and to import metadata into my reference manager, Mendeley. I’ve written my own implementations in R and ruby, and keep an eye on their exciting new tools on the &lt;a href=&quot;https://github.com/crossref&quot;&gt;Crossref Github page&lt;/a&gt;. I wrote to bibsonomy when I realized they were not using the CrossRef API to look up metadata by DOIs, and they have now implemented this feature. I use DOIs to look up papers I’ve come across, and to share content I am reading. (Crossref’s &lt;a href=&quot;http://shortdoi.org/&quot;&gt;DOI shortener&lt;/a&gt; is great for this). I even use DOI-based links to &lt;a href=&quot;http://carlboettiger.info/2013/02/22/semantic-citations-for-the-notebook-and-knitr.html&quot;&gt;embed semantic information&lt;/a&gt; into links and citations of articles.&lt;/p&gt;
&lt;p&gt;But I still have no idea what researchers mean when they suggest that this makes something &lt;em&gt;citable&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&quot;some-background-on-dois&quot;&gt;Some background on DOIs&lt;/h3&gt;
&lt;p&gt;At its heart, a DOI is a very simple concept. It is a “permanent identifier”. All this means is that is is really just a URL redirect. Type http://dx.doi.org/mnn into any browser and get redirected to where the article actually lives. Why does that make it permanent? Because if the journal decides to change their URL structure, the DOI’s redirect can just be mapped to the new address and voila, it still works. That is, a DOI is simply a tool to fight &lt;a href=&quot;https://en.wikipedia.org/wiki/Link_rot&quot;&gt;link-rot&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So you might ask, why does the ability to remap the address have anything to do with being “permanent?” It doesn’t, really. The permanence comes not so much from the technology as from the social contract that goes with it. As CrossRef’s &lt;a href=&quot;http://blogs.plos.org/mfenner/2009/02/17/interview_with_geoffrey_bilder/&quot;&gt;Geoffery Bilder eloquently explains&lt;/a&gt;, a publisher can only receive DOIs if they promise to keep these redirects up-to-date. A publisher who fails to maintain this responsibility would presumably lose their right to receive DOIs. A brilliant, simple, social incentive.&lt;/p&gt;
&lt;p&gt;This still does not guarantee permanence – e.g. what would happen to the content if the publisher disappears. That problem is not addressed by the DOI technology itself, but by a robust backup archiving solution, such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;, which provides a geo-politically distributed network of backup copies for many journals. Again the social contract comes into play – presumably CrossRef would not provide a publisher with DOIs if they did not have such a robust archival solution in place.&lt;/p&gt;
&lt;p&gt;So far we have seen two crucial functions of the DOI – as a permanent identifier that can be used to reach the content despite link rot, and as an incentive to maintain good archival backups of the content and the links to it.&lt;/p&gt;
&lt;h3 id=&quot;what-do-we-mean-by-citations-anyway&quot;&gt;What do we mean by citations, anyway?&lt;/h3&gt;
&lt;p&gt;So what does this have to do with being citable? Obviously these are nice properties to have for things we cite – but they are by no means a requirement. (As &lt;a href=&quot;https://twitter.com/noamross/status/337987521243918337&quot;&gt;Noam Ross observes&lt;/a&gt;, try finding a permanent identifier for “Personal Communication”). Books, reports, and other grey literature frequently appear in citations, as do links to websites. MLA even has guidelines on the proper format to &lt;a href=&quot;http://www.mla.org/style/handbook_faq/cite_a_tweet&quot;&gt;cite a tweet&lt;/a&gt; (which, incidentally, come closer to having a permanent identifier and an archival strategy than most other things in this list). So what do we mean by citable anyway?&lt;/p&gt;
&lt;p&gt;But what about the reference list? While a publisher may be just fine including some link to your software, is it really cited if it isn’t in the reference list? Journals restrict what appears in the reference list because these references are indexed by the infamous citation counters like Thompson-Reuters. (A frequent complaint is that many journals do not similarly index citations appearing in the reference list of the supplementary materials, making it difficult or impossible to give appropriate attribution to large numbers of data providers, for instance). Does having a DOI address this problem?&lt;/p&gt;
&lt;h4 id=&quot;citation-counts-in-dois&quot;&gt;Citation counts in DOIs&lt;/h4&gt;
&lt;p&gt;Counting citations depends on who is counting them. The most well-known is Thompson-Reuters, which has their own process for deciding what gets counted (based on publisher), so no guarantee there. Meanwhile Google Scholar counts anything meeting it’s &lt;a href=&quot;http://carlboettiger.info/2012/11/23/citing-lab-notebook-entries.html&quot;&gt;indexing requirements &amp;amp; arbitrary selection&lt;/a&gt;. I have recently learned that CrossRef just launched it’s own &lt;a href=&quot;https://github.com/articlemetrics/alm/wiki/Crossref&quot;&gt;internal citation counting&lt;/a&gt;, which is available from the CrossRef metadata (totals only for the public, publishers can resolve which articles did the citing…). However, most proposals to make some alternative research product “citable” by giving it a DOI use DataCite DOIs (e.g. fig&lt;strong&gt;share&lt;/strong&gt;, PeerJ Preprints), which lag behind in this feature. Moving the control of citation data beyond the grasp of particular publishing companies like TR is undoubtedly an important step forward. The &lt;a href=&quot;http://www.jisc.ac.uk/whatwedo/programmes/inf11/jiscexpo/jiscopencitation.aspx&quot;&gt;Open Citation Project&lt;/a&gt; is a more comprehensive, if very young, move in this direct. (Hat tip to Martin Fenner for explaining CrossRef citations to me).&lt;/p&gt;
&lt;h3 id=&quot;additional-metadata&quot;&gt;Additional Metadata&lt;/h3&gt;
&lt;p&gt;In addition to resolving links, DOI providers also serve a rich collection of metadata about the publication that can be &lt;a href=&quot;http://www.crosscite.org/cn/&quot;&gt;queried by DOI&lt;/a&gt; or by &lt;a href=&quot;https://github.com/CrossRef/cr-search&quot;&gt;other elements&lt;/a&gt; like author and title. Rich semantic formats and disambiguation of author names by connections to ORCID IDs are among the many advantages of this. Because many of these tools are publicly accessible by through their APIs, it is easy for other developers to build services upon them.&lt;/p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;While DOI providers have done an excellent job in ensuring persistent URLs, archived content, and valuable metadata, these things are largely the product of the social contract between publisher and the DOI provider. It is not possible for an author or organization to simply “get DOIs” for all their content. But it is not the only way to provide these features, either. While I understand the value in providing a simple and reliable way to encapsulate each of these concepts as “has a DOI,” it also appears to put these features beyond the reach of individual researchers. If issues of persistent URLs, archived content, and rich metadata tools are always reduced to “has a DOI,” publishers become the only path to achieve these ends. On the contrary, a rich collection of tools is available to researchers.&lt;/p&gt;
&lt;p&gt;So what do we mean when we say a DOI makes something ‘citable?’ If this is shorthand for the properties we would want in something citable: persistent identifier, archival content, machine-readable metadata, than we should start to recognize other things that share these features. Further innovation requires valuing the features the DOI provides, not simply a “brand name” researchers recognize.&lt;/p&gt;
&lt;h2 id=&quot;alternative-tools&quot;&gt;Alternative tools&lt;/h2&gt;
&lt;p&gt;In a &lt;a href=&quot;http://purl.org/cboettig/2013/05/31/notebook-features-digital-archiving&quot;&gt;recent post&lt;/a&gt; in a series on technical features of my open notebook, I discuss some of the tools available to address these challenges. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The use of &lt;a href=&quot;http://en.wikipedia.org/wiki/Persistent_uniform_resource_locator&quot;&gt;PURLs&lt;/a&gt; for persistent identifiers&lt;/li&gt;
&lt;li&gt;Git for archival redundancy&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://greycite.knowledgeblog.org&quot;&gt;Greycite&lt;/a&gt; for metadata extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, if you ever need a DOI for a research product, there is always &lt;a href=&quot;http://figshare.com&quot;&gt;figshare&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook features: digital archiving</title>
   <link href="/2013/05/31/notebook-features-digital-archiving.html"/>
   <updated>2013-05-31T00:00:00+00:00</updated>
   <id>/2013/05/31/notebook-features-digital-archiving</id>
   <content type="html">&lt;p&gt;Note: this entry is part of a &lt;a href=&quot;http://carlboettiger.info/2013/04/26/notebook-features-introduction.html&quot;&gt;series of posts&lt;/a&gt; which explain some of the technical features of my lab notebook.&lt;/p&gt;
&lt;p&gt;Archival preservation of digital scholarly content is an important challenge throughout the research community. As the notebook is the permanent record of my regular scholarly endeavors, this provides the opportunity to experiment with tools and techniques for digital archiving while also better preserving the notebook. In the experimental spirit that drives all my exploration here, I am testing several ways to accomplish this. In so doing, I learn which approaches are easiest to implement, to use, and gather feedback from, while also hedging my bets in the event that any given strategy should fail.&lt;/p&gt;
&lt;p&gt;Archiving digital content involves two fundamental challenges that can be difficult to satisfy simultaneously: providing a robust backup copy of the &lt;em&gt;content&lt;/em&gt;, and providing a consistent location (such as a URL) where the content can be retrieved.&lt;/p&gt;
&lt;h2 id=&quot;a-custom-domain&quot;&gt;A custom domain&lt;/h2&gt;
&lt;p&gt;The simplest archival measure employed in the notebook comes from hosting through my own domain, &lt;a href=&quot;http://carlboettiger.info&quot;&gt;carlboettiger.info&lt;/a&gt; rather than an external server. By controlling the domain structure myself, I am not tied to a University server that can be altered by an IT department without my knowledge, thereby breaking my links. When I choose to move platforms, as I did in migrating from &lt;a href=&quot;/2012/09/19/migrating-from-wordpress-to-jekyll.html&quot;&gt;Wordpress to Jekyll&lt;/a&gt;, I could ensure that links would be appropriately mapped. This was not the case when I started my open notebook on the OpenWetWare platform, since links are all mapped to the &lt;a href=&quot;http://openwetware.org&quot;&gt;openwetware.org&lt;/a&gt; domain which I obviously cannot control, even though I could at least migrate my content. &lt;a href=&quot;https://github.com/cboettig/labnotebook/blob/8481569132142c850e585a2fc8c12a671527cd4f/_plugins/redirects.rb&quot;&gt;HTML redirects&lt;/a&gt; make sure links still resolve when I change structure (e.g. &lt;a href=&quot;/archives/211&quot;&gt;carlboettiger.info/archives/211&lt;/a&gt;). I don’t have to worry about moving my domain when I change institutions, and can seamlessly migrate to a different server or DNS provider to get better rates or uptime performance.&lt;/p&gt;
&lt;p&gt;Of course these advantages are also the greatest weaknesses of this approach – they all depend entirely on me. I could make or forget to make any number of changes that could cause this all to break. Time has shown that even the best-intentioned researchers are not the best curators of there own data, and no doubt I am no exception. How can the content and its identifying addresses outlive me or my interest in it?&lt;/p&gt;
&lt;h2 id=&quot;purls-preserving-identifiers&quot;&gt;PURLs: preserving identifiers&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://purl.org&quot;&gt;PURLs&lt;/a&gt;, or Persistent Uniform Resource Locators, provide a DOI-like mechanism for addressing the challenge of link-rot. As &lt;a href=&quot;http://blogs.plos.org/mfenner/2009/02/17/interview_with_geoffrey_bilder/&quot;&gt;Geoffrey Bilder eloquently argues&lt;/a&gt;, the technological solution is quite simple, the real challenge lies on the social side of the implementation – a social contract that promises content providers will maintain their identifiers if they want to continue to receive identifiers. Though users must register to be able to create PURLs, PURL does not provide such enforcement.&lt;/p&gt;
&lt;p&gt;The PURLs solution is a bit more web-native solution than DOIs, in being more democratic, using a URL structure, and being built upon widely distributed servers and open-source web technology. Not surprisingly, other web-native systems such as most of the major semantic web ontology providers rely on PURLs, e.g. Dublin Core uses &lt;a href=&quot;http://purl.org/dc/terms/&quot;&gt;purl.org/dc/terms/&lt;/a&gt;. The PURL &lt;a href=&quot;http://purl.oclc.org/docs/faq.html&quot;&gt;FAQ&lt;/a&gt; provides a great overview.&lt;/p&gt;
&lt;p&gt;Implementing PURLs for the notebook was very straight-forward. After registering as a user at &lt;a href=&quot;http://purl.org&quot;&gt;purl.org&lt;/a&gt; I applied for my own top-level domain: &lt;code&gt;cboettig&lt;/code&gt;, which I then mapped to my current domain, &lt;a href=&quot;http://carlboettiger.info&quot;&gt;carlboettiger.info&lt;/a&gt; By enabling partial redirects, each page on my site will also be resolved using this top-level domain followed by my existing page structure. Following my existing structure is not necessary – I could map each page to an arbitrary path in my domain, but would have to enter these somewhat manually. While the partial redirect is simpler to implement, it does require that I maintain the rest of the link structure.&lt;/p&gt;
&lt;p&gt;In this way, &lt;a href=&quot;http://purl.org/cboettig/lab-notebook.html&quot;&gt;purl.org/cboettig/lab-notebook.html&lt;/a&gt; now resolves to &lt;a href=&quot;http://carlboettiger.info/lab-notebook.html&quot;&gt;carlboettiger.info/lab-notebook.html&lt;/a&gt;. Likewise, each page in the notebook can be similarly resolved from the purl.org domain instead of my personal carlboettiger.info domain. Should I ever somehow lose control of carlboettiger.info, I could re-assign my PURL to redirect to my new domain URL. This provides DOI-like technology of permanent identifiers for every page in the notebook.&lt;/p&gt;
&lt;h2 id=&quot;github-preserving-content-and-versions&quot;&gt;GitHub: preserving content and versions&lt;/h2&gt;
&lt;p&gt;Committing content to an external repository is the recommended way to avoid link-rot from the user errors and website changes that so frequently plague self-archiving of scholarly content. Keeping multiple copies of content in geographically distinct locations is the time-honored approach of digital archiving. Git and GitHub make this easy. Not only does this mean that a backup copy is publicly available and forkable online, but it is also easy to clone copies on each of the machines I work on and rely on git to keep them in sync. Should Github disappear, a little &lt;code&gt;git remote add&lt;/code&gt; and everything will be effortlessly deployed with complete history elsewhere.&lt;/p&gt;
&lt;p&gt;The notebook has two Github repositories: the “source-code” consisting of plain-text (markdown) content and Jekyll-based templates on &lt;a href=&quot;http://github.com/cboettig/labnotebook&quot;&gt;labnotebook&lt;/a&gt;, and a second for the rendered HTML &lt;a href=&quot;http://github.com/cboettig/cboettig.github.com&quot;&gt;cboettig.github.com&lt;/a&gt; (which also now hosts the website).&lt;/p&gt;
&lt;p&gt;While a custom domain and PURLs provide persistent &lt;em&gt;locators&lt;/em&gt; for the content, distributed copies on Git help archive the content itself. Should my domain vanish or Github disappear, copies of the content, complete with version history, would remain distributed across various machines with a copy of the repository. Links to Github would break in that process, unless we had remapped all links from the notebook to Github using PURLs.&lt;/p&gt;
&lt;h2 id=&quot;greycite-programmatic-access-and-indexing-of-metadata&quot;&gt;Greycite: Programmatic access and indexing of metadata&lt;/h2&gt;
&lt;p&gt;I think of good metadata as the third leg to proper digital archiving, in addition to permanent identifiers and backup of content. We want to be able to point a tool at the permanent identifier / URL of an entry and extract reliable information on the author, time published and last modified, title, author, key words, etc. that might be useful in citing or categorizing the content. Providing this information is really the subject of adding &lt;a href=&quot;http://carlboettiger.info/2012/10/23/semantic-markup-examples-for-the-lab-notebook.html&quot;&gt;Semantic metadata&lt;/a&gt; to the site, and is covered in another entry in this series. Meanwhile, the &lt;a href=&quot;http://greycite.knowledgeblog.org&quot;&gt;Greycite&lt;/a&gt; tool and it’s API are an excellent way to extract this metadata into a variety of useful formats, working much the same way that CrossRef’s tool does using DOIs. Here is an &lt;a href=&quot;http://greycite.knowledgeblog.org/?uri=http%3A%2F%2Fpurl.org%2Fcboettig%2F2012%2F10%2F23%2Fsemantic-markup-examples-for-the-lab-notebook.html&quot;&gt;example query&lt;/a&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm6.staticflickr.com/5325/8940923396_fcf4941197.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;robust-archiving-with-figshare&quot;&gt;&lt;strong&gt;Robust archiving&lt;/strong&gt; with fig&lt;strong&gt;share&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Depositing a copy of the notebook on fig&lt;strong&gt;share&lt;/strong&gt; is one of the most robust archival solutions of which I am currently aware. Not so much because it has the coveted DOI solution to the permanent identifier problem but because it has the promise of &lt;a href=&quot;http://clocks.org&quot;&gt;CLOCKSS&lt;/a&gt; archiving, should anything ever happen to fig&lt;strong&gt;share&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nevertheless, it raises several challenges. The native home for the content is as rendered HTML at my domain, not as raw HTML on an archive completely unassociated with that domain, difficult to view, and divorced from my usual workflow, unlike my usual publishing source-code to Github and website to my domain. It also raises questions of just what to archive and when. I discuss some of these strengths and challenges as a separate post, &lt;a href=&quot;http://purl.org/cboettig/2013/05/31/notebook-features-archiving-with-figshare&quot;&gt;archiving the lab notebook on figshare: advantages and challenges&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Digital archiving is a challenging problem that is not completely addressed by any one of these approaches. In the end, robust archiving will be best left in the hands of its experts. Unfortunately, the best examples currently available (such as CLOCKSS, national libraries, etc.) will not archive a researcher’s web page directly. The solutions explored here are not perfect, but they are free and simple to implement. I’d love to hear what others think.&lt;/p&gt;
&lt;h3 id=&quot;see-also&quot;&gt;See also&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.digitalpreservation.gov/ndsa/&quot;&gt;DigitalPreservation.gov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://internetarchive.org&quot;&gt;The Internet Archive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Archiving the lab notebook on figshare</title>
   <link href="/2013/05/31/notebook-features-archiving-with-figshare.html"/>
   <updated>2013-05-31T00:00:00+00:00</updated>
   <id>/2013/05/31/notebook-features-archiving-with-figshare</id>
   <content type="html">&lt;h2 id=&quot;robust-archiving-through-clockss&quot;&gt;Robust archiving through CLOCKSS&lt;/h2&gt;
&lt;p&gt;One of the most comprehensive approaches I have come across so far uses fig&lt;strong&gt;share&lt;/strong&gt;. This offers the most promising avenue for content preservation, but is weakest in managing the URIs and associating them with the original content. All fig&lt;strong&gt;share&lt;/strong&gt; content is archived by &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;, an international library cooperation providing redundant and geopolitically distributed backup of the archives around the world (and used by many academic journals, both subscription based &amp;amp; open access). Should fig&lt;strong&gt;share&lt;/strong&gt; vanish from the face of the planet, it will trigger the release of all of its content to resolve through the CLOCKSS servers, with the same appearance and resolving at the same URLs as the original figshare content. Presumably the DOIs provided to figshare content will also continue to resolve there.&lt;/p&gt;
&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;It would certainly be preferable to have the notebook archived by CLOCKSS directly, since the association between the original online content at carlboettiger.info is lost in archiving the entries on figshare. More problematically, the content as archived on fig&lt;strong&gt;share&lt;/strong&gt; is not recognized by search engines, etc., as a separate HTML pages to index, but merely as a bundle of attached text files. On the upside, the content becomes part of the global scientific datasets preserved and indexed by fig&lt;strong&gt;share&lt;/strong&gt; with appropriate metadata, etc., increasing the chances for discovery through that venue. Also, fig&lt;strong&gt;share&lt;/strong&gt; provides a convenient API that can help automate deposition.&lt;/p&gt;
&lt;h3 id=&quot;what-content-what-format&quot;&gt;What content? What format?&lt;/h3&gt;
&lt;p&gt;Deciding just what to archive in the fig&lt;strong&gt;share&lt;/strong&gt; database is also less straight forward than it may seem. I have gone through a few iterations:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Archiving the markdown.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Archiving external images with Data URIs.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Archiving the HTML versions of pages alone&lt;/li&gt;
&lt;li&gt;Archiving the whole git repository, &lt;code&gt;_site&lt;/code&gt; HTML included (?)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I began by archiving the markdown files that I write to create each entry. These are plain-text files that can be easily read in any text editor, even if the conventions for rendering them as HTML are lost. Like HTML, figures are linked to external files, and are thus not captured by this solution. To work around this, I adopted the trick of using &lt;a href=&quot;http://carlboettiger.info/2013/01/24/Data-URIs-for-image-archives.html&quot;&gt;Data URIs&lt;/a&gt; to embed images. This places a binary encoding of the image in the text itself, which can be rendered by almost any browser as the appropriate image. While this keeps the content together, the long data URI strings are rather out-of-place inside a plain text document. Further, the markdown loses all of the valuable semantics that are automatically added to each page when Jekyll renders them to HTML. As Phil Lord argues, if there’s any format that future archivists can read successfully – it will be HTML. Consequently I’ve settled on archiving the HTML versions of each notebook entry, with images embedded as Data URIs. Each HTML file contains rich metadata in the header, sidebar, and footer, that give more information about the content and its context in the notebook (relative path, categories and tags, timestamps and SHA hashes, etc). I have archived these entries in annual chunks following the year/month/day directory structure already employed on the site.&lt;/p&gt;
&lt;h3 id=&quot;how-about-site-assets&quot;&gt;How about site assets?&lt;/h3&gt;
&lt;p&gt;There is still additional external content used to render the site – CSS and javascript files – that are not captured in this approach. Though entries actually render &lt;a href=&quot;http://stackoverflow.com/questions/14046738&quot;&gt;just fine without the css&lt;/a&gt;, it would certainly be possible to include this material in the archive (though some Javascript comes from external CDNs). This does make for a bit larger and more cluttered archive, and more to the point is a rather crude solution to a problem already solved by Internet archiving programs such as CLOCKSS or internetarchive.org.&lt;/p&gt;
&lt;h3 id=&quot;versioning&quot;&gt;Versioning?&lt;/h3&gt;
&lt;p&gt;Lastly there is the concern of preserving the version history of entries. Though fig&lt;strong&gt;share&lt;/strong&gt; provides versioning of its content, this doesn’t capture finer resolution of individual page changes available through the Github repository. At the expense of creating an ever more cumbersome archival object, one could include the &lt;code&gt;.git&lt;/code&gt; history, either for the HTML rendered version (which lives at &lt;a href=&quot;https://github.com/cboettig/cboettig.github.com/&quot;&gt;cboettig.github.com&lt;/a&gt;) or the source files used to create it (&lt;a href=&quot;https://github.com/cboettig/labnotebook&quot;&gt;labnotebook&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&quot;connecting-to-the-original-instances&quot;&gt;Connecting to the original instances?&lt;/h3&gt;
&lt;p&gt;Of course this fails to address the preservation of externally linked content. The most frequent outbound links point to other publications through, usually their DOIs, which we hope will take care of themselves. The most important externally linked content in the notebook entries are the links to scripts, functions, and manuscripts in the various project repositories on Github. The simplest solution is to embed the most important scripts in the notebook entries themselves. Archiving the project repositories is an additional challenge, but if a user can recover a copy of the project repository (along with it’s &lt;code&gt;.git&lt;/code&gt; history) then it would be possible to identify the linked file using the SHA hash from these links (by matching it against the SHAs in the log). See my entry on &lt;a href=&quot;/2013/05/03/notebook-features-hashes-providing-an-immutable-and-verifiable-research-record.html&quot;&gt;SHA hashes&lt;/a&gt; for more on this topic.&lt;/p&gt;
&lt;h2 id=&quot;links-to-the-archives&quot;&gt;Links to the archives&lt;/h2&gt;
&lt;p&gt;Current and previous archives of my lab notebook can be found on figshare by year. Older versions of these archives have taken a different approach, including just archiving the markdown files. The links use the DOI and point to the most recent version. (At this time linking to explicit versions with FigShare’s DataCite DOI links doesn’t appear to be working)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dx.doi.org/10.6084/m9.figshare.96916&quot;&gt;Lab Notebook, 2010&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://dx.doi.org/10.6084/m9.figshare.96919&quot;&gt;Lab Notebook, 2011&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://dx.doi.org/10.6084/m9.figshare.106620&quot;&gt;Lab Notebook, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Comforting Trends In Scientific Software Use</title>
   <link href="/2013/05/21/comforting-trends-in-scientific-software-use.html"/>
   <updated>2013-05-21T00:00:00+00:00</updated>
   <id>/2013/05/21/comforting-trends-in-scientific-software-use</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Author’s Note&lt;/strong&gt;: Having refrained from actually posting this for 8 months, as I think I have quite mellowed more my critique. I do believe that education and peer review are the best way forward in tackling these issues, but cannot overstate how much of a long and rocky road that process will be. The Mozilla Science Foundation is really leading these efforts with their code review pilots (as I have discussed in posts since writing this), and through their work with Software Carpentry training. Still, I leave this post as a bookmark of my intial thoughts and a reminder of these challenges.&lt;/p&gt;
&lt;p&gt;Consider reading these other posts on software review and training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/09/25/mozilla-software-review.html&quot;&gt;Reflections on the Mozilla Code Review Pilot&lt;/a&gt; (phase 1)&lt;/li&gt;
&lt;li&gt;ISEES Meeting software lifecycle: &lt;a href=&quot;http://carlboettiger.info/2013/08/13/ISEES-Day-1.html&quot;&gt;Day 1&lt;/a&gt;, &lt;a href=&quot;http://carlboettiger.info/2013/08/14/ISEES-day-2.html&quot;&gt;Day 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ISEES Software training &lt;a href=&quot;http://carlboettiger.info/2013/09/10/ISEES-training-workshop-day-1.html&quot;&gt;Day 1&lt;/a&gt;, &lt;a href=&quot;http://carlboettiger.info/2013/09/11/ISEES-Workforce-Development-Day-2.html&quot;&gt;Day 2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/07/09/reviewing-software-revisited.html&quot;&gt;Reviewing Software, revisited&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;What I look for in software papers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;I have just been reading through &lt;code&gt;r citet(&amp;quot;10.1126/science.1231535&amp;quot;, &amp;quot;critiques&amp;quot;)&lt;/code&gt;. While I agree with a lot of the sentiment in this article, (we place way too little emphasis on good coding practices, validation, etc.), I actually found this article incredibly frustrating. Following a little &lt;a href=&quot;https://plus.google.com/112929796403983408632/posts/8whV6rtvsuw&quot;&gt;rant on Google+&lt;/a&gt; (G+ has to be good for something, right?) and subsequent discussion, I’ve tried formulating my thoughts into a blog post here.&lt;/p&gt;
&lt;p&gt;What first got my goat were the proposed “solutions” to the problem. I think it is hopelessly naive to write that the solution is “better computational education” and “peer review of code”.&lt;/p&gt;
&lt;h3 id=&quot;calling-for-better-education-is-a-cop-out&quot;&gt;Calling for better education is a cop-out&lt;/h3&gt;
&lt;p&gt;I completely agree that better education is sorely needed, and am delighted to see the authors cite the &lt;a href=&quot;http://softwarecarpentry.org&quot;&gt;Sofware Carpentry&lt;/a&gt; project (Though couldn’t they have made this a proper citations with a link?). However, this is an all too common cop-out of an answer that will do little to address the real problem. For decades such position papers have called for better education to address all our weaknesses – biologists should learn better mathematics, better statistics, better programming skills. They should also learn proper data management and archiving skills no doubt. While we’re at it, they should learn more about communicating science and public speaking too. What these calls to “raise standards” have in common is a &lt;strong&gt;dearth of incentives&lt;/strong&gt;. Reward these talents with jobs and advancement (publications and grants, after all, are just a means to such ends, are they not?) and the education will follow. Economics teaches us that if we want to change behavior, we change incentives. Unless this education translates into the currency of academia, we will only do our students a disservice by forcing it upon them. (I do note that Software Carpentry rightly claims that the kind of training it offers does promise to pay off in the current currency through time savings that will be realized down the line…)&lt;/p&gt;
&lt;h3 id=&quot;peer-review-of-code-is-not-the-answer&quot;&gt;Peer review of code is not the answer&lt;/h3&gt;
&lt;p&gt;If “peer review” were the ultimate solution to good code, we’d see that standard in the software industry too, wouldn’t we? Peer review actually is used in the software industry, if perhaps more the way we treat “friendly review” then with the black-and-white view we attach to peer review in the scientific literature. Peer review would never guarantee the validity of code, though it would certainly help. The real bugbear with peer review of code is just how difficult it would be to establish as a practice. The authors are silent on these challenges: organizing, incentivising, transitioning to, and paying for such a system would all be major hurdles to overcome.&lt;/p&gt;
&lt;p&gt;I would trust software that has a long and active development history with an engaged user base much more than anything that has simply been “peer reviewed”. I suspect that instituting peer review of code in the sciences would be a huge challenge for a potentially limited payoff. If open source taught us nothing else, it is that “with many eyes all bugs are shallow”. Robust software must ultimately rely on bug report feedback cycle of users (and developers following the &lt;a href=&quot;&quot;&gt;“dog food” rule&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;so-what-is-the-problem-exactly&quot;&gt;So what is the problem, exactly?&lt;/h2&gt;
&lt;p&gt;If the proposed solutions are weak, the statement of the problem is not actually much more convincing. What, exactly, is the “troubling trend” alluded to in the title? The primary concern from their survey appears to be that scientists rely on personal recommendations, ease of use, and frequency of use in published studies when choosing what software to use. This approach is only “dangerous” if we assume that a scientist has a choice of N potential software applications to perform a task, of which some fraction F are faulty. (And that a “worrying” 80% want to become better programmers…)&lt;/p&gt;
&lt;p&gt;But does this study argue any of the software there users approached was faulty? No. Do they provide evidence that the metrics a researcher uses (popularity, trust, ease of use) are not good predictors of decent software? No! They just assume this can’t possibly be a good criterion to evaluate software.&lt;/p&gt;
&lt;p&gt;Suggesting that this criterion is to blame misses the underlying problem entirely. Researchers will always prefer software that has been used in published studies, is easy to use, and recommended by people they trust. If this is leading to problems, we must fix the software, not the people.&lt;/p&gt;
&lt;p&gt;Reliance on common software is replacing reliance on undisclosed software written from scratch by each researcher. It is much easier to identify and correct errors when the community all uses a common piece of software then when everything is done from scratch. Crucially, this shows the emergence of a common code base – a shared software infrastructure, emerging in many fields. This development is to be celebrated and taken advantage of rather than something to fret over. A shared infrastructure is a powerful thing.&lt;/p&gt;
&lt;h2 id=&quot;real-solutions-changing-incentives&quot;&gt;Real solutions: changing incentives&lt;/h2&gt;
&lt;p&gt;Of course there are some real gems in the paper that should have recieved more emphasis.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Current models for how scientists and journals are rewarded must change, as the would-be editors of the Open Research Computation journal (now a series of the journal Source Code for Biology and Medicine) discovered during efforts to establish a journal for publishing peer-reviewed software ( 27).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reward software the way we reward papers. Github model of contributions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook features: SHA Hashes</title>
   <link href="/2013/05/03/notebook-features-hashes-providing-an-immutable-and-verifiable-research-record.html"/>
   <updated>2013-05-03T00:00:00+00:00</updated>
   <id>/2013/05/03/notebook-features-hashes-providing-an-immutable-and-verifiable-research-record</id>
   <content type="html">&lt;p&gt;Note: this entry is part of a &lt;a href=&quot;http://carlboettiger.info/2013/04/26/notebook-features-introduction.html&quot;&gt;series of posts&lt;/a&gt; which explain some of the technical features of my lab notebook.&lt;/p&gt;
&lt;p&gt;I version manage all changes to my entry using git. Each page is linked to its source history on Github, which will display a list of all previous edits to the post with an easy-to-read commit log and highlighted diffs. A version history is often considered an essential part of an open lab notebook, where changes to the notebook are documented and preserved. While wikis, Google docs, Dropbox, Wordpress plugins, or just regular backups can provide version history of pages, none come close to comparison with a full version management system such as git. This is because git’s underlying architecture is based on&lt;br /&gt;&lt;a href=&quot;http://www-cs-students.stanford.edu/~blynn/gitmagic/ch08.html&quot;&gt;The magic of cryptographic SHA hashes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hashes provide an immutable and verifiable record of any all changes I make. Because the hash is generated by the cryptographic SHA1 algorithm from the contents of the site, it is impossible to make changes without causing the hash to update. By referencing the content’s hash value we can be sure to link to a constant version of the entry. These can be verified by re-generating the hash (requiring the previous state of the repository in this case, see note below). Unlike publication timestamps, versions, or DOIs, this provides a way not only to reference particular versions of a file, but a cryptographically secure way to &lt;em&gt;verify&lt;/em&gt; that the version is what it claims to be. &lt;a href=&quot;http://www.tkuhn.ch/&quot;&gt;Tobias Kuhn&lt;/a&gt; has &lt;a href=&quot;http://www.force11.org/node/4301&quot;&gt;observed&lt;/a&gt; that this is a valuable feature we should want to see for all scientific publishing. Each of my posts now displays its SHA hash on the sidebar along with other metadata. While the &lt;code&gt;history&lt;/code&gt; button already provides a convenient way to browse all previous versions of a post, I chose to display the SHA hash directly so that the hash value would be part of the document metadata, while also highlighting this feature.&lt;/p&gt;
&lt;p&gt;In addition to the GitHub repository for my lab notebook, My research code, analyses, and manuscripts are collected into Github repositories by project. This allows my analysis and paper writing to benefit from this same immutable and verifiable record. Because GitHub uses the SHA hashes in its link structure, this also provides a convenient way to link to a particular version of code in a given entry. This way, I can be sure the contents of the file displayed at that link never change, even as I continue to update that file. Even if the file or containing directory is later deleted or moved, the link will still resolve. Only if the entire project repo were deleted or if Github itself dissolved would the link be lost. Even then, using the SHA hash given in the link we could determine the contents of the file from some other copy of the repository (such as a local or figshare archive).&lt;/p&gt;
&lt;p&gt;Tobias is actually working on his own SHA hash approach which is somewhat superior to the simpler method of using git. The Github hash corresponds to the state of the entire repository/notebook at the time of the commit, rather than the contents of an individual file. Consequently, one would need a snapshot of the entire repository, available on Github, to perform the verification. Tobias is looking into generating hashes based on the contents of the file directly – so far, only RDF data – that could provide a unique and verifiable reference for any scholarly data or publication.&lt;/p&gt;
&lt;p&gt;Version managing the notebook and code has many more practical day-to-day benefits, such as recovering from a mistaken deleted or corrupted file, merging changes made on different machines or by collaborators, or creating branches to test new features without disrupting current version, and comparing differences as a file evolves.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Scholarly Infrastructure Thoughts</title>
   <link href="/2013/05/02/scholarly-infrastructure-thoughts.html"/>
   <updated>2013-05-02T00:00:00+00:00</updated>
   <id>/2013/05/02/scholarly-infrastructure-thoughts</id>
   <content type="html">&lt;h2 id=&quot;sustainable-research&quot;&gt;Sustainable Research&lt;/h2&gt;
&lt;p&gt;Happened across a provocative &lt;a href=&quot;http://cran.r-project.org/web/packages/mcmc/ChangeLog&quot;&gt;example&lt;/a&gt; of why we need a software ecosystem, though it was certainly not intended to be one, which led me to ask myself:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How complex does an algorithm have to be before a talented researcher with expertise in both the relevant mathematics and computer science will make a significant mistake in their first release of the software?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a corollary we might also ask, “How much less care do we put into research code not destined for release?”&lt;/p&gt;
&lt;p&gt;This changelog clearly reflects these difficulties face well-established researchers with long publication records on these very methods. If such individuals can make mistakes in packages, are we supposed to trust the myriad personal implementations of this and more complex algorithms that bulwark our literature today?&lt;/p&gt;
&lt;p&gt;Academic knowledge is currently built in the mortar and bricks of publication and citation. Publications advance new claims built up on existing claims (and very occasionally replacing them) through citations. It an approach that does not scale well on many fronts. To verify information we must trace the citation chain, which grows far to quickly to for human processing and is not usually amenable to computer processing. Yet here it is the statistical scaling that concerns me more – a paper advances that a claim is true with a certain probability, given that the methods are implemented without error. The more publications we string together, the higher the probability that we observe false positives, but also that we observe implementation errors. For much of research today, we need not construct the scientific argument in this manner.&lt;/p&gt;
&lt;p&gt;Thanks to computational advances of the last several decades, public repositories of the data and the methods (such as can be implemented as software anyway), we can build on existing work by direct analysis of the data and methods, rather than treating the conclusions as given. Evidence would no longer come primarily in the highly circumstantial manner of citations to previous claims, but to direct analysis of data. Using a common pool of data and methods would align incentives better to maintain and improve upon this infrastructure (the way major companies contribute to the underlying ‘plumbing’ provided by shared open source infrastructure), while there is no incentive for the literature to work in such a way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cameronneylon.net/blog/whats-the-right-model-for-shared-scholarly-communications-infrastructure&quot;&gt;Cameron Neylon&lt;/a&gt;, and link in my comment.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Musings On Conservation Literature</title>
   <link href="/2013/05/02/musings-on-conservation-literature.html"/>
   <updated>2013-05-02T00:00:00+00:00</updated>
   <id>/2013/05/02/musings-on-conservation-literature</id>
   <content type="html">&lt;p&gt;Just because I’m a theorist deeply entrenched in methodological concerns about uncertainty and decision making doesn’t mean I don’t think about practical conservation from time to time. Some musings from my &lt;a href=&quot;http://dynamicecology.wordpress.com/2013/04/29/the-implementation-gap-in-conservation-biology-is-math-contributing-to-the-problem&quot;&gt;comment here&lt;/a&gt;, copied over for my own reference.&lt;/p&gt;
&lt;p&gt;Though I would like to believe the gap stems from the problems you discuss, I think that differing objectives between research and application may play a much larger role. I suspect that scientific papers that are most useful and influential for conservation practitioners and policymakers are those which confirm what they already believe, or whatever the interests opposed to them least want to hear. Let’s call these “Cassandra” papers, since in this context they usually forecast disaster. For the practitioner it may matters little whether the math is simple or complex, clearly explained or impenetrable, or even right or not so right. Worm et al 2006 paper which the media quickly decided predicted the end of global fisheries within 50 years is perhaps a good example.&lt;/p&gt;
&lt;p&gt;Okay, so beyond bolstering arguments already being made by those who propose, implement or legislate conservation against their opposition, there are certainly unknowns that they might turn to research to answer. Resource allocation might be an example of this; e.g. do we prioritize purchasing pristine areas that are not likely to be threatened or less pristine areas in more immediate danger (a la Pfaff). Let’s call these “rule of thumb” papers, where the conclusion is an easily applied guide-line. It seems doubtful that the practitioners would be inhibited by their access and understanding of the math in this case, since they want the research to provide an answer they can trust, and not worry so much about what math justifies it. They are more likely to use proxies of quality (journal, researcher, affiliation, popularity of the method), then working through the assumptions to see if they like them; no?&lt;/p&gt;
&lt;p&gt;So there is a third case in which the conclusion is of the sort “apply my method and it will tell you what to do”, as opposed to “here’s what to do”. I think only this case falls at risk to the mathematics being a barrier, though when accompanied by user-friendly software tools perhaps that can be dismissed as well. These “methods” papers are probably the favorite option of many researchers, as they seem the most rigorous, accurate way, reflecting the details of the problem at hand. Scientists are probably least fond of the first example, where even the paper’s authors may feel the conclusions are being overstated, while others feel such work is wrong and counterproductive. I’d guess many researchers are lukewarm towards the middle case, as better than a coin flip. I imagine from the conservation practitioner’s ranking is reversed. To what extent would you agree with this classification? If so, how is the conservation literature distributed across these categories, and how might we want it to be distributed? Do we indeed have the greatest impact writing Cassandra papers rather than writing nice clear methods, and if so, what are the implications?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook features: reproducible code</title>
   <link href="/2013/04/26/notebook-features-reproducible-code.html"/>
   <updated>2013-04-26T00:00:00+00:00</updated>
   <id>/2013/04/26/notebook-features-reproducible-code</id>
   <content type="html">&lt;p&gt;I now use the dynamic documentation software called &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; to write most entries that shore results and figures. The code to replicate the results is included automatically, ensuring that what I say I did and what code I actually ran to get the results are consistent. Though I have written about knitr before, both regarding its &lt;a href=&quot;http://carlboettiger.info/2012/03/21/knitr-github-and-a-new-phase-for-the-lab-notebook.html&quot;&gt;use in my notebook&lt;/a&gt; and &lt;a href=&quot;http://carlboettiger.info/2012/04/07/writing-reproducibly-in-the-open-with-knitr.html&quot;&gt;in my manuscripts&lt;/a&gt;, here I provide a quick summary of how a reader might actually reproduce a figure or result they come across in the notebook, as well as some of the possible problems involved.&lt;/p&gt;
&lt;p&gt;As the code required for any given analysis can be quite involved, it is not pratical to provide free-standing scripts in this way. Instead, I write the algorithms as functions provided by an R package dedicated to the project, e.g. &lt;a href=&quot;http://github.com/cboettig/nonparametric-bayes&quot;&gt;nonparametric-bayes&lt;/a&gt;, &lt;a href=&quot;http://github.com/cboettig/multiple_uncertainty&quot;&gt;multiple-uncertainty&lt;/a&gt;, or &lt;a href=&quot;http://github.com/cboettig/earlywarning&quot;&gt;warning-signals&lt;/a&gt;, which is version-managed on Github. The code displayed in the post can then be limited to the specific calls to analysis, data manipulation, and plotting functions unique to the exploration shown, without repeating the code for all algorithms involved.&lt;/p&gt;
&lt;p&gt;The code for the analyses is also stored on github using the same dynamic documentation approach with knitr. These scripts are found in the &lt;code&gt;inst/examples&lt;/code&gt; directory of my packages. This approach allows a given analysis to evolve with my research in a more tractable way than simply pasting updated copies as successive notebook entries. The notebook entries then become a place for me to synthesize the results of a script.&lt;/p&gt;
&lt;p&gt;Though the package functions are usually backwards-compatible, proper reproducibility is only attained by having the version of the package from time of the result. This is easily accomplished by the seemless integration of Github and R using the devtools package. Consider a figure from a page of the notebook, such as the final histogram plot from &lt;a href=&quot;http://www.carlboettiger.info/2012/12/20/results-comparing-gp-to-parametric.html&quot;&gt;this entry&lt;/a&gt;. The entry links to the script responsible for the figure, &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/9d5cd1f027bdfe5f356dce83756726c95a6fcdd8/inst/examples/myers-exploration.md&quot;&gt;https://github.com/cboettig/nonparametric-bayes/blob/9d5cd1f027bdfe5f356dce83756726c95a6fcdd8/inst/examples/myers-exploration.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can install the entire research compendium at exactly the state it was at the time of the analysis using the hash (long chain of seemingly random characters, see the (upcoming) entry on &lt;a href=&quot;&quot;&gt;hashes&lt;/a&gt;) using the clever &lt;code&gt;devtools&lt;/code&gt; R package,&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;install_github&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;nonparametric-bayes&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;cboettig&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;9d5cd1f027bdfe5f356dce83756726c95a6fcdd8&amp;quot;&lt;/span&gt;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then copy and paste the code linked from the figure to replicate the results. This provides a fast and effective way replicate the work appearing in or linked to any entry. More importantly perhaps, this approach also allows one to repeat any given analysis with the most recent version of an algorithm and compare the results, since the package structure provides a logical seperation between algorithm and analysis. In practice such fine-grained control and invistigation is more important than simply being able to regenerate what has already been done without any further input.&lt;/p&gt;
&lt;p&gt;This is not entirely failsafe. The package may depend on other packages, which themselves may have changed. For my use cases, it is a deal more reliable than running the current version of a package that is actively changing during my research. Readers interested in even more robust replication and verification should take a look at Roger Peng’s package &lt;code&gt;stashR&lt;/code&gt; package and associated publications &lt;span class=&quot;showtooltip&quot; data-html=&quot;true&quot; title=&quot;&lt;p&gt;Eckel S and Peng RD (2012). stashR: A Set of Tools for Administering SHared Repositories. R package version 0.3-5.“&amp;gt;&lt;a href=&quot;http://CRAN.R-project.org/package=stashR&quot;&gt;Eckel &amp;amp; Peng (2012)&lt;/a&gt;&lt;/span&gt; .&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sandy Eckel, Roger Peng, (2012) stashR: A Set of Tools for Administering SHared Repositories. &lt;a href=&quot;http://CRAN.R-project.org/package=stashR&quot;&gt;http://CRAN.R-project.org/package=stashR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook features: an introduction</title>
   <link href="/2013/04/26/notebook-features-introduction.html"/>
   <updated>2013-04-26T00:00:00+00:00</updated>
   <id>/2013/04/26/notebook-features-introduction</id>
   <content type="html">&lt;p&gt;In keeping this open lab notebook, I have sought to address three goals (in addition to all the traditional reasons for keeping a lab notebook)&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Provide an educational resource&lt;/li&gt;
&lt;li&gt;Experiment with scientific infrastructure and tools for sharing and replicating research&lt;/li&gt;
&lt;li&gt;Facilitate the rapid and open dissemination of scientific research&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;which are coincidentally evocative of &lt;a href=&quot;http://www.nsf.gov/pubs/2007/nsf07046/nsf07046.jsp&quot;&gt;NSF’s Broader Impacts&lt;/a&gt; areas. In this series of posts I plan to explore and illustrate some of my experiments to address these goals through various web-based tools available for an open notebook platform. Many of these have been documented in the notebook itself as I experiment with them (see &lt;a href=&quot;http://www.carlboettiger.info/tags.html#notebook-technology&quot;&gt;#notebook-technology&lt;/a&gt;). Not all of those experiments pan out and older tools and techniques are often replaced with newer ones as I explore, and these posts are usually more technical notes written to help me think through and remember what I’m trying out. In order to provide a more accessible snapshot of notebook features, I thought it might be helpful to write a series of posts describing these tools and techniques.&lt;/p&gt;
&lt;p&gt;Below is an index of posts in this theme that I will continue to update as I have a chance to finish them. If there’s anything about the notebook that you’d like to hear more about, feel free to suggest it in the comments.&lt;/p&gt;
&lt;h3 id=&quot;written&quot;&gt;Written&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/04/26/notebook-features-reproducible-code.html&quot;&gt;Reproducible code: embedding code and dynamic documents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/04/04/notebook-parsing.html&quot;&gt;Parsing linked data in the semantic notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/05/03/notebook-features-hashes-providing-an-immutable-and-verifiable-research-record.html&quot;&gt;Hashes: An immutable and verifiable record of research&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://carlboettiger.info/2013/05/31/notebook-features-digital-archiving.html&quot;&gt;Digital archiving&lt;/a&gt;. See also a separate entry on &lt;a href=&quot;http://purl.org/cboettig/2013/05/31/notebook-features-archiving-with-figshare&quot;&gt;advantages and challenges in archiving with figshare&lt;/a&gt;, and some &lt;a href=&quot;/2013/06/03/DOI-citable.html&quot;&gt;comments about DOIs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;planned&quot;&gt;Planned&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Readable, multi-device typography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Notebook analytics: Who reads this stuff anyway?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;A fast, inexpensive, and scalable online platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Online notebook essentials: link, tag, search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Integrating social networks with the notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook Features: parsing linked data in the semantic notebook</title>
   <link href="/2013/04/04/notebook-parsing.html"/>
   <updated>2013-04-04T00:00:00+00:00</updated>
   <id>/2013/04/04/notebook-parsing</id>
   <content type="html">&lt;p&gt;In a &lt;a href=&quot;/2011/05/08/building-a-semantic-notebook.html&quot;&gt;post a while back&lt;/a&gt; I originally put forward the idea of a semantic lab notebook. Semantics, or linked data, are among the most powerful concepts to emerge in online science for scholarly data organization and communication. I have slowly been adding and exploring new ways to introduce semantic concepts into the notebook, which I have documented along the way under my &lt;a href=&quot;http://www.carlboettiger.info/tags.html#semantics&quot;&gt;#semantics&lt;/a&gt; tag. In this post, rather than discuss how to generate the semantic data, I try to focus on some of the things we can &lt;em&gt;do&lt;/em&gt; with it. This really just scratches the surface of possibilities, but should at least illustrate the general idea.&lt;/p&gt;
&lt;p&gt;We will start with some very simple examples exploiting the semantics inherent in HTML5. We can then work up to richer examples that rely on XML-based parsing. The richest potential of the semantic notebook lies in leveraging the RDFa content, whose terms are defined as ontologies over which a machine can apply reasoning and formal logic against other web content (see, e.g. &lt;span class=&quot;showtooltip&quot; title=&quot;Shadbolt N, Berners-Lee T and Hall W (2006). The Semantic Web
Revisited. _Ieee Intelligent Systems_, *21*, pp. 96-101. ISSN
1541-1672,  http://dx.doi.org/10.1109/MIS.2006.62.&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/MIS.2006.62&quot; rel=&quot;http://purl.org/spar/cito/obtainsBackgroundFrom&quot; &gt;Shadbolt &lt;em&gt;et. al.&lt;/em&gt; (2006)&lt;/a&gt;&lt;/span&gt; for further unformation). Though we show how to extract the and parse the RDF here, exploiting the RDF in the last example must wait to a later post.&lt;/p&gt;
&lt;h2 id=&quot;parsing-semantic-html&quot;&gt;Parsing semantic HTML&lt;/h2&gt;
&lt;p&gt;Our first set of examples will address parsing the semantic HTML directly. For background on how these elements are added to the notebook, see &lt;a href=&quot;/2012/10/14/semantic-lab-notebook.html&quot;&gt;this entry&lt;/a&gt;. We will use R with it’s excellent collection of web, XML parsing, and text-mining tools to take advantage of the semantic structure. First we load the required packages,&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(RCurl)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(XML)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(tm)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(wordcloud)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(RColorBrewer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get a list of pages to download from the sitemap&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pagelist &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;readLines&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;http://carlboettiger.info/sitemap.txt&amp;quot;&lt;/span&gt;)
pagelist &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;pagelist[&lt;span class=&quot;kw&quot;&gt;grep&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;/201&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;d/&amp;quot;&lt;/span&gt;, pagelist)]  &lt;span class=&quot;co&quot;&gt;# drop non-posts)&lt;/span&gt;
pages &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;lapply&lt;/span&gt;(pagelist, getURLContent, &lt;span class=&quot;dt&quot;&gt;followlocation =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, if an archive is available locally, (e.g. from figshare cache), we can read the files in directly.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pages &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;system&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;ls ~/Documents/labnotebook/_site/2***/*/*/*.html&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;intern =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We parse each of the pages as HTML so that we can manipulate them with XML tools&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;html &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;lapply&lt;/span&gt;(pages, htmlParse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance, we can easily get the content of all entries:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;text &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(html, xpathSApply, &lt;span class=&quot;st&quot;&gt;&amp;quot;//article&amp;quot;&lt;/span&gt;, xmlValue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also extract metadata based on the semantic markup.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;titles &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(html, xpathSApply, &lt;span class=&quot;st&quot;&gt;&amp;quot;//title&amp;quot;&lt;/span&gt;, xmlValue)
categories &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(html, xpathSApply, &lt;span class=&quot;st&quot;&gt;&amp;quot;//node()[@class=&amp;#39;category&amp;#39;]&amp;quot;&lt;/span&gt;, 
    xmlValue)
tags &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(html, xpathSApply, &lt;span class=&quot;st&quot;&gt;&amp;quot;//node()[@class=&amp;#39;tag&amp;#39;]&amp;quot;&lt;/span&gt;, xmlValue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R makes it easy to summarize this data, e.g. by generating a table of the number of entries in each category, or a wordcloud of the tags.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;table&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;unlist&lt;/span&gt;(categories))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 computation      ecology    evolution open-science     teaching 
          40          376          287           85           17 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;wordcloud&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;Corpus&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;VectorSource&lt;/span&gt;(tags)))&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm9.staticflickr.com/8258/8620398951_0c2fd56e26_o.png&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;extracting-citations&quot;&gt;Extracting citations&lt;/h3&gt;
&lt;p&gt;Citation information can be encoded&lt;/p&gt;
&lt;p&gt;We can perform more direct text mining as well. For instance, we extract all DOIs found in the text:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;doi_pattern =&lt;span class=&quot;st&quot;&gt; &amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?![&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\&amp;quot;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;amp;&amp;#39;&amp;lt;&amp;gt;])&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;S)+)&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;b&amp;quot;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;require&lt;/span&gt;(gsubfn)
dois &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;strapply&lt;/span&gt;(text, doi_pattern, &lt;span class=&quot;dt&quot;&gt;perl =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)  &lt;span class=&quot;co&quot;&gt;#text[-462]&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;head&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sort&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;table&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;unlist&lt;/span&gt;(dois))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
10.1002/(SICI)1520-6602(1998)1:1                 10.1002/bjs.6880 
                               1                                1 
                10.1002/etc.2140           10.1006/jtbi.1998.0660 
                               1                                1 
          10.1006/jtbi.2000.1080           10.1006/jtbi.2001.2299 
                               1                                1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or generate a wordcloud of the full text&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;Corpus&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;VectorSource&lt;/span&gt;(text))
carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;tm_map&lt;/span&gt;(carl, removePunctuation)
carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;tm_map&lt;/span&gt;(carl, tolower)
carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;tm_map&lt;/span&gt;(carl, function(x) &lt;span class=&quot;kw&quot;&gt;removeWords&lt;/span&gt;(x, &lt;span class=&quot;kw&quot;&gt;stopwords&lt;/span&gt;()))

carl.tdm &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;TermDocumentMatrix&lt;/span&gt;(carl)
carl.m &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(carl.tdm)
carl.v &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sort&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rowSums&lt;/span&gt;(carl.m), &lt;span class=&quot;dt&quot;&gt;decreasing =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
carl.d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;data.frame&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;word =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;names&lt;/span&gt;(carl.v), &lt;span class=&quot;dt&quot;&gt;freq =&lt;/span&gt; carl.v)


&lt;span class=&quot;kw&quot;&gt;wordcloud&lt;/span&gt;(carl.d$word, carl.d$freq, &lt;span class=&quot;dt&quot;&gt;scale =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.4&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;min.freq =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;max.words =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;random.order =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;rot.per =&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.15&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;colors =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;brewer.pal&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;8&lt;/span&gt;, 
        &lt;span class=&quot;st&quot;&gt;&amp;quot;Dark2&amp;quot;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm9.staticflickr.com/8385/8621498714_2fe3e04226_o.png&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;rdfa-parsing&quot;&gt;RDFa parsing&lt;/h2&gt;
&lt;p&gt;RDF triples are the mainstay of semantic, linked data. Unlike the more text-mining oriented examples above, data in this format follows a strict and universal standard which allows a machine to identify meaning rather precisely. Critically, this allows one to automatically link data appearing in the notebook to data elsewhere on the web without the ambiguities of natural language that for instance, might confuse the animal jaguar with the car.&lt;/p&gt;
&lt;p&gt;RDFa is a way of adding these precise statements to HTML, again see the &lt;a href=&quot;/2012/10/14/semantic-lab-notebook.html&quot;&gt;earlier entry&lt;/a&gt; on how this is done. The technically inclined will note that the namespaces of the RDFa itself are not accessible in the XML parsing we used above, since they do not correspond to nodes or attributes, but appear only in the values of attributes. Fortunately, there are many excellent tools to extract this RDFa data, turning it into the XML formatted RDF triples we need. e can perform this using the &lt;a href=&quot;http://any23.org&quot;&gt;Any23&lt;/a&gt; API&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;download.file&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;paste&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;http://any23.org/rdfxml&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;http://carlboettiger.info&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;sep =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;), &lt;span class=&quot;st&quot;&gt;&amp;quot;temp.xml&amp;quot;&lt;/span&gt;)
doc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;xmlParse&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;temp.xml&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which creates a beautiful RDF XML file of all linked data found in the entry.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;doc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;rdf:RDF xmlns:xhtml=&amp;quot;http://www.w3.org/1999/xhtml/vocab#&amp;quot; xmlns:dcterms=&amp;quot;http://purl.org/dc/terms/&amp;quot; xmlns:rdf=&amp;quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;quot;&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://www.carlboettiger.info/&amp;quot;&amp;gt;
    &amp;lt;dcterms:title xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl Boettiger&amp;lt;/dcterms:title&amp;gt;
    &amp;lt;xhtml:license rdf:resource=&amp;quot;http://creativecommons.org/publicdomain/zero/1.0/&amp;quot;/&amp;gt;
    &amp;lt;dcterms:title xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl Boettiger&amp;lt;/dcterms:title&amp;gt;
    &amp;lt;dcterms:creator xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl Boettiger&amp;lt;/dcterms:creator&amp;gt;
    &amp;lt;dcterms:date xml:lang=&amp;quot;en&amp;quot;&amp;gt;2013-04-04T11:07:14-07:00&amp;lt;/dcterms:date&amp;gt;
    &amp;lt;dcterms:format xml:lang=&amp;quot;en&amp;quot;&amp;gt;text/html&amp;lt;/dcterms:format&amp;gt;
    &amp;lt;dcterms:language xml:lang=&amp;quot;en&amp;quot;&amp;gt;en&amp;lt;/dcterms:language&amp;gt;
    &amp;lt;dcterms:identifier xml:lang=&amp;quot;en&amp;quot;&amp;gt;/index.html&amp;lt;/dcterms:identifier&amp;gt;
    &amp;lt;dcterms:rights xml:lang=&amp;quot;en&amp;quot;&amp;gt;CC0&amp;lt;/dcterms:rights&amp;gt;
    &amp;lt;dcterms:source xml:lang=&amp;quot;en&amp;quot;&amp;gt;Lab Notebook&amp;lt;/dcterms:source&amp;gt;
    &amp;lt;dcterms:subject xml:lang=&amp;quot;en&amp;quot;&amp;gt;Ecology&amp;lt;/dcterms:subject&amp;gt;
    &amp;lt;dcterms:type xml:lang=&amp;quot;en&amp;quot;&amp;gt;website&amp;lt;/dcterms:type&amp;gt;
    &amp;lt;title xmlns=&amp;quot;http://ogp.me/ns#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl Boettiger&amp;lt;/title&amp;gt;
    &amp;lt;author xmlns=&amp;quot;http://ogp.me/ns#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;http://www.carlboettiger.info/index.html#me&amp;lt;/author&amp;gt;
    &amp;lt;first_name xmlns=&amp;quot;http://ogp.me/ns/profile#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl&amp;lt;/first_name&amp;gt;
    &amp;lt;last_name xmlns=&amp;quot;http://ogp.me/ns/profile#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Boettiger&amp;lt;/last_name&amp;gt;
    &amp;lt;published_time xmlns=&amp;quot;http://ogp.me/ns/article#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;2013-04-04T11:07:14-07:00&amp;lt;/published_time&amp;gt;
    &amp;lt;site_name xmlns=&amp;quot;http://ogp.me/ns#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Lab Notebook&amp;lt;/site_name&amp;gt;
    &amp;lt;url xmlns=&amp;quot;http://ogp.me/ns#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;http://www.carlboettiger.info/index.html&amp;lt;/url&amp;gt;
    &amp;lt;type xmlns=&amp;quot;http://ogp.me/ns#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;website&amp;lt;/type&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://xmlns.com/foaf/0.1/Person&amp;quot;/&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://schema.org/Person#Person&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://www.carlboettiger.info/assets/img/carlboettiger.png&amp;quot;&amp;gt;
    &amp;lt;depiction xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; xml:lang=&amp;quot;en&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info&amp;quot;&amp;gt;
    &amp;lt;homepage xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://carlboettiger.info&amp;quot;/&amp;gt;
    &amp;lt;url xmlns=&amp;quot;http://schema.org/Person#&amp;quot; rdf:resource=&amp;quot;http://carlboettiger.info&amp;quot;/&amp;gt;
    &amp;lt;name xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Carl Boettiger&amp;lt;/name&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;jobTitle xmlns=&amp;quot;http://schema.org/Person#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;a post-doctoral researcher&amp;lt;/jobTitle&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:nodeID=&amp;quot;node17eprp1n4x899515&amp;quot;&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://xmlns.com/foaf/0.1/Person&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;knows xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:nodeID=&amp;quot;node17eprp1n4x899515&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:nodeID=&amp;quot;node17eprp1n4x899515&amp;quot;&amp;gt;
    &amp;lt;homepage xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://users.soe.ucsc.edu/~msmangel/&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;knows xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:nodeID=&amp;quot;node17eprp1n4x899515&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://users.soe.ucsc.edu/~msmangel/&amp;quot;&amp;gt;
    &amp;lt;name xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Marc Mangel&amp;lt;/name&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:nodeID=&amp;quot;node17eprp1n4x899516&amp;quot;&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://xmlns.com/foaf/0.1/Person&amp;quot;/&amp;gt;
    &amp;lt;homepage xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://swfsc.noaa.gov/staff.aspx?&amp;amp;amp;id=17294&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://swfsc.noaa.gov/staff.aspx?&amp;amp;amp;id=17294&amp;quot;&amp;gt;
    &amp;lt;name xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Steve Munch&amp;lt;/name&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;affiliation xmlns=&amp;quot;http://schema.org/Person#&amp;quot; rdf:resource=&amp;quot;http://boe.ucsc.edu/~msmangel/CSTAR.html&amp;quot;/&amp;gt;
    &amp;lt;workplaceHomepage xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://www.ucsc.edu/&amp;quot;/&amp;gt;
    &amp;lt;weblog xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://www.carlboettiger.info/lab-notebook.html&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:nodeID=&amp;quot;node17eprp1n4x899517&amp;quot;&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://schema.org/PostalAddress&amp;quot;/&amp;gt;
    &amp;lt;address xmlns=&amp;quot;http://schema.org/Person#&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Center for Stock Assessment Research, 110 Shaffer Rd, Santa Cruz, CA 95050, USA&amp;lt;/address&amp;gt;
    &amp;lt;streetAddress xmlns=&amp;quot;http://schema.org/PostalAddress/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Center for Stock Assessment Research, 110 Shaffer Rd&amp;lt;/streetAddress&amp;gt;
    &amp;lt;addressLocality xmlns=&amp;quot;http://schema.org/PostalAddress/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;Santa Cruz&amp;lt;/addressLocality&amp;gt;
    &amp;lt;addressRegion xmlns=&amp;quot;http://schema.org/PostalAddress/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;CA&amp;lt;/addressRegion&amp;gt;
    &amp;lt;postalCode xmlns=&amp;quot;http://schema.org/PostalAddress/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;95050&amp;lt;/postalCode&amp;gt;
    &amp;lt;addressCountry xmlns=&amp;quot;http://schema.org/PostalAddress/&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;USA&amp;lt;/addressCountry&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;https://orcid.org/0000-0002-1642-628X&amp;quot;&amp;gt;
    &amp;lt;orcid xmlns=&amp;quot;http://purl.org/spar/datacite/&amp;quot; rdf:resource=&amp;quot;https://orcid.org/0000-0002-1642-628X&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://www.carlboettiger.info#me&amp;quot;&amp;gt;
    &amp;lt;rdf:type rdf:resource=&amp;quot;http://xmlns.com/foaf/0.1/Person&amp;quot;/&amp;gt;
    &amp;lt;account xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;http://www.cloudflare.com/email-protection#f497969b9180809d93b49399959d98da979b99&amp;quot;/&amp;gt;
    &amp;lt;account xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;https://twitter.com/cboettig&amp;quot;/&amp;gt;
    &amp;lt;account xmlns=&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot; rdf:resource=&amp;quot;https://github.com/cboettig&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
  &amp;lt;rdf:Description rdf:about=&amp;quot;http://www.carlboettiger.info/&amp;quot;&amp;gt;
    &amp;lt;license xmlns=&amp;quot;http://www.carlboettiger.info/&amp;quot; rdf:resource=&amp;quot;http://creativecommons.org/publicdomain/zero/1.0/&amp;quot;/&amp;gt;
    &amp;lt;license xmlns=&amp;quot;http://creativecommons.org/ns#&amp;quot; rdf:resource=&amp;quot;http://creativecommons.org/publicdomain/zero/1.0/&amp;quot;/&amp;gt;
  &amp;lt;/rdf:Description&amp;gt;
&amp;lt;/rdf:RDF&amp;gt;
 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now explore this data using the XML tools illustrated above. The rigidity of the XML rather than HTML parsing and the use of namespaces gives us greater precision.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Semantic Citations For The Notebook And Knitr</title>
   <link href="/2013/02/22/semantic-citations-for-the-notebook-and-knitr.html"/>
   <updated>2013-02-22T00:00:00+00:00</updated>
   <id>/2013/02/22/semantic-citations-for-the-notebook-and-knitr</id>
   <content type="html">&lt;p&gt;I have on ocassion been exploring the use of &lt;a href=&quot;/tags.html/#semantics&quot;&gt;semantic&lt;/a&gt; markup in the notebook. In this post I illustrate how I am handling semantic citations. One of the more intriguing ideas is the ability to add semantic meaning to citations through the CITO ontology of &lt;span class=&quot;showtooltip&quot; title=&quot;Shotton D (2010). Cito, The Citation Typing Ontology. _Journal
of Biomedical Semantics_, *1*. ISSN 2041-1480, 
http://dx.doi.org/10.1186/2041-1480-1-S1-S6.&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1186/2041-1480-1-S1-S6&quot; rel=&quot;http://purl.org/spar/cito/usesMethodIn&quot; &gt;Shotton (2010)&lt;/a&gt;&lt;/span&gt;. Citation counts form a central part of academic discourse, but contain very little information regarding the reason for the citation. Most notably, ‘negative’ citations refuting a claim carry just the same weight as those confirming or relying upon a claim. Given the scale and expansion of academic literature, it is rarely reasonable to explore this citation graph manually. CITO provides a language for embedding the meaning of the citation, such as “discusses”, “refutes”, or “usesMethodIn”, to the citation. (For instance, my earlier citation to Shotton identifies itself as “usesMethodIn”, as I will explain).&lt;/p&gt;
&lt;p&gt;The main barrier to this approach is a lack of adoption. One of the primary concerns is the burden it places on authors of adding the extra data. On one hand, authors already bother formatting and reformatting layout, spelling, and reference order to the arcane specifications of different journals, which suggests authors can be persuaded to do some pretty tedious tasks if the publishers would require it. After all, the task of adding citations is already much easier than it was in the days of paper journals. Still, it is much simpler to remove a tedious requirement than to add a new one. My hope is that intelligent tools can simplify this process, as they already have with other aspects of managing citations, and encourage the use of CITO. In this spirit, I have recently started trying to consistently use the CITO ontology in my notebook entries as a test case, using some tools of my own design.&lt;/p&gt;
&lt;h3 id=&quot;semantics-in-knitcitations&quot;&gt;Semantics in knitcitations&lt;/h3&gt;
&lt;p&gt;Several months ago I created the R package &lt;a href=&quot;https://github.com/cboettig/knitcitations&quot;&gt;knitcitations&lt;/a&gt; to provide a citation platform for &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; dynamic documents, which provide executable code and automatic inclusion of results inside plain-text (markdown) descriptions. I write most of my research scripts and many of my notebook entries in this manner. The package can generate citations by DOI, circumventing the need for maintaining bibtex or similar database of citation information, using commands such as&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;citet&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;10.1186/2041-1480-1-S1-S6&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extending the package to support CITO was rather straight forward. Using the latest version of knitcitations, one can generate in-line citations with CITO semantics simply by passing the reason for the citation as well, such as&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;citet&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;10.1186/2041-1480-1-S1-S6&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;cito=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;usesMethodIn&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which generates the following HTML:&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;http://dx.doi.org/10.1186/2041-1480-1-S1-S6&amp;#39;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;http://purl.org/spar/cito/usesMethodIn&amp;#39;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Shotton (2010)&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This provides a convient platform to generate semantic citations in this lab notebook. As before, knitcitations will also generate a complete reference list at the end of the document by calling the &lt;code&gt;bibliography&lt;/code&gt; function at the end.&lt;/p&gt;
&lt;h3 id=&quot;semantic-overkill&quot;&gt;Semantic overkill?&lt;/h3&gt;
&lt;p&gt;It is possible to add far more semantic data to this reference list at the end of an article. Invisible semantic markup can identify to a machine what value corresponds to the volume number or issue number, or journal name, e,g, using the BIBO ontology. I have added support for ths kind of markup to knitcitations as well, and &lt;a href=&quot;/2013/02/12/notes.html&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;/2013/02/21/notes.html&quot;&gt;of&lt;/a&gt; my posts provide examples. The raw markup looks like this:&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;div&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; prefix=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc: http://purl.org/dc/terms/,&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;                      bibo: http://purl.org/ontology/bibo/,&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;                      foaf: http://xmlns.com/foaf/spec/,&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;                      biro: http://purl.org/spar/biro/&amp;quot;&lt;/span&gt;
&lt;span class=&quot;ot&quot;&gt;        rel=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://purl.org/spar/biro/ReferenceList&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;ul&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; class=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;bibliography&amp;#39;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;kw&quot;&gt;&amp;lt;li&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:title&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Fisheries: Does Catch Reflect Abundance?.&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:creator&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:givenName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Daniel&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:familyName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Pauly&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:creator&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:givenName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Ray&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:familyName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Hilborn&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:creator&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:givenName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Trevor A.&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:familyName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Branch&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;  (&lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:date&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;2013&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;)  &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; rel=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://purl.org/dc/terms/isPartOf&amp;quot;&lt;/span&gt; 
&lt;span class=&quot;ot&quot;&gt;                            resource=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;[http://purl.org/dc/terms/journal]&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://purl.org/dc/terms/title&amp;quot;&lt;/span&gt;
&lt;span class=&quot;ot&quot;&gt;                                content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot; Nature &amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
                          &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:shortTitle&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; Nature &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
               &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:volume&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;494&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;    &lt;span class=&quot;kw&quot;&gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:doi&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://dx.doi.org/10.1038/494303a&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;10.1038/494303a&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;li&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:title&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Net Gains.&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:creator&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:givenName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;unknown&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;foaf:familyName&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;unknown&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;  (&lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;dc:date&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;2013&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;)  &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; rel=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://purl.org/dc/terms/isPartOf&amp;quot;&lt;/span&gt; 
&lt;span class=&quot;ot&quot;&gt;                            resource=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;[http://purl.org/dc/terms/journal]&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://purl.org/dc/terms/title&amp;quot;&lt;/span&gt;
&lt;span class=&quot;ot&quot;&gt;                                content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot; Nature &amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
                          &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:shortTitle&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; Nature &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
               &lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:volume&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;494&lt;span class=&quot;kw&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;    &lt;span class=&quot;kw&quot;&gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;bibo:doi&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://dx.doi.org/10.1038/494282a&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;10.1038/494282a&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;kw&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, I have since decided that such markup is largely overkill. The DOI uniquely identifies the publication already, and allows us to programmatically retrieve the rest of the data (title, authors, journal, etc) from semantically identified XML by querying against services such as CrossRef. &lt;em&gt;This is the essential concept of linked data&lt;/em&gt;, by which both source and referer are enriched.&lt;/p&gt;
&lt;p&gt;Moreover, DOIs follows a specific construction that lets us reliably &lt;a href=&quot;http://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page&quot;&gt;identify them in plain text using regular expressions&lt;/a&gt;, making any futher semantics to declare that we are citing the article mostly irrelevant. This is convient for identifying all citations appearing in the notebook without any markup. The CITO example above has the advantage of providing a link and associating the DOI with the reason for the citation, by virtue of being inside the same html anchor element.&lt;/p&gt;
&lt;h3 id=&quot;replacing-the-reference-list&quot;&gt;Replacing the reference list?&lt;/h3&gt;
&lt;p&gt;If we are not going to semantically mark up the reference list, we could consider abolishing the reference list all together. After all, as a tool for the digital reader the concept is rather vestigal – I hate losing my place by scrolling to the end of an article just to see to what reference number 7 refers. With the method shown thus far, the reader can open the link to access this information, but that still interrupts the flow of reading. The digitally native solution is a mouse-over or tooltip effect that displays this information, as many professional publishers already use in their HTML versions.&lt;/p&gt;
&lt;p&gt;Once again, this is straight forward to support using the knitcitations package, at least for sites that include the popular &lt;a href=&quot;http://twitter.github.com/bootstrap&quot;&gt;bootstrap&lt;/a&gt; javascript libraries, such as this notebook. I have added an option to the in-text citation functions to provide such tooltips in a span element, such that calling the command&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;span&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; class=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;showtooltip&amp;#39;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; title=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;Shotton D (2010). &amp;quot;Cito, The Citation Typing Ontology.&amp;quot; _Journal of&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;Biomedical Semantics_, *1*. ISSN 2041-1480, &lt;/span&gt;&lt;span class=&quot;er&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;URL:&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;http://dx.doi.org/10.1186/2041-1480-1-S1-S6&amp;gt;.&amp;#39;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;http://dx.doi.org/10.1186/2041-1480-1-S1-S6&amp;#39;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;http://purl.org/spar/cito/usesMethodIn&amp;#39;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Shotton (2010)&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This behavior can be toggled on by calling&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;cite_options&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;tooltip=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;after loading the knitcitations library. &lt;strong&gt;EDIT&lt;/strong&gt;: Note that this requires the javascript trigger on the class &lt;code&gt;showtooltip&lt;/code&gt;, which can be done by adding this to your header:&lt;/p&gt;
&lt;pre class=&quot;js&quot;&gt;&lt;code&gt;    &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
      $(document).ready(function (){
        $(&amp;quot;.showtooltip&amp;quot;).tooltip();
      });
    &amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;citing-without-dois&quot;&gt;Citing without DOIs&lt;/h3&gt;
&lt;p&gt;Not all the literature we may wish to cite includes DOIs, such as &lt;a href=&quot;http://arxiv.org&quot;&gt;arXiv&lt;/a&gt; preprints, Wikipedia pages, or other academic blogs. Even when a DOI is present it is not always trivial to locate. With version 0.4-0, knitcitations can produce citations given any URL using the &lt;a href=&quot;http://greycite.knowledgeblog.org&quot;&gt;Greycite API&lt;/a&gt; (&lt;span class=&quot;showtooltip&quot; title=&quot;Lord P (2012). Greycite. 
http://knowledgeblog.org/greycite [Online. last-accessed:
2012-10-10 13:36:24].  http://knowledgeblog.org/greycite.&quot;&gt;&lt;a href=&quot;http://knowledgeblog.org/greycite&quot; rel=&quot;http://purl.org/spar/cito/usesMethodIn&quot; &gt;Lord, 2012&lt;/a&gt;&lt;/span&gt;). For instance, this citation is created with the command &lt;code&gt;citep(&amp;quot;http://knowledgeblog.org/greycite&amp;quot;, cito=&amp;quot;usesMethodIn&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Github Issues Tracker: The Perfect Research Todo List</title>
   <link href="/2012/12/06/github-issues-tracker:-the-perfect-research-todo-list.html"/>
   <updated>2012-12-06T00:00:00+00:00</updated>
   <id>/2012/12/06/github-issues-tracker:-the-perfect-research-todo-list</id>
   <content type="html">&lt;p&gt;Github issues tracker has increasingly become my research to-do list. Far beyond bugs and features of the code associated with the project, the issues sign-post different directions for investigation and the progress I’ve made in each. Tags serve to group issues related to a common sub-project (as in my &lt;a href=&quot;https://github.com/cboettig/pdg_control/issues&quot;&gt;pdg-control&lt;/a&gt;) repo or priotize tasks (as in my &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/issues&quot;&gt;nonparametric-bayes&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;Issues not only have title and tags, but support a comment thread for progress and discussion of the issue. Thanks to github-flavored-markdown, issues can reference each other simply by number, and can be updated or closed automatically by mentioning the issue number in a commit.&lt;/p&gt;
&lt;p&gt;Issues can also be grouped into shared deadlines, or milestones; a feature I haven’t fully exploited (but see our &lt;a href=&quot;https://github.com/cboettig/ews-review/issues/milestones&quot;&gt;ews-review&lt;/a&gt; paper). In any collaborative project the issues can be assigned to different people, (though currently this requires they have a Github account).&lt;/p&gt;
&lt;p&gt;A consequence of this workflow is a conveniently numbered, color-coded and cross-linked collection of steps involved in a given research project. This tends to be a higher-level overview than the individual commit log, particularly as I often use commits to track multiple runs with different parameters, or move code across to different supercomputers that do the actual runs.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm9.staticflickr.com/8490/8250376085_09f734c6f1.jpg&quot; alt=&quot;Example from closed issues on pdg-control&quot; /&gt;&lt;figcaption&gt;Example from closed issues on &lt;a href=&quot;https://github.com/cboettig/pdg_control/issues?labels=&amp;amp;milestone=&amp;amp;page=1&amp;amp;state=closed&quot;&gt;&lt;code&gt;pdg-control&lt;/code&gt;&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I am still figuring out the right level or “resolution” on which to create and track issues.&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; On one extreme, almost every commit could be seen as resolving an issue. The ideal use for me is probably nearer the other extreme, where individual issues are rather big-picture, and may be referenced by many commits. Perhaps the right way to think about it is that the questions addressed by resolving an issue are on the level of &lt;em&gt;what is interesting to others&lt;/em&gt;, while changes in individual commits are more for me. Hopefully I get better at finding this relevant level.&lt;/p&gt;
&lt;p&gt;Another nice feature of issues is that they can be closed when a particular line of investigation hits a dead-end, or stalls, or when the problem is resolved. Unlike the resulting paper from an investigation which essentially summarizes the issues that were closed successfully, the issues tracker also reveals the dead-ends, as well as those issues that were not closed a time of publication (but perhaps left to “further research”). Hopefully I will have some decent examples of this in the repositories accompanying my next papers.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Unlike issues, commits do not have a native tag structure (so-called “tags” mark important events in the commit history rather than grouping common commits). So at least this would group commits by the tag of the associated issue.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Citing Lab Notebook Entries</title>
   <link href="/2012/11/23/citing-lab-notebook-entries.html"/>
   <updated>2012-11-23T00:00:00+00:00</updated>
   <id>/2012/11/23/citing-lab-notebook-entries</id>
   <content type="html">&lt;p&gt;C. Titus Brown has an excellent &lt;a href=&quot;http://ivory.idyll.org/blog/posting-blog-entries-to-figshare.html&quot;&gt;post&lt;/a&gt; discussing his exploration into the merits and technicalities cross-posting his blog posts to figshare. The &lt;a href=&quot;https://github.com/ropensci/rfigshare&quot;&gt;&lt;code&gt;rfigshare&lt;/code&gt; package&lt;/a&gt; written by &lt;a href=&quot;http://emhart.github.com/&quot;&gt;Ted Hart&lt;/a&gt; and myself can do just this, once we puzzled out some of the same challenges (Notes to Titus: though the documentation doesn’t mention it, you can get a programmatic list of available categories from &lt;a href=&quot;http://api.figshare.com/v1/categories&quot;&gt;http://api.figshare.com/v1/categories&lt;/a&gt;. Figshare can also take code as a fileset or dataset, and may soon add a type for it. The coolest thing about figshare is perhaps the way they add types in response to how they see users using the service).&lt;/p&gt;
&lt;p&gt;The real discussion, though, is not about &lt;em&gt;how&lt;/em&gt;, but &lt;em&gt;why&lt;/em&gt;? Currently figshare doesn’t render the html or markdown/restructured-text source (other than as plain text), so it’s not a great place to &lt;em&gt;read&lt;/em&gt; the posts. This may change in the near future as well, but the primary motivation for doing this seems to be on the ability to get an honest-to-goodness DOI for your entry. So why a DOI?&lt;/p&gt;
&lt;p&gt;The answer most often put forward is that “a DOI facilitates citing a paper in the formal literature.” I think that’s not entirely accurate. For one, the formal literature has no trouble citing things that do not have a DOI. Perhaps the answer is meant to mean track citations to my content, &lt;em&gt;e.g.&lt;/em&gt; for statistical/impact purposes. This may be closer to the mark, but it still needs work. What counts depends on whose doing the counting. Anyone browsing the citation counts of a work through different mediums has surely encountered Thompson-Reuters has very specific criteria to be included, so I don’t believe they are tracking citations to anything with a DOI. Nor, I think is Scopus. Perhaps the target is Google then?&lt;/p&gt;
&lt;p&gt;If you want Google Scholar to count the number of citations to your blog post, I have good news for you. You do not need a DOI and cross-posting to figshare. You just need to follow the &lt;a href=&quot;http://scholar.google.com/intl/en/scholar/inclusion.html#indexing&quot;&gt;metadata requirements&lt;/a&gt; as outlined by Google Scholar itself. This will help Google Scholar identify your blog post as a academic object, and add it to your profile. Anything indexed by Google scholar that cites your blog post will count as a reference. The basics are really simple. Have metadata such as:&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;resource_type&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;Lab Notebook&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;citation_journal_title&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;Lab Notebook&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;citation_publication_date&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;2012-11-23 00:00:00 +0000&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;citation_date&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;2012-11-23 00:00:00 +0000&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;citation_author&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;Carl Boettiger&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;meta&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; name=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;citation_title&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; content=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;Citing Lab Notebook Entries&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just replace &lt;code&gt;Lab Notebook&lt;/code&gt; etc. with the correct values for your site. The example above will do so automatically if you use Jekyll using information specified in the YAML header. It seems the entry also needs a section called either “References” or “Bibliography” followed by a list of references. Not sure what format Google Scholar will do best at disambiguating. (Note that what I refer to as “Google Scholar” metadata looks like it was originally the convention adopted by HireWire Press.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.russet.org.uk&quot;&gt;Phil Lord&lt;/a&gt; and colleagues have written the fantastic API called &lt;a href=&quot;http://knowledgeblog.org/greycite&quot;&gt;greycite&lt;/a&gt; [Lord 2012], which can be used to generate citation data from any website that has semantic markup (including HTML5 semantics, also Dublin Core or OpenGraph ontologies and Google Scholar metadata). The API can return citations formatted in bibtex or JSON. Try it out at &lt;a href=&quot;http://greycite.knowledgeblog.org&quot;&gt;greycite.knowledgeblog.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now there is another reason to use DOIs and figshare, which is, I think, entirely independent – archival preservation (decoupled journal, anyone?). DOIs are potentially more permanent than URLs (though I’ll leave that debate to the experts). Figshare content is backed up by &lt;a href=&quot;http://clockss.org/&quot;&gt;CLOCKSS&lt;/a&gt;. Forever is a long time, but this certainly sounds like a better archiving strategy. Figshare should be providing the necessary Google Scholar metadata on each object now. My current practice has been to archive my notebook in annual chunks on figshare (posting entries individually feels fragmented and cluttered to me), and rely on the more native web solution of HTML metadata to allow my entries to “citable”. Perhaps this is not ideal, but for the short term, the content is discoverable and citable via Google Scholar, which points to my address. If my site vanishes from the web, one might hope that an academic search for my lab notebook might recover the content from the archived version.&lt;/p&gt;
&lt;h2 id=&quot;edit-thoughts-examples-and-disclaimer&quot;&gt;Edit: thoughts, examples and disclaimer&lt;/h2&gt;
&lt;p&gt;I stumbled across Google’s indexing of some of my lab notebook entries somewhat by accident. For instance, one of my &lt;a href=&quot;http://www.carlboettiger.info/2011/10/28/optimal-control-examples-continued-bellman-dynamic-programming.html&quot;&gt;entries&lt;/a&gt; on optimal control in which I had listed some references started turning up &lt;a href=&quot;http://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=zj2rRtEAAAAJ&amp;amp;citation_for_view=zj2rRtEAAAAJ:4TOpqqG69KYC&quot;&gt;in Google Scholar&lt;/a&gt; Being able to engage in a scholarly exchange through blogs that can cite and be cited by the formal literature certainly sounds like an important step forward in generating new publishing models. Of course it is also obvious that the procedure I have outlined could be trivially exploited for some rather blatant gaming of Google Scholar’s citation statistics. This post certainly is not an endorsement of gaming such statistics. If nothing else, perhaps this once again underscores the weakness of reliance on metrics of quantity when trying to infer quality. Will we ever let go of that fallacy?&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Titus Brown (2012) Posting blog entries to figshare, Living in an Ivory Basement. http://ivory.idyll.org/blog/posting-blog-entries-to-figshare.html&lt;/li&gt;
&lt;li&gt;Phillip Lord (2012) Greycite. Knowledge Blog. http://knowledgeblog.org/greycite&lt;/li&gt;
&lt;li&gt;Carl Boettiger (2011) Optimal Control examples continued: Bellman, Dynamic Programming, Lab Notebook http://www.carlboettiger.info/2011/10/28/optimal-control-examples-continued-bellman-dynamic-programming.html&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Semantic Lab Notebook</title>
   <link href="/2012/10/14/semantic-lab-notebook.html"/>
   <updated>2012-10-14T00:00:00+00:00</updated>
   <id>/2012/10/14/semantic-lab-notebook</id>
   <content type="html">&lt;p&gt;As the lab notebook grows, to make the maximum use of content it would be particularly useful to maximize the ability for a computer to understand the content, allowing us to identify, manipulate, and connect data using scripts and software. This is the concept of &lt;em&gt;linked data&lt;/em&gt;, or a &lt;em&gt;semantic notebook&lt;/em&gt;. I have explored this this idea before in the context of a &lt;a href=&quot;2011/05/08/building-a-semantic-notebook.html&quot;&gt;wordpress-based platform&lt;/a&gt;, but now that Jekyll has let me strip away some of the abstraction of Wordpress it seems a good time to revisit this idea.&lt;/p&gt;
&lt;h2 id=&quot;semantics-in-html5&quot;&gt;Semantics in HTML5&lt;/h2&gt;
&lt;p&gt;Already the notebook is written in HTML5, which has considerable semantic structure compared to it’s predecessors. HTML5 introduces the structural elements&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;nav&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;header&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;article&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;aside&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;footer&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;that intuitively define the organization of an page, setting off the important content from the window-dressing. HTML5 also defines inline elements &lt;code&gt;&amp;lt;time&amp;gt;&lt;/code&gt; (see the links in Caveats), and &lt;code&gt;&amp;lt;mark&amp;gt;&lt;/code&gt;, and the existing tags for metadata, which let us specify &lt;code&gt;&amp;lt;title&amp;gt;&lt;/code&gt;, and basic &lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt; tags for metadata such as author, &lt;code&gt;&amp;lt;meta name=&amp;quot;author&amp;quot; content=&amp;quot;Author Name&amp;quot;&amp;gt;&lt;/code&gt;, encoding, description, and keywords. Links &lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; use the &lt;code&gt;rel&lt;/code&gt; attribute to describe the link target. Though one can write anything at all in this text, HTML5 defines a small vocabulary with recognized meaning, some old, some new, such as &lt;code&gt;&amp;lt;a rel=&amp;quot;tag&amp;quot;&lt;/code&gt;, &lt;code&gt;rel=&amp;quot;license&amp;quot;&lt;/code&gt; as well as the older &lt;code&gt;&amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;style.css&amp;quot;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;a rel=&amp;quot;nofollow&amp;quot;&lt;/code&gt;. &lt;a href=&quot;http://diveintohtml5.info/semantics.html&quot;&gt;Here is a great overview of the semantics in HTML5&lt;/a&gt;, with a bit more about the available . To expand our vocabulary beyond these elements, however, we will need more tools.&lt;/p&gt;
&lt;h2 id=&quot;linked-data&quot;&gt;Linked Data&lt;/h2&gt;
&lt;p&gt;W3C confusingly provides two standards for formally defining semantic content using an external vocabulary or ontology. The first is microdata, introduced as a simpler alternative to the second, RDFa, an HTML-adaptation of the RDF XML standard (originally developed for the now-defunct XHTML 2.0).&lt;/p&gt;
&lt;h3 id=&quot;microdata&quot;&gt;Microdata&lt;/h3&gt;
&lt;p&gt;Microdata introduces new attributes into HTML tags like &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;. The first of these is &lt;code&gt;itemtype&lt;/code&gt;, which points to an external resource such as schema.org to define the vocabulary. To have this vocabulary apply to child elements, we just add the attribute &lt;code&gt;itemscope&lt;/code&gt;. Then we can set the value of attribute &lt;code&gt;itemprop&lt;/code&gt; in this or following elements to give it semantically defined meaning, such as&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;div&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; itemscope itemtype=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://data-vocabulary.org/Person&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;h1&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; itemprop=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Carl Boettiger&lt;span class=&quot;kw&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; itemprop=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;url&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://www.carlboettiger.info&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;http://carlboettiger.info&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;img&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; itemprop=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;photo&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; src=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;https://en.gravatar.com/userimage/12904315/7edea703b826fbbe07f2ae4d95b8416b.jpg?16&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the terms such as &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;url&lt;/code&gt; have precise meanings attached to “Person”, as specified at the “http://data-vocabulary.org/Address”. This ability to point to an external vocabulary is really the key concept of linked data. In the spirit of HTML5, microdata is much simpler than RDFa, but also more limited. &lt;a href=&quot;http://manu.sporny.org/2011/uber-comparison-rdfa-md-uf/&quot;&gt;Here is an excellent comparison&lt;/a&gt;, but for our purposes we will use RDFa as it is more common in academic use and will more seamlessly allow us to use academic ontologies. (While microdata has a clear mapping to RDF, it is not clear that any ontology that can be expressed in RDF can also be expressed in microdata).&lt;/p&gt;
&lt;h3 id=&quot;rdfa&quot;&gt;RDFa&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://www.w3.org/TR/xhtml-rdfa-primer/&quot;&gt;RDFa&lt;/a&gt; can be written in a very similar manner. The &lt;code&gt;itemprop&lt;/code&gt; attribute is replaced by the &lt;code&gt;property&lt;/code&gt; attribute, and &lt;code&gt;vocab&lt;/code&gt; replaces &lt;code&gt;itemtype&lt;/code&gt; and automatically implies &lt;code&gt;itemscope&lt;/code&gt; to child nodes.&lt;/p&gt;
&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;div&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; vocab=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://xmlns.com/foaf/0.1/&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; typeof=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;Person&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;h1&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;Carl Boettiger&lt;span class=&quot;kw&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;a&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;homepage&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; href=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;http://www.carlboettiger.info&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;http://carlboettiger.info&lt;span class=&quot;kw&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kw&quot;&gt;&amp;lt;img&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; property=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;depiction&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; src=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;https://en.gravatar.com/userimage/12904315/7edea703b826fbbe07f2ae4d95b8416b.jpg?16&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;/&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;kw&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could have used only the &lt;code&gt;property&lt;/code&gt; attribute along with the complete URI to the term definition instead of this structure. &lt;code&gt;typeof&lt;/code&gt; indicates that the child elements are all of the same type (in this case, all belong to the same “Person”). If we were omitting &lt;code&gt;vocab&lt;/code&gt; and writing out all URIs, we would have had &lt;code&gt;&amp;lt;div typeof=&amp;quot;http://xmlns.com/foaf/0.1/Person&amp;quot;&amp;gt;&lt;/code&gt; as well. In RDFa, we also have the &lt;code&gt;resource&lt;/code&gt; attribute, which allows us to specify a URI for the element being described. This could be a relative or absolute URL. If the object is a link, using &lt;code&gt;resource&lt;/code&gt; is not necessary, and the property is taken to describe the URL given in &lt;code&gt;href&lt;/code&gt; or &lt;code&gt;src&lt;/code&gt; rather than the anchor text.&lt;/p&gt;
&lt;p&gt;Two older options lie at either extreme: the most basic is the older technique of &lt;a href=&quot;http://microformats.org/wiki/html5&quot;&gt;microformatting&lt;/a&gt;, basically relying on standard &lt;code&gt;class&lt;/code&gt; and &lt;code&gt;rel&lt;/code&gt; attributes to convey semantic information. Simple and without new attributes, but probably too limited for our purposes so we won’t concern ourselves with it further. The other venerable approach is serve the page as XHTML, which renders in the browser in much the same way but can be parsed by a machine as XML, with all the power and extensibility that presents. Unlike earlier HTML standards, HTML5 is already valid XML (already “serialized”), so the same page can be served in either format (earlier specs were SGML, standardized general markup languages, of which XML is just a subset). To allow parsing of the HTML5 pages as either type (see &lt;a href=&quot;http://en.wikipedia.org/wiki/Polyglot_markup&quot;&gt;polyglot&lt;/a&gt;), we need only add the language and namespace to the opening &lt;code&gt;&amp;lt;html&amp;gt;&lt;/code&gt; tag,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html lang=&amp;quot;en&amp;quot; xmlns=&amp;quot;http://www.w3.org/1999/xhtml&amp;quot; xml:lang=&amp;quot;en&amp;quot;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;An essential caveat is that these are all new approaches which may not render well in legacy browsers, particularly Windows Internet Explorer. Some of these, like recognizing and styling the HTML5 semantic elements in IE, can be addressed in CSS, for which Twitter Bootstrap does a decent job. Another caveat The HTML5 spec has not been finalized, and some things are still in flux, as the &lt;a href=&quot;http://html5doctor.com/time-and-data-element/&quot;&gt;removal&lt;/a&gt; and &lt;a href=&quot;http://www.brucelawson.co.uk/2012/best-of-time/&quot;&gt;reinstatement&lt;/a&gt; of the &lt;code&gt;&amp;lt;time&amp;gt;&lt;/code&gt; element illustrates.&lt;/p&gt;
&lt;h3 id=&quot;useful-tools-for-checking-implementation&quot;&gt;Useful tools for checking implementation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://validator.w3.org/nu&quot;&gt;W3C HTML5 Validator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.google.com/webmasters/tools/richsnippets&quot;&gt;Google rich snippets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://sparql.captsolo.net/browser/browser.py?url=http://www.w3.org/2007/08/pyRdfa/extract?uri=http://www.carlboettiger.info&quot;&gt;RDF extraction tool&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;scholarly-semantic-markup-for-the-notebook&quot;&gt;Scholarly Semantic Markup for the Notebook&lt;/h1&gt;
&lt;p&gt;Now that we’ve familiarized ourselves with the options, it’s time to see what semantic content we can implement.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Content/Data&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Example types&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Links to Potential vocabularies&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Page structure&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;code&gt;&amp;lt;header&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://diveintohtml5.info/semantics.html&quot;&gt;HTML5&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Post metadata&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;keywords, timestamps&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://schema.org/BlogPosting&quot;&gt;Schema.org microdata&lt;/a&gt; or &lt;a href=&quot;http://purl.org/terms/dc&quot;&gt;Dublin Core&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Author metadata&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Name, contact, networks,&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://xmlns.com/foaf/0.1/&quot;&gt;FOAF&lt;/a&gt;, &lt;a href=&quot;http://purl.org/terms/dc&quot;&gt;Dublin Core&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;interests, publications)&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Licenses&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;CC0&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://creativecommons.org/ns&quot;&gt;CreativeCommons&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Citations&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;bib info, reason for citing&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://purl.org/spar/cito&quot;&gt;CiTO&lt;/a&gt;, &lt;a href=&quot;http://purl.org/spar/biro&quot;&gt;BiRO&lt;/a&gt;,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://purl.org/ontologies/bibo&quot;&gt;bibo&lt;/a&gt;, &lt;a href=&quot;http://purl.org/terms/dc&quot;&gt;Dublin Core&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Taxonomic data&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;species names&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://rs.tdwg.org/dwc/terms/index.htm&quot;&gt;Darwin Core&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Ecological data&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;measurements, units, etc&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;del&gt;&lt;a href=&quot;http://knb.ecoinformatics.org/software/eml/&quot;&gt;EML&lt;/a&gt;&lt;/del&gt; &lt;a href=&quot;http://ecoinformatics.org/oboe/oboe.1.0/oboe-core.owl&quot;&gt;OBOE&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In another entry I will try and highlight where and what semantic content I have added (work in progress), with examples of each vocabulary. The first four types are relatively static content that can be easily woven into the Jekyll template files. Using Jekyll &amp;amp; Liquid to pull in template information from the &lt;code&gt;_config.yaml&lt;/code&gt; should help avoid repetitive entry and make updating the linked data easier. The last three are entry-specific content, and will be more challenging. I hope to add semantic support to &lt;a href=&quot;https://github.com/cboettig/knitcitations&quot;&gt;knitcitations&lt;/a&gt;, including the option for CiTO types, which should make entry of citation data quick and easy (&lt;a href=&quot;http://stackoverflow.com/questions/12867586&quot;&gt;SO question illustrating semantic citation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The last two are much richer, specific vocabularies. For the moment, it might be best to use these to give more precise meaning to tags, which are already used as metadata on posts. This would allow posts to be still created in simple markdown without the burden of adding in lines of RDFa and cluttering the markup. Meanwhile full datasets provided in EML are likely to live as separate files, rather than as a random table in the middle of a notebook entry.&lt;/p&gt;
&lt;p&gt;As always, feedback, corrections or suggestions are appreciated!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Welcome to my Lab Notebook - Reloaded</title>
   <link href="/2012/09/28/Welcome-to-my-lab-notebook.html"/>
   <updated>2012-09-28T00:00:00+00:00</updated>
   <id>/2012/09/28/Welcome-to-my-lab-notebook</id>
   <content type="html">&lt;p&gt;Welcome to my lab notebook, version 3.0. My &lt;a href=&quot;http://openwetware.org/wiki/User:Carl_Boettiger/Notebook&quot;&gt;original open lab notebooks&lt;/a&gt; began on the wiki platform &lt;a href=&quot;http://openwetware.org&quot;&gt;OpenWetWare&lt;/a&gt;, moved to a personally hosted Wordpress platform, and now run on a Jekyll-powered platform (&lt;a href=&quot;http://www.carlboettiger.info/README.html&quot;&gt;site-config&lt;/a&gt;), but the basic idea remains the same. For completeness, earlier entries from both platforms have been migrated here. Quoting from &lt;a href=&quot;http://www.carlboettiger.info/archives/211&quot;&gt;my original introduction&lt;/a&gt; to the Wordpress notebook:&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2&gt;Disclaimer: Not a Blog&lt;/h2&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://www.flickr.com/photos/twid/3013680713/in/faves-cboettig/&quot;&gt;&lt;img src=&quot;http://farm4.staticflickr.com/3053/3013680713_cfcebbd403_t.jpg&quot; alt=&quot;floatright&quot; /&gt;&lt;/a&gt; Welcome to my open lab notebook. This is the active, permanent record of my scientific research, standing in place of the traditional paper bound lab notebook. The notebook is primarily a tool for me to &lt;em&gt;do&lt;/em&gt; science, not communicate it. I write my entries with the hope that they are intelligible to my future self; and maybe my collaborators and experts in my field. Only the occasional entry will be written for a more general audience. […] In these pages you will find not only thoughts and ideas, but references to the literature I read, the codes or manuscripts I write, derivations I scribble and graphs I create and mistakes I make. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;why-an-open-notebook-is-it-working&quot;&gt;Why an open notebook? Is it working?&lt;/h2&gt;
&lt;p&gt;My original introduction to the notebook from November 2010 dodged this question by suggesting the exercise was merely an experiment to see if any of the purported benefits or supposed risks were well-founded. Nearly three years in, can I draw any conclusions from this open notebook experiment?&lt;/p&gt;
&lt;p&gt;In that time, the notebook has seen six projects go from conception to &lt;a href=&quot;http://www.carlboettiger.info/vita.html&quot;&gt;publication&lt;/a&gt;, and a seventh founder on a null result (see &lt;a href=&quot;http://carlboettiger.info/tags.html#tribolium&quot;&gt;#tribolium&lt;/a&gt;). Several more projects continue to unfold. I have often worked on several projects simultaneously, and some projects branch off while others merge, making it difficult to capture all the posts associated with a single paper into a single tag or category. Of course not all ideas make it into the paper, but they remain captured in the notebook. I often return to my earlier posts for my own reference, and frequently pass links to particular entries to collaborators or other colleagues. On occasion I have pointed reviewers of my papers to certain entries discussing why we did &lt;code&gt;y&lt;/code&gt; instead of &lt;code&gt;x&lt;/code&gt;, and so forth. Both close colleagues and researchers I’ve never met have emailed me to follow up on something they had read in my notebook. This evidence suggests that the practice of open notebook science can faciliate both the performance and dissemination of research while remaining compatible and even synergistic with academic publishing.&lt;/p&gt;
&lt;p&gt;I am both proud and nervous to know of a half dozen other researchers who have credited me for inspiring them to adopt open or partially open lab notebooks online. I am particularly grateful for the examples, interactions, and ideas from established practitioners of open notebook science in other fields. My collaborators have been largely been somewhere between favorable and agnostic towards the idea, with the occasional request for delayed or off-line notes. More often gaps arise from my own lapses in writing (or at least being intelligible), though the automated records from Github in particular, as well as Flickr (image log), Mendeley (reading log), and Twitter and the like help make up for some of the gaps.&lt;/p&gt;
&lt;p&gt;&lt;!-- Liam Revell, Scott Chamberlain, Alistair Boettiger, Noam Ross, Nick Fabina, Lee Worden, Mario Pineda-Krch--&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-integrated-notebook-becomes-the-knitted-notebook&quot;&gt;The Integrated Notebook becomes the Knitted Notebook&lt;/h2&gt;
&lt;p&gt;In creating my wordpress lab notebook, &lt;a href=&quot;http://www.carlboettiger.info/archives/211&quot;&gt;I put forward the idea of an “Integrated Lab Notebook”&lt;/a&gt;, a somewhat convoluted scheme in which I would describe my ideas and analyses in Wordpress posts, embed figures from Flickr, and link them to code on Github. &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;Knitr&lt;/a&gt; simplified all that. I can now write code, analysis, figures, equations, citations, etc, into a single &lt;code&gt;Rmarkdown&lt;/code&gt; format and track it’s evolution through git version control. The &lt;code&gt;knitr&lt;/code&gt; markdown format goes smoothly on Github, the lab notebook, and even into generating pdf or word documents for publication, never seperating the code from the results. For details, see “&lt;a href=&quot;http://www.carlboettiger.info/2012/04/07/writing-reproducibly-in-the-open-with-knitr.html&quot;&gt;writing reproducibly in the open with knitr&lt;/a&gt;.”&lt;/p&gt;
&lt;h2 id=&quot;navigating-the-open-notebook&quot;&gt;Navigating the Open Notebook&lt;/h2&gt;
&lt;p&gt;You can page through the notebook chronologically just like any paper notebook using the “Next” and “Previous” buttons on the sidebar. The notebook also leverages all of the standard features of a blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the ability to search,&lt;/li&gt;
&lt;li&gt;browse the archives &lt;a href=&quot;http://www.carlboettiger.info/archives.html&quot;&gt;by date&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;browse by &lt;a href=&quot;http://www.carlboettiger.info/tags.html&quot;&gt;tag&lt;/a&gt; or&lt;/li&gt;
&lt;li&gt;browse by &lt;a href=&quot;http://www.carlboettiger.info/categories.html&quot;&gt;category&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;follow the &lt;a href=&quot;http://www.carlboettiger.info/atom.xml&quot;&gt;RSS feed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;add and share comments in Disqus&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use categories as the electronic equivalent of separate paper notebooks, dividing out my ecological research projects, evolutionary research topics, my teaching notebook, and a few others. As such, each entry is (usually) made into exactly one category. I use tags for more flexible topics, usually refecting particular projects or methods, and entries can have zero or multiple tags.&lt;/p&gt;
&lt;p&gt;It can be difficult to get the big picture of a project by merely flipping through entries. The chronological flow of a notebook is a poor fit to the very nonlinear nature of research. Reproducing particular results frequently requires additional information (also data and software) that are not part of the daily entries. Github repositories have been the perfect answer to these challenges.&lt;/p&gt;
&lt;h2 id=&quot;the-real-notebook-is-github&quot;&gt;(The real notebook is Github)&lt;/h2&gt;
&lt;p&gt;My Github repositories offer a kind of inverted version of the lab notebook, grouped by project (tag) rather than chronology. Each of my research projects is now is given it’s own public Github repository. I work primarily in R because it is widely used by ecologists and statisicians, and has a strong emphasis on reproducible research. The “R package” structure turns out to be brilliantly designed for research projects, which specifies particular files for essential metadata (title, description, authors, software dependencies, etc), data, documentation, and source code (see &lt;a href=&quot;http://www.carlboettiger.info/2012/05/06/research-workflow.html&quot;&gt;my workflow&lt;/a&gt; for details). Rather than have each analysis described in full in my notebook, they live as seperate &lt;code&gt;knitr&lt;/code&gt; markdown files in the &lt;a href=&quot;https://github.com/cboettig/pdg_control/tree/master/inst/examples&quot;&gt;&lt;code&gt;inst/examples&lt;/code&gt;&lt;/a&gt; directory of the R package, where their &lt;a href=&quot;https://github.com/cboettig/pdg_control/commits/master/inst/examples&quot;&gt;history&lt;/a&gt; can be browsed on Github, complete with their commit logs. Long or frequently used blocks of code are written into functions with proper documentation in the package source-code directory &lt;code&gt;/R&lt;/code&gt;, keeping the analysis files cleaner and consistent.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/cboettig/pdg_control/issues?state=closed&quot;&gt;issues tracker&lt;/a&gt; connected to each Github repository provides a rich TO DO list for the project. Progress on any issue often takes the form of subsequent commits of a particular analysis file, and that commit log can automatically be appended to the issue.&lt;/p&gt;
&lt;h2 id=&quot;the-social-lab-notebook&quot;&gt;The social lab notebook&lt;/h2&gt;
&lt;p&gt;When scripting analyses or writing papers, pretty much everything can be captured on Github. I have recently added a short &lt;a href=&quot;https://github.com/cboettig/jekyll-labnotebook-plugins&quot;&gt;script&lt;/a&gt; to Jekyll which will pull the relevant commit logs into that day’s post automatically. Other activities fit less neatly into this mold (reading, math, notes from seminars and conferences), so these things get traditional notebook entries. I’m exploring automated integration for other activities, such as pulling my current reading from Mendeley or my recent discussions from Twitter into the notebook as well. For now, feed for each of these appear at the top of my &lt;a href=&quot;http://www.carlboettiger.info/lab-notebook.html&quot;&gt;notebook homepage&lt;/a&gt;, with links to the associated sites.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Migrating From Wordpress To Jekyll</title>
   <link href="/2012/09/19/migrating-from-wordpress-to-jekyll.html"/>
   <updated>2012-09-19T00:00:00+00:00</updated>
   <id>/2012/09/19/migrating-from-wordpress-to-jekyll</id>
   <content type="html">&lt;p&gt;Thanks to a recent bugfix in the &lt;a href=&quot;https://github.com/thomasf/exitwp&quot;&gt;exitwp&lt;/a&gt; scripts I was able to export all my Wordpress entries. The script pulls the entries from the Wordpress database and formats them in markdown, along with extracting all metadata such as timestamp, tags, categories, publication status, and Wordpress id number, which are all embedded as YAML header information.&lt;/p&gt;
&lt;h3 id=&quot;why-migrate-the-old-entries&quot;&gt;Why migrate the old entries?&lt;/h3&gt;
&lt;p&gt;When I moved to the Jekyll notebook, I left the Wordpress site standing with all its existing content and simply remapped the home pages. While it was nice to have the Wordpress system as a fall back while I got used to Jekyll, it meant that I couldn’t benefit from the faster and lighter nature of Jekyll while much of my traffic occurred on the Wordpress pages. I was running a dedicated virtual private server for the Wordpress hosting, and still the site wasn’t very responsive. To get an idea, try “flipping through the pages” of the new Jekyll notebook using the “next” and “previous” post buttons. The new pages load immediately, making it easy to flip through looking for some word or image I can’t find with tags or searches. Flipping through pages is the time-honored way of interacting with paper notebooks, so I’m glad that the new system is responsive enough to imitate this well.&lt;/p&gt;
&lt;h3 id=&quot;converting-shortcodes-from-wordpress-plugins&quot;&gt;Converting shortcodes from Wordpress Plugins&lt;/h3&gt;
&lt;p&gt;Migrating files using &lt;code&gt;exitwp&lt;/code&gt; is a relatively seamless affair. Unfortunately, my Wordpress posts make rather heavy use of plugins, which work through shortcodes that still appear in the converted files, such as &lt;code&gt;[flickr]&lt;/code&gt;, &lt;code&gt;[latex]&lt;/code&gt;, &lt;code&gt;[code lang=&amp;quot;r&amp;quot;]&lt;/code&gt;, and &lt;code&gt;[cite]&lt;/code&gt;, for image embedding, equation embedding, syntax highlighting, and citations by DOI, respectively. While a suite of Jekyll plugins exist for each of these tasks, I’d like my regular posts to be pure markdown (other than the YAML header), making it easy to migrate or reuse the content in the future in different platforms. Here is the &lt;a href=&quot;https://github.com/cboettig/sandbox/blob/c489da83ca7a78345b7981be4de5933ae7d63ac0/shortcodes.R&quot;&gt;R script I used for converting my shortcodes&lt;/a&gt;, as described below.&lt;/p&gt;
&lt;h4 id=&quot;syntax-highlighting-short-codes&quot;&gt;Syntax Highlighting short-codes&lt;/h4&gt;
&lt;p&gt;For instance, several markdown extension languages including, Github-flavored markdown, already allow for syntax-highlighting with fenced code blocks. I wrote a short script to replace all (most) of the shortcodes in use on my Wordpress site. (Over the years, I’d actually used a variety of different syntax highlighting plugins, making it necessary to match a variety of different shortcodes). Other than matching a lot of patterns, this was pretty straight forward.&lt;/p&gt;
&lt;h4 id=&quot;flickr-images&quot;&gt;flickr images&lt;/h4&gt;
&lt;p&gt;My workflow pushes all the images and graphs I create to flickr when the script runs. (I could also automatically push the images to Wordpress, or simply to my own site, but started using flickr pretty early on for this. The SOAP API is excellent, as is the Rflickr R package) Images were embedded into my Wordpress site using the shortcode and image ID number. To generate stand-alone markdown, I wanted tp replace all the image codes with the flickr URLs to the images, which is a trivial task with the API and Rflickr.&lt;/p&gt;
&lt;h4 id=&quot;latexmathjax-equations&quot;&gt;LaTeX/Mathjax Equations&lt;/h4&gt;
&lt;p&gt;Again I had used a variety of shortcodes for latex plugins before discovering MathJax, so I had to handle a variety of syntaxes here. Previously I’ve tried to write equations in a manner compatible with the redcarpet markdown parser that powers Github-Flavored Markdown (see the &lt;a href=&quot;https://github.com/nono/Jekyll-plugins&quot;&gt;corresponding Jekyll plugin&lt;/a&gt;), e.g. surrounding equations with &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; blocks to avoid the parser mangling them. Instead, I’ve found it easier now to switch to pandoc for my markdown parser (and &lt;a href=&quot;https://github.com/dsanson/jekyll-pandoc-plugin&quot;&gt;corresponding Jekyll Plugin&lt;/a&gt;). Pandoc uses the basic TeX &lt;code&gt;$&lt;/code&gt; and &lt;code&gt;$$&lt;/code&gt; for inline and display equations, rendering it into the MathJax (and modern LaTeX standard) &lt;code&gt;\(&lt;/code&gt; and &lt;code&gt;\[&lt;/code&gt; on conversion. Being pandoc-compatible is quite nice, making it easy to create an e-book, epub, or latex-based pdf of my notebook. I hate having to use a markdown syntax that isn’t compatible with most other extended markdown parsers, but with so many different, inconsistent parsers this cannot be easily avoided. Regardless, replacing the various shortcodes to the Pandoc syntax is just a simple regexpr step.&lt;/p&gt;
&lt;h4 id=&quot;citations&quot;&gt;Citations&lt;/h4&gt;
&lt;p&gt;Many of my posts relied on kcite to generate bibliographies for the post using DOIs. A short call to my &lt;a href=&quot;http://www.carlboettiger.info/2012/05/30/knitcitations.html&quot;&gt;knitcitations&lt;/a&gt; package allowed me to generate the parenthetical citations and bibliographies instead. In principle this step could be run at the time I compile the site with Jekyll. Pages would be valid &lt;code&gt;.Rmd&lt;/code&gt;, needing knitr as “markdown” interpreter. This is an intriguing way to go, but probably too complicated for the moment. Running additional R code would be nice, but could slow the time needed to build the site, even with knitr’s caching. Meanwhile, converted pages simply have the citations already processed and the bibliography added in as HTML at the end of the post.&lt;/p&gt;
&lt;h3 id=&quot;url-redirects&quot;&gt;URL Redirects&lt;/h3&gt;
&lt;p&gt;My Jekyll site uses the SEO recommended structure of year/month/day/page-title for URLs, while my Wordpress site used simple random id numbers. To ensure that links to the old Wordpress pages resolve to the newly migrated pages, I added a little Jekyll plugin, &lt;a href=&quot;https://github.com/cboettig/labnotebook/blob/master/_plugins/redirects.rb&quot;&gt;redirects.rb&lt;/a&gt;. One of the elegant things about Jekyll is the ability to create such plugins that do this just the way you would image doing so – creating auxiliary pages at each of the redirect URLs with a simple redirect command. To install, place &lt;code&gt;redirects.rb&lt;/code&gt; in &lt;code&gt;_plugins&lt;/code&gt;, &lt;code&gt;redirects.html&lt;/code&gt; in &lt;code&gt;_layouts&lt;/code&gt;, and in &lt;code&gt;_config.yml&lt;/code&gt; add the line &lt;code&gt;redirects: yes&lt;/code&gt; (see &lt;a href=&quot;https://github.com/cboettig/labnotebook&quot;&gt;site source&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;
&lt;p&gt;I am using Disqus as the comment engine for entries. Since URLs change in the above step and I do not provide a unique disqus code to each page, I am attempting to migrate comments using the &lt;a href=&quot;http://help.disqus.com/customer/portal/articles/286778-using-the-migration-tools&quot;&gt;Disqus “migrate threads” tool&lt;/a&gt;, which simply takes a csv file listing old and new urls in consecutive columns. Will have to wait and see if this works.&lt;/p&gt;
&lt;h3 id=&quot;private-entries&quot;&gt;Private entries&lt;/h3&gt;
&lt;p&gt;Several entries on the Wordpress site were marked private, and only visible after administrative login. Most of these entries are actually solicited journal reviews of various manuscripts which I cannot make public. &lt;code&gt;exitwp&lt;/code&gt; marks these posts as &lt;code&gt;published: false&lt;/code&gt; in the metadata, so it is easy to remove them from the Jekyll source site. They can be managed in a separate Github-excluded directory (so that they cannot be seen on the website source pages on Github either) that can be &lt;code&gt;.htaccess&lt;/code&gt; password protected.&lt;/p&gt;
&lt;h3 id=&quot;categories-and-tags&quot;&gt;Categories and Tags&lt;/h3&gt;
&lt;p&gt;Categories and tags are handled automatically in the conversion. One of the greatest things about having all the content on a common system is the ability to share a common tag-pool.&lt;/p&gt;
&lt;h3 id=&quot;rss&quot;&gt;RSS&lt;/h3&gt;
&lt;p&gt;If you follow the notebook through RSS feeds, all the migrated posts will be showing up as new unread entries, as the atom feed dates entries by when they appear, not the stated post date. My apologies!&lt;/p&gt;
&lt;h3 id=&quot;rough-edges&quot;&gt;Rough edges&lt;/h3&gt;
&lt;p&gt;A couple posts have rare shortcodes which are probably easier to correct by hand. My apologies for any rough edges resulting from the conversion (or the exchange of markdown parsers). Hopefully I’ll get these ironed out in the near future, but feel free to leave a comment on the page or the issues tracker on any such errors.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Esa Changes Arxiv Policy Following Community Comments</title>
   <link href="/2012/09/05/ESA-changes-ArXiv-policy-following-community-comments.html"/>
   <updated>2012-09-05T00:00:00+00:00</updated>
   <id>/2012/09/05/ESA-changes-ArXiv-policy-following-community-comments</id>
   <content type="html">&lt;p&gt;Earlier today, Scott Collins, the president of the Ecological Society of America has &lt;a href=&quot;https://twitter.com/ESA_Prez2013/status/243335363693797376&quot;&gt;announced&lt;/a&gt; that the society will now accept articles that have previously been posted on preprint servers. This comes on the heels of a growing discussion in our community. Ethan White has a good summary over on &lt;a href=&quot;http://jabberwocky.weecology.org/2012/09/05/esa-journals-will-now-allow-papers-with-preprints/&quot;&gt;Jabberwocky Ecology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many voices have joined the discussion over the past month, and it is exciting and vindicating to see the Society engage and discuss these questions. With this announcement out, I thought I might share my original letter. Here’s the text of an email I sent on August 1st to Don Strong, editor-in-chief of Ecology and a friend to open science known for his bottom-up view that ecologists should show their publication preferences by their actions. Don kindly forwarded this email to other members of the board.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dear Don,&lt;/p&gt;
&lt;p&gt;Perhaps you have already seen the perspective appearing in Nature this week, “&lt;a href=&quot;http://www.nature.com/news/geneticists-eye-the-potential-of-arxiv-1.11091&quot;&gt;Geneticists eye the potential of Arxiv&lt;/a&gt;.” I must say it is particularly saddening to see the Ecological Society of America being singled out in the discussion as a professional society that opposes pre-prints, particularly given that ESA is not beholden to corporate publishers, and in the face a rising swell of interest in pre-print server capacity, as evidenced by the Nature article (which quotes our own Graham Coop), the emergence of an NCEAS working group to bring about a preprint server, and the extensive online discussions about ESA’s stance against pre-prints which was no doubt responsible for bringing ESA into the negative limelight of this perspective.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&quot;http://jabberwocky.weecology.org/2012/07/18/esa-journals-do-not-allow-papers-with-preprints/&quot;&gt;discussion launched by Professor Ethan White&lt;/a&gt; on his blog suggests that ESA has in fact only recently removed a clause in the policies explicitly permitting the use of the arXiv. Meanwhile members of the &lt;a href=&quot;http://www.nceas.ucsb.edu/projects/12651&quot;&gt;NCEAS working group&lt;/a&gt; had considered approaching ESA to partner in their efforts to bring about a preprint server and culture, hoping to take advantage of the agility ESA has as a respected and independent society publisher. Ecology as a field has been a leading example of pushing for innovative and open publishing practices such as mandatory data archiving, and ESA has led this front for decades with innovations such as Ecological Archives and more recent Ecosphere. It shames me to see our society and our field painted as a backwater of regressive policies in such prominent magazines when even journals such as Nature, Science, &amp;amp; PNAS permit and encourage the use of preprint servers such as the arXiv. I would hate to see our most innovative research migrate away from the Society at a time when ESA could be leading our field through this academic publishing transition that is now discussed broadly in the NY Times, Guardian, Economist, &amp;amp; USA Today. This is a chance for the Ecological Society to both lead and flourish.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I hope that these issues can be discussed in some fashion with the input of our community at the annual meeting in Portland. The British Ecological Society already plans to use the meeting to discuss a digital future of their journals, surely ESA will be doing as much at it’s own meeting, formally or informally?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for your time and consideration. Please let me know if there is anything I can do to help.&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Carl&lt;/p&gt;
&lt;/blockquote&gt;
</content>
 </entry>
 
 <entry>
   <title>knitr, github, and a new phase for the lab notebook</title>
   <link href="/2012/03/21/knitr-github-and-a-new-phase-for-the-lab-notebook.html"/>
   <updated>2012-03-21T12:13:42+00:00</updated>
   <id>/2012/03/21/knitr-github-and-a-new-phase-for-the-lab-notebook</id>
   <content type="html">&lt;p&gt;I have recently modified the basic workflow of my lab notebook since discovering &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;knitr&lt;/a&gt;. Before, I would write code files which I could track on &lt;a href=&quot;https://github.com&quot;&gt;github&lt;/a&gt;, push figures created by the code to flickr, and then write a notebook entry on wordpress describing what I was doing. I’d embed each figure I wanted into the entry, and each figure got an automatic link to github for the script that created it (which usually worked, though it didn’t say where in the script the command came from, and it required manually specifying the script name).&lt;/p&gt;
&lt;p&gt;Because knitr allows me to write a single file containing code and formatted text, and will automatically display the code and embed the images, I can avoid that more convoluted workflow and just write.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://farm8.staticflickr.com/7256/7003687571_09f79cfc15_o.png&quot; /&gt;](https://github.com/cboettig/pdg_control/blob/master/inst/examples)&lt;/p&gt;
&lt;p&gt;What makes this so excellent is that knitr allows me to write in markdown, and github automatically displays nicely formatted markdown instead of raw script when you visit the page. So whereas before I would keep a bunch of working &lt;a href=&quot;https://github.com/cboettig/pdg_control/tree/master/demo&quot;&gt;R scripts&lt;/a&gt; in &lt;code&gt;projectname/demo&lt;/code&gt; I now keep a bunch of &lt;a href=&quot;https://github.com/cboettig/pdg_control/tree/master/inst/examples&quot;&gt;markdown scripts&lt;/a&gt; in &lt;code&gt;inst/examples&lt;/code&gt;, ((since I’m using the R package for projects and demo/ doesn’t want non-R scripts)).&lt;/p&gt;
&lt;p&gt;The great thing about this is that I can just click on each script and see nicely rendered text, links, code, and figures right on github.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://farm7.staticflickr.com/6217/7003679683_d9f472efd6_o.png&quot; alt=&quot;example entry&quot; /&gt;](https://github.com/cboettig/pdg_control/blob/master/inst/examples/Reed.md)&lt;/p&gt;
&lt;p&gt;While I can push this same markdown script to wordpress and have it be rendered in my notebook, I think maintaining these examples on github is preferable. Note that every script-name appears twice, once with and once without the &lt;code&gt;_knit_&lt;/code&gt; extension. The &lt;code&gt;_knit_&lt;/code&gt; extension indicates the file I ran to create the output (the code is in html comments so you can only see them in raw form). Because all the code is displayed in the output file (unless I call knitr options to surpress this), there’s really no need to view the &lt;code&gt;_knit_&lt;/code&gt; file to reproduce the example, everything is in one place in the output &lt;code&gt;.md&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;These files benefit from the github managed version history, just clicking the history button gives a list of all the former versions, with code and results right there.&lt;/p&gt;
&lt;p&gt;While I could update a post in the notebook in the same way, the version control of this wordpress notebook is more crude, and more importantly, the blog-format is designed for a linear flow, whereas in a given day I might update each of these example scripts. This seems like a much more natural workflow then having consecutive entries in the notebook with updated versions of the analysis the day before, and more natural than going back and changing a previous notebook entry.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://farm8.staticflickr.com/7094/7003692801_d14a23855e_o.png&quot; /&gt;](https://github.com/cboettig/pdg_control/commits/master/inst/examples/model_uncertainty.md)&lt;/p&gt;
&lt;p&gt;Github can also gives nice snapshot of what’s changed, along with the commit notes for each project, for each day. Now that I can display figures and formatted text on github, as well as code, what role does the Wordpress notebook play? I think this wordpress notebook can resume is proper role as a lab notebook, containing reflections and synthesis on what I’ve done, rather than the more comprehensive copies of each analysis and each figure. Because it’s the internet, I can link to each of the analyses of that day using version-stable links from github, or links that always give the most recent version. This requires additional effort, but it’s a reflection I should be doing anyway. We’ll see how it goes. Meanwhile, welcome to the open lab notebook v2.0.&lt;/p&gt;
&lt;h2 id=&quot;a-few-more-details&quot;&gt;A few more details&lt;/h2&gt;
&lt;h5 id=&quot;longer-code-r-functions&quot;&gt;Longer code &amp;amp; R functions&lt;/h5&gt;
&lt;p&gt;When I write code longer than a few lines, I try to make it a function, or collection of functions, and include basic Roxygen-style documentation with it so I don’t have to read the code to remember how to use it. These functions naturally live in the &lt;code&gt;R/&lt;/code&gt; directory of the project. The project’s R package can be installed, and all these functions are then available. Each of my example scripts calls functions belonging to the package, but those functions change less regularly. To fully reproduce the example, it would be necessary to grab a copy of the R package from the same commit-version as the script. In practice, most of the time any version of the package R functions could be used.&lt;/p&gt;
&lt;h5 id=&quot;github-wiki&quot;&gt;Github wiki&lt;/h5&gt;
&lt;p&gt;Github has wiki pages which I could use instead of putting my entries in &lt;code&gt;inst/example&lt;/code&gt;, since both render markdown (the wiki will additionally render mathjax math, all be it as a png). However, the wiki is aimed at online editing, and exists as a separate repository, so just keeping the markdown files in the project directory is simpler.&lt;/p&gt;
&lt;h5 id=&quot;images&quot;&gt;Images&lt;/h5&gt;
&lt;p&gt;Github doesn’t actually host the images. In fact, my images are still being pushed to flickr, and you can see them there. Knitr is handling this automatically as the figures are created, keeping track of the links so they can be included in the output markdown. Knitr can do this with imgur out of the box, and I’ve also written a wrapper to let it push the images to a wordpress site. Doesn’t really matter where they are stored, they can always be viewed on Github.&lt;/p&gt;
&lt;h5 id=&quot;heavy-computing&quot;&gt;Heavy computing&lt;/h5&gt;
&lt;p&gt;I’ve found that the knitted markdown examples make the most sense for fast-running examples. Despite excellent caching support, I’ve found it best to run really long-running examples as external R scripts, and then save and import this data. Such scripts are often being run on a cluster that can’t push images to the internet anyway.&lt;/p&gt;
&lt;h5 id=&quot;power-of-openness&quot;&gt;Power of openness?&lt;/h5&gt;
&lt;p&gt;It occurs to me that this system would be harder but not impossible, in a closed environment. I could upload the figures to flickr with private status. The links are hashes, so cannot be guessed. If the output were then hosted on a secure site (running a markdown renderer, such as Jekyll), instead of github, these links would still work to display the images. Then one could give selected access to those pages. But the open solution works out of the box.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Citing R packages</title>
   <link href="/2012/03/20/citing-r-packages.html"/>
   <updated>2012-03-20T10:02:09+00:00</updated>
   <id>/2012/03/20/citing-r-packages</id>
   <content type="html">&lt;p&gt;I’m not always careful in citing all the R packages I use. R actually has some rather nice built-in mechanisms to support this, so I really have no excuse. Here’s some quick examples:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;citation&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;ouch&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;To cite the ouch package in publications
use:

  Aaron A. King and Marguerite A. Butler
  (2009), ouch: Ornstein-Uhlenbeck models
  for phylogenetic comparative hypotheses (R
  package),
  http://ouch.r-forge.r-project.org

  Butler, M. A. and King, A. A. (2004)
  Phylogenetic comparative analysis: a
  modeling approach for adaptive evolution
  Am. Nat. 164:683--695

As ouch is continually evolving, you may
want to cite its version number. Find it
with &amp;#39;help(package=ouch)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can I have that in bibtex format please?&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;toBibtex&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;citation&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;ouch&amp;quot;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;@Manual{,
  title = {ouch: Ornstein-Uhlenbeck models for phylogenetic comparative hypotheses},
  author = {Aaron A. King and Marguerite A. Butler},
  year = {2009},
  url = {http://ouch.r-forge.r-project.org},
}
@Article{,
  author = {Marguerite A. Butler and Aaron A. King},
  title = {Phylogenetic comparative analysis: a modeling approach for adaptive evolution},
  journal = {American Naturalist},
  year = {2004},
  volume = {164},
  pages = {683--695},
  url = {http://www.journals.uchicago.edu/AN/journal/issues/v164n6/40201/40201.html},
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this package provides the citation information for both the package and the associated journal article simultaneously, and R has successfully identified the formats as ‘Manual’ and ‘Article’ respectively.&lt;/p&gt;
&lt;p&gt;After running your code, consider creating a custom bibtex file containing the citation information for all the packages you have just used. (The file can be imported into most citation managers, if LaTeX isn’t your thing).&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;sink&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;test.bib&amp;quot;&lt;/span&gt;)
out &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;names&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sessionInfo&lt;/span&gt;()$otherPkgs), 
    function(x) &lt;span class=&quot;kw&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;citation&lt;/span&gt;(x), &lt;span class=&quot;dt&quot;&gt;style =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Bibtex&amp;quot;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also simply generate the list of loaded package in LaTeX format, which could be automatically included.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;toLatex&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sessionInfo&lt;/span&gt;(), &lt;span class=&quot;dt&quot;&gt;locale =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;for-package-authors&quot;&gt;For package authors&lt;/h3&gt;
&lt;p&gt;R will attempt to automatically construct the citation information for the package automatically from the description file, so it is not strictly necessary to do anything to your package to create it. Note that R has recently adopted a new syntax to specify the authors, which is a bit more precise. Instead of using Authors: in the DESCRPTION, we use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Authors@R: c(person(&amp;quot;Carl&amp;quot;, &amp;quot;Boettiger&amp;quot;, role = c(&amp;quot;aut&amp;quot;, &amp;quot;cre&amp;quot;), email = &amp;quot;cboettig@gmail.com&amp;quot;), 
  person(&amp;quot;Duncan&amp;quot;, &amp;quot;Temple Lang&amp;quot;, role = &amp;quot;aut&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines the roles (author, creator, etc, see &lt;code&gt;?person&lt;/code&gt; for details), and ‘creator’ takes the place of the &lt;code&gt;Maintainer:&lt;/code&gt; designation, and requires an email address. If you wish to add an additional publication as part of the citation information (such as the example from &lt;code&gt;ouch&lt;/code&gt; above, you can specify this in the CITATION file. For the example this looks like:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;citHeader&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;To cite the ouch package in publications use:&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;citEntry&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;entry =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Article&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;author =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;personList&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.person&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Marguerite A. Butler&amp;quot;&lt;/span&gt;), 
        &lt;span class=&quot;kw&quot;&gt;as.person&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Aaron A. King&amp;quot;&lt;/span&gt;)), &lt;span class=&quot;dt&quot;&gt;title =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Phylogenetic comparative analysis: a modeling approach for adaptive evolution&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;journal =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;American Naturalist&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;year =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2004&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;volume =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;164&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;pages =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;683--695&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;url =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;http://www.journals.uchicago.edu/AN/journal/issues/v164n6/40201/40201.html&amp;quot;&lt;/span&gt;, 
    &lt;span class=&quot;dt&quot;&gt;textVersion =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;paste&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Butler, M. A. and King, A. A. (2004)&amp;quot;&lt;/span&gt;, 
        &lt;span class=&quot;st&quot;&gt;&amp;quot;Phylogenetic comparative analysis: a modeling approach for adaptive evolution&amp;quot;&lt;/span&gt;, 
        &lt;span class=&quot;st&quot;&gt;&amp;quot;Am. Nat. 164:683--695&amp;quot;&lt;/span&gt;))
&lt;span class=&quot;kw&quot;&gt;citFooter&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;As ouch is continually evolving, you may want to cite its version number. Find it with &amp;#39;help(package=ouch)&amp;#39;.&amp;quot;&lt;/span&gt;)

[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] &lt;span class=&quot;st&quot;&gt;&amp;quot;As ouch is continually evolving, you may want to cite its version number. Find it with &amp;#39;help(package=ouch)&amp;#39;.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;attr&lt;/span&gt;(,&lt;span class=&quot;st&quot;&gt;&amp;quot;class&amp;quot;&lt;/span&gt;)
[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] &lt;span class=&quot;st&quot;&gt;&amp;quot;citationFooter&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(It seems like there should be a simple way to generate this automatically from the bibtex format, but I haven’t discovered it.)&lt;/p&gt;
&lt;h3 id=&quot;r-as-a-citation-tool&quot;&gt;R as a citation tool&lt;/h3&gt;
&lt;p&gt;I wrote an R function for the Crossref API in our rplos package. We should probably be formatting the output as an R bibentry, taking advantage of R’s understanding of citation structure. Then I could work automatic citation look-up into my posts using inline knitr calls, such as:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;crossref&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;10.1038/44766&amp;quot;&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;style=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;text&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternate mechanism could read in a local bibtex file and drop in the correct entry in the desired format.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Journey to freedom - a code's tale of open source license migration</title>
   <link href="/2012/01/31/journey-to-freedom-a-codes-tale-of-open-source-license-migration.html"/>
   <updated>2012-01-31T17:58:08+00:00</updated>
   <id>/2012/01/31/journey-to-freedom-a-codes-tale-of-open-source-license-migration</id>
   <content type="html">&lt;p&gt;My software wants to be free. It wants to be seen and used and loved by as many people as possible. When first it heard of open source licenses, it set sail to join the company of great software in the promised land, but finding true freedom has been a tortured journey.&lt;/p&gt;
&lt;h3 id=&quot;in-the-clutches-of-the-gpl&quot;&gt;In the clutches of the GPL&lt;/h3&gt;
&lt;p&gt;Created and defended by the &lt;a href=&quot;http://www.fsf.org/&quot;&gt;Free Software Foundation&lt;/a&gt; and used by such venerable institutions as the Linux Kernel, the gcc compiler, and the R statistical environment, the &lt;a href=&quot;http://en.wikipedia.org/wiki/GNU_General_Public_License&quot;&gt;GNU’s General Public License&lt;/a&gt; seemed like a gold standard to call home. Many of my codes quickly migrated to this beacon of light, while others, already making use of other GPL licensed software, seemed compelled to follow by it’s viral share-alike clauses. But my code wasn’t free. It couldn’t play nicely with users from the pantheon of other open source licenses, upsetting free software developers and university tech transfer offices alike. It was scorned by open science advocates and repositories. And when it wanted to leave and migrate to a more free and permissive license, it felt trapped.&lt;/p&gt;
&lt;p&gt;By now the software was written as an R package (combining C source code and R functions), depended on several R packages, and even compiled against the &lt;a href=&quot;http://www.gnu.org/software/gsl/&quot;&gt;GSL C library&lt;/a&gt;, all GPL licensed software. Surely it was stuck?&lt;/p&gt;
&lt;p&gt;Only after close examination did the package realize that it could escape. It’s not a derivative work. It’s not distributing the other R packages or R itself. That software have to be installed separately – it is only &lt;strong&gt;dynamically linked&lt;/strong&gt; ((called at runtime, not compiled into the binary libraries of my package)) to my package. The case is less obvious for the dependency of the C library on the GSL code, since this must be compiled to run. If the C functions provided by the GSL are compiled statically, they are pulled from their source and crammed into the binary next to my own code – making a derivative work that doesn’t need the GSL libraries installed separately to run. Fortunately that is not the case, these functions are also dynamically linked, and my code is free to migrate.&lt;/p&gt;
&lt;h3 id=&quot;bsd-still-incompatible&quot;&gt;BSD: still incompatible?&lt;/h3&gt;
&lt;p&gt;Fleeing the convoluted and tortured shore of GPL, my code happily reached the land of &lt;a href=&quot;http://en.wikipedia.org/wiki/BSD_licenses&quot;&gt;BSD&lt;/a&gt;. In it’s forth iteration as FreeBSD, with only two simple clauses, it seemed anything was possible. The only thing it does require are that the license always be distributed with the source code or binaries of the software. This makes it difficult to apply to repositories that also archive data – facts about the world owned under anyone’s copyright – and other non-code material such as documentation that may be found in an R package or an academic data repository. For this reason, the Dryad repository objects to that little clause of redistributing the FreeBSD license, and insists on public domain declaration provided by the &lt;a href=&quot;http://creativecommons.org/publicdomain/zero/1.0&quot;&gt;CC0&lt;/a&gt; license.&lt;/p&gt;
&lt;h3 id=&quot;cc0-free-at-last&quot;&gt;CC0, free at last?&lt;/h3&gt;
&lt;p&gt;Ah, so we arrive at the public domain, or as &lt;a href=&quot;http://creativecommons.org/publicdomain/zero/1.0/legalcode.txt&quot;&gt;close as I can legally get&lt;/a&gt;. While this seems to be the ultimate freedom my code has long sought, it finds itself rather lonely on these foreign shores. The Open Source Initiative doesn’t yet recognize it, which means it doesn’t meet the “open source license” requirements of academic journals like &lt;a href=&quot;http://www.ploscompbiol.org/static/guidelines.action&quot;&gt;PLoS Computational Biology&lt;/a&gt; or academic conferences like &lt;a href=&quot;http://www.open-bio.org/wiki/BOSC_2012&quot;&gt;Bioinformatics Open Source Conference&lt;/a&gt; or iEvoBio. It has not yet been used for R packages on the CRAN repository (see below), and would require using R’s custom license file mechanism rather than a simple LICENSE: CC0 line in the package meta-data. It does get a mention for being GPL compatible by the FSF ((i.e. you can redistribute it under the GPL)), but the license that isn’t a license needs more support. Following their guidelines, I’ve written the Open Software Initiative hoping it will gain that recognition.&lt;/p&gt;
&lt;h2 id=&quot;will-osi-adopt-cc0&quot;&gt;Will OSI adopt CC0?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Hello,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would like to submit the Creative Commons zero license for consideration.  I am new to this list, forgive me if it has already been considered. I understand that this license &lt;a href=&quot;http://www.gnu.org/licenses/license-list.html#CC0&quot;&gt;has been considered&lt;/a&gt; GPL compatible by the FSF with this &lt;a href=&quot;http://wiki.creativecommons.org/CC0_FAQ#May_I_apply_CC0_to_computer_software.3F_If_so.2C_is_there_a_recommended_implementation.3F&quot;&gt;recommended format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Submission type: Approval License Name: Creative Commons Zero, CC0 Category: Licenses that are popular and widely used or with strong communities The &lt;a href=&quot;http://creativecommons.org/publicdomain/zero/1.0/legalcode.txt&quot;&gt;legal code as plain text&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The scientific community is increasingly embracing this option as the most open and compatible license.  It is required by scientific data repositories (that also archive scientific software, with cite-able DOIs) such as &lt;a href=&quot;http://datadryad.org/depositing#whycc0&quot;&gt;Dryad&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I feel it is important that this license achieve recognition by the Open Source initiative, as certain scientific journals &amp;amp; conferences permit only OSI recognized licenses, (e.g. PLoS Computational Biology &lt;a href=&quot;http://www.ploscompbiol.org/static/guidelines.action&quot;&gt;requirements&lt;/a&gt; or Bioinformatics open source conference &lt;a href=&quot;http://www.open-bio.org/wiki/BOSC_2012&quot;&gt;requirements&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;licensing-in-r&quot;&gt;Licensing in R&lt;/h2&gt;
&lt;p&gt;R uses a simple but powerful license format in the DESCRIPTION files of all packages, which contain the essential meta-data for the package. Common FOSS licenses are recognized automatically (including version numbers or version inequalities). Since CC0 doesn’t (yet) make this list, we can use a custom license file where we provide the full plaintext CC0 license in the file LICENSE and write “file LICENSE” in the DESCRIPTION (&lt;a href=&quot;https://github.com/cboettig/fluctuationDomains&quot;&gt;my example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We can also see the frequency with which various licenses are used on the R repository, CRAN, with a few lines of code:&lt;/p&gt;
&lt;pre class=&quot;sourceCode R&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
p =&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;available.packages&lt;/span&gt;(contrib.
&lt;span class=&quot;kw&quot;&gt;url&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;http://cran.r-project.org&amp;quot;&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;source&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sort&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;table&lt;/span&gt;(p[, &lt;span class=&quot;st&quot;&gt;&amp;quot;License&amp;quot;&lt;/span&gt;]))&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Are open lab notebooks considered prior publication?</title>
   <link href="/2012/01/16/are-open-lab-notebooks-considered-prior-publication.html"/>
   <updated>2012-01-16T10:47:17+00:00</updated>
   <id>/2012/01/16/are-open-lab-notebooks-considered-prior-publication</id>
   <content type="html">&lt;p&gt;This question invariably comes up at some point in any discussion of open notebook science.  This concern is usually voiced in reference to the high-visibility magazines, which many scientists seem to assume will have very restrictive conditions.  A quick read of their policies shows otherwise.  Here are the links to pre-publication policies of major journals, with my short summaries &amp;amp; comments.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.nature.com/authors/policies/confidentiality.html&quot;&gt;Nature&lt;/a&gt; - Go right ahead. Explicit protection clause for open/collaborative blogs/wikis.  Just don’t talk to the press – guess that could include tweeting.  Also &lt;em&gt;Nature&lt;/em&gt; would like to see those links when you submit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pnas.org/content/96/8/4215.full&quot;&gt;PNAS&lt;/a&gt; - Nope, not prior publication here either. In fact, PNAS has one of the most liberal stances on prepublication which is well worth reading.  Even summaries of the work in the media encouraged – they only remind authors not to forget to publish!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sciencemag.org/site/feature/contribinfo/faq/index.xhtml#prioronline_faq&quot;&gt;Science&lt;/a&gt; - The policy listed there sounds more conservative (phrases like “most cases” and “contact the editors” don’t inspire confidence).  Luckily, actually contacting the editors does: Deputy editor Brooks Hanson writes to me:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We allow posting on not-for-profit pre-print archives, so posting on &lt;a href=&quot;http://arxiv.org/&quot;&gt;arxiv.org&lt;/a&gt; is fine at any time.  we ask that if you post before acceptance that you do not indicate that the paper is submitted to Science.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As to lab notebooks, it would probably depend on what was in the “notebook” but if it were just notes or thoughts, it would probably not be a great concern.&lt;/p&gt;
&lt;h2 id=&quot;approximating-policies&quot;&gt;Approximating policies?&lt;/h2&gt;
&lt;p&gt;Many discipline-specific journals don’t state their policy on this as clearly in their advise to authors.  Of course writing to the editors is the best way to get clarification, but I think their are reasonable proxies to guess what’s okay and what’s not.&lt;/p&gt;
&lt;p&gt;Posting on preprint servers in advance of submission is probably a greater threat to the journal’s exclusive control of the content than the less-digestible and less accessible (decentralized) posting of an open notebook, and as it’s a more widespread practice it may be a good signal to look for.  F1000 also &lt;a href=&quot;http://posters.f1000.com/journalresponses&quot;&gt;compiled a list&lt;/a&gt; of journals that would consider archiving a poster as prepublication, which could be used as another proxy – if they are okay with posters, notebooks are probably in the clear. The &lt;a href=&quot;http://www.sherpa.ac.uk/romeo/issn/0036-8075/&quot;&gt;Sherpa/Romeo&lt;/a&gt; project provides links to the pre-publication policies of many journals.  Its database also identifies which journals allow what level of public archiving.  Search for your favorite journals there.  To the extent that “Yellow” refers “at the time of submission,” rather than post-publication, one might assume that publisher wouldn’t much object to an open notebook.  Of course, I’d love clarification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RoMEO Colour&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Archiving policy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sherpa.ac.uk/romeo/browse.php?colour=green&amp;amp;la=en&amp;amp;fIDnum=%7C&amp;amp;mode=simple&quot;&gt;Green&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Can archive pre-print &lt;em&gt;and&lt;/em&gt; post-print or publisher’s version/PDF&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sherpa.ac.uk/romeo/browse.php?colour=blue&amp;amp;la=en&amp;amp;fIDnum=%7C&amp;amp;mode=simple&quot;&gt;Blue&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Can archive post-print (ie final draft post-refereeing) or publisher’s version/PDF&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sherpa.ac.uk/romeo/browse.php?colour=yellow&amp;amp;la=en&amp;amp;fIDnum=%7C&amp;amp;mode=simple&quot;&gt;Yellow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Can archive pre-print (ie pre-refereeing)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sherpa.ac.uk/romeo/browse.php?colour=white&amp;amp;la=en&amp;amp;fIDnum=%7C&amp;amp;mode=simple&quot;&gt;White&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Archiving not formally supported&lt;/p&gt;
&lt;p&gt;For instance, looks like the Royal Society journals are okay too. They receive a Green status, &lt;a href=&quot;http://rsif.royalsocietypublishing.org/site/misc/preparing-articles.xhtml#question7&quot;&gt;permits&lt;/a&gt; pre-review author’s copy on preprint servers.  Published copy uses Creative Commons Attribution (cc-by) &lt;a href=&quot;http://royalsocietypublishing.org/site/authors/licence.xhtml&quot;&gt;licensing&lt;/a&gt;. Their duplicate publishing &lt;a href=&quot;http://royalsocietypublishing.org/site/authors/policy.xhtml&quot;&gt;statement&lt;/a&gt; makes no reference to archives, only other publishers.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How google views me: by search terms or citation counts?</title>
   <link href="/2011/09/09/how-google-views-me-by-search-terms-or-citation-counts.html"/>
   <updated>2011-09-09T09:55:39+00:00</updated>
   <id>/2011/09/09/how-google-views-me-by-search-terms-or-citation-counts</id>
   <content type="html">&lt;p&gt;How Google search views me: here’s a word cloud of search terms reaching my site in August.  Word cloud produced with R: click-through for link to source-code. Uses the rather convenient tm package for text-mining functions in R. Note that this shows the frequency of individual words used in searches, rather than the whole search term.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm7.staticflickr.com/6073/6128330822_c3f089a3da_o.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre class=&quot;sourceCode R&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;require&lt;/span&gt;(tm)
&lt;span class=&quot;kw&quot;&gt;require&lt;/span&gt;(wordcloud)
&lt;span class=&quot;kw&quot;&gt;require&lt;/span&gt;(RColorBrewer)

carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;read.csv&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;searchterms.csv&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;colClasses=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;character&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;numeric&amp;quot;&lt;/span&gt;))

words &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;character&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(carl[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]]))
m &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
for(i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(carl[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]])){
  n &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;carl[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]][i]
  x &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;carl[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]][i]
  words[m:(m+(n&lt;span class=&quot;dv&quot;&gt;-1&lt;/span&gt;))] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(x, n)
  m &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;m+n
}

 carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;Corpus&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;DataframeSource&lt;/span&gt;(carl))
 carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;tm_map&lt;/span&gt;(carl, removePunctuation)
 carl &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;tm_map&lt;/span&gt;(carl, tolower)

 carl.tdm &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;TermDocumentMatrix&lt;/span&gt;(carl)
 carl.m &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(carl.tdm)
 carl.v &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sort&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rowSums&lt;/span&gt;(carl.m), &lt;span class=&quot;dt&quot;&gt;decreasing=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
 carl.d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;data.frame&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;word=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;names&lt;/span&gt;(carl.v), &lt;span class=&quot;dt&quot;&gt;freq=&lt;/span&gt;carl.v)

pal2 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;brewer.pal&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;st&quot;&gt;&amp;quot;Dark2&amp;quot;&lt;/span&gt;)

&lt;span class=&quot;kw&quot;&gt;png&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;wordcloud.png&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;800&lt;/span&gt;,&lt;span class=&quot;dt&quot;&gt;height=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;800&lt;/span&gt;) 
&lt;span class=&quot;co&quot;&gt;#larger canvas doesn&amp;#39;t increase plot size&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;wordcloud&lt;/span&gt;(carl.d$word,carl.d$freq, &lt;span class=&quot;dt&quot;&gt;scale=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;8&lt;/span&gt;,.&lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;),&lt;span class=&quot;dt&quot;&gt;min.freq=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;,
&lt;span class=&quot;dt&quot;&gt;max.words=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;Inf&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;random.order=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;rot.per=&lt;/span&gt;.&lt;span class=&quot;dv&quot;&gt;15&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;colors=&lt;/span&gt;pal2)
&lt;span class=&quot;kw&quot;&gt;dev.off&lt;/span&gt;()

&lt;span class=&quot;kw&quot;&gt;require&lt;/span&gt;(socialR)
&lt;span class=&quot;kw&quot;&gt;upload&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;wordcloud.png&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;script=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;wordcloud.R&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s another way that Google views me: results from the &lt;a href=&quot;http://code.google.com/p/citations-gadget/&quot;&gt;Google citation gadget&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Citations for “Carl Boettiger”: &lt;strong&gt;77&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cited Publications: &lt;strong&gt;18&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;H-Index: &lt;strong&gt;5&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hmm… I think the wordcloud approach might be more accurate… Perhaps no one needed reminding not to trust these kinds of tools, despite the predilection to believe anything quantitative (See the real story on my CV tab (pdf). The other lesson might be that alternative forms of publishing are likely to get picked up by some of these algorithms. Many of the &lt;a href=&quot;http://scholar.google.com/scholar?start=0&amp;amp;q=author:%22Carl+Boettiger%22&amp;amp;hl=en&amp;amp;lr=lang_en&amp;amp;as_sdt=0,5&quot;&gt;items&lt;/a&gt; that come up are talks I’ve deposited on Nature Precedings, and a few are even just notebook posts. Interesting that there’s a growing number of services designed to help scientists strengthen their review dossier using these tools.&lt;/p&gt;
&lt;p&gt;Does this mean that alternative publishing (&lt;em&gt;i.e.&lt;/em&gt; conference slides, prepints, notebook posts) is a good way to trick the metrics? That people won’t trust the numbers of bloggers, etc, assuming they are inflated? Or that these kinds of communication methods may gain acceptance as part of the scientific discourse?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Citation tools & Future of Publishing</title>
   <link href="/2011/02/16/citation-tools-future-of-publishing.html"/>
   <updated>2011-02-16T23:04:50+00:00</updated>
   <id>/2011/02/16/citation-tools-future-of-publishing</id>
   <content type="html">&lt;p&gt;There has been a lot of rapid development in scientific tools based on a Wordpress platform recently, perhaps spurred in part by the recent &lt;a href=&quot;https://sites.google.com/site/beyondthepdf/&quot;&gt;Beyond-the-PDF&lt;/a&gt; and &lt;a href=&quot;http://blogs.plos.org/mfenner/2011/01/11/having-fun-with-citations-at-scienceonline2011/&quot;&gt;sessions in Science Online&lt;/a&gt; conferences.  A new &lt;a href=&quot;https://groups.google.com/forum/#!forum/wordpress-for-scientists&quot;&gt;discussion group&lt;/a&gt; has emerged around these tools, &lt;a href=&quot;http://blogs.plos.org/mfenner/2011/02/04/discussing-wordpress-for-scientists/&quot;&gt;as described by Martin Fenner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have been exploring better tools for citation management within my lab notebook. Recently I have been using the &lt;a href=&quot;http://wordpress.org/extend/plugins/papercite/&quot;&gt;papercite plugin&lt;/a&gt;to add citations from my BibTeX files, which are generated from Mendeley, as I &lt;a href=&quot;http://www.carlboettiger.info/archives/570&quot;&gt;described earlier&lt;/a&gt;.  This has a few performance issues on server load and memory, and a somewhat limited format/appearance options, but more importantly lacks some of the features of being developed in these other citation packages.&lt;/p&gt;
&lt;p&gt;To begin, see Martin’s very compelling piece on  &lt;a href=&quot;http://blogs.plos.org/mfenner/2010/12/11/citations-are-links-so-where-is-the-problem/&quot;&gt;citations are links&lt;/a&gt;.  Certainly a link (&lt;em&gt;i.e.&lt;/em&gt;, to the pdf article) can be  &lt;em&gt;part &lt;/em&gt;of a citation, but there’s a lot that can be done once we start thinking of the citation itself as a link.  Wordpress has a nice interface to handle and organize links, which until now has mostly puzzled me as a overdeveloped blogroll.  The &lt;a href=&quot;http://wordpress.org/extend/plugins/bibtex-importer/&quot;&gt;Import BibTeX&lt;/a&gt; plugin can read my bibliography files and transform each citation into a link in the Wordpress interface, attaching categories corresponding to the source type and to tags I add, (such as to which Mendeley collection the article belongs).  Then the &lt;a href=&quot;http://wordpress.org/extend/plugins/link-to-link/&quot;&gt;Link to Link&lt;/a&gt; plugin makes it easy to search and insert these references into my posts.  Currently this can’t be done using the bibtex keys, which is a minor bother.&lt;/p&gt;
&lt;p&gt;Here’s where things start to get more interesting.  The links can embed machine-readable semantic information that adds meta-data to the citation, such as if the citation provides background, methods, supports or disagrees with a claim being made.  This can be made visible to the user, but more interestingly this can also be tracked by a computer, which allows for the generation of much more detailed citation statistics.  A standard Citation Typing Ontology has been developed and described by David Shotton[cite source=‘doi’]10.1186/2041-1480-1-S1-S6[/cite] and &lt;a href=&quot;http://blogs.plos.org/mfenner/2011/02/14/how-to-use-citation-typing-ontology-cito-in-your-blog-posts/&quot;&gt;CiTO is now implemented&lt;/a&gt;in Link to Link code.  While Link to Link seems to have the 10 most common citation types, the ontology provides many more, as listed in the  &lt;a href=&quot;http://imageweb.zoo.ox.ac.uk/pub/2009/citobase/cito-20091124-1.4/cito-content/owldoc/&quot;&gt;CiTO types&lt;/a&gt; documentation.&lt;/p&gt;
&lt;p&gt;Finally, the automatic bibliographic information is displayed by the &lt;a href=&quot;http://wordpress.org/extend/plugins/kcite/&quot;&gt;Kcite&lt;/a&gt; plugin, currently using the DOI or PMID, which integrates with the Link to Link code.  Having installed both plugins and checked the configuration boxes in link to link which allow it to use the kcite shortcode and CiTO syntax, I’m able to highlight some text, select the citation through the search box opened by clicking the Link to Link icon and have it automatically insert the Kcite shortcode using the DOI:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[cite source=&amp;#39;doi&amp;#39; rel=cito:cites]10.1186/2041-1480-1-S1-S6[/cite]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the moment this doesn’t quite work as it seems the rel=cito text disrupts the kcite intepretation of the shortcode.  Without this it renders correctly.&lt;/p&gt;
&lt;p&gt;Note that in general Link to Link with CiTO works with anything with a URL, and will still embed the CiTO information in the link if no DOI is available, just not add the kcite bibliography.&lt;/p&gt;
&lt;p&gt;Also not sure why the kcite bibliography includes the link to the json format, seems it shouldn’t display this.  Still active discussion on the mailing list on these topics, and it’s been quite impressive to see how fast development has been moving on these so far.&lt;/p&gt;
&lt;h3 id=&quot;things-id-like-to-see&quot;&gt;THings I’d like to see&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to expose/search CiTO meta-data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;kcite option for citing by (Author,year) footnote text, not just by number, like so [1].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Basic fixes: Kcite currently errors with the rel=cito:cites text.  Should at least ignore that text until support is built in for CiTO.  Apparently link2link should be rendering this with quotes: rel=cito:‘cites’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;toggle json display.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;why-store-citations-in-a-link-library-in-wordpress&quot;&gt;Why store Citations in a link library in Wordpress?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Link2Link requires uploading Bibtex file to add the reference to wordpress link library.  Mendeley automatically generates the bibliography – would be great if this could be automatically placed online (i.e. by dropbox) and read in by Wordpress rather than having to manually update.  Having to import bibtex every time I add a new article, and then search for the added article via link2link, is rather inefficient.  Can merely grab the doi from Mendeley and insert with kcite.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The most obvious advantage of having the references in the Wordpress link library is the ability for link2link to search them and grab the DOI, though searching Mendeley for the DOI is just as easy.  So why else would this be useful?  A few thoughts:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The link library allows for adding semantic markup to the link, though currently Wordpress  does this only for &lt;a href=&quot;http://codex.wordpress.org/Defining_Relationships_with_XFN&quot;&gt;XFN&lt;/a&gt; (XHTML Friends Network), designed to describe people.  Having CiTO directly in the link editor beside XFN would be nice, though not particularly more functional.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Potential to use blogroll functions of wordpress to display the library listed in the links, though in general Mendeley groups seem a much more powerful way to do this.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Data Mangement on UC3 Merritt Repository</title>
   <link href="/2011/02/16/data-mangement-on-uc3-merritt-repository.html"/>
   <updated>2011-02-16T19:06:11+00:00</updated>
   <id>/2011/02/16/data-mangement-on-uc3-merritt-repository</id>
   <content type="html">&lt;p&gt;I’ve been trying to learn a little more about the potential for data management solutions through the &lt;a href=&quot;http://www.cdlib.org/services/uc3/merritt/index.html&quot;&gt;UC3 Merritt repository&lt;/a&gt;, and decide how this compares to commercial alternatives such as Amazon S3, Picasa/Flickr (obviously for images only), and field-specific and publication specific repositories such as Dryad.  Merritt email support have been very helpful in answering my questions and concerns.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;:  of the data itself, of the  URLs &amp;amp; object identifiers, of access to the data?  Not surprisingly they have standard redundancy backup procedures for archival standards in place.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object identifiers&lt;/strong&gt;:  UC3 also provides DOIs for objects which can be purchased independently of archiving in the Merritt repository and are indexed in Web of Science.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost and pricing structure&lt;/strong&gt;: $1040/Tb/year, prorated for how much storage you actually use.  No charge for upload/download, unlike S3.  &lt;a href=&quot;http://www.cdlib.org/services/uc3/ezid/index.html&quot;&gt;EZID&lt;/a&gt; for DOIs are separate charge:  Annual charge unlimited number of DOIs: $75/user, $375/group, or $1,125 for whole university&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Good:&lt;/strong&gt; An institutional repository seems like a safer bet than an Internet business, even if it’s as large as Amazon or Google.  After all, the UC’s have been around a lot longer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Not-So-Good&lt;/strong&gt;: The big concern for me here is that this is guaranteed (both for IDs and for data itself) only as long as you are (or someone else is) paying the annual fee.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other Concerns:&lt;/strong&gt; Can I automate script to upload, download, and embed data?  The wealth of tools available for Flickr or Github through the user-extensible API makes it very easy to integrate these repositories into my workflow.  I am still unclear how easy this would be with the UC3 archiving services.&lt;/p&gt;
&lt;p&gt;Still promising: The staff recognize these challenges, particularly in their contrast to the standard short-term grant-based funding model of science and in working with the research institutions for support covering these costs.  Further, it seems the details aren’t set in stone yet, but are an evolving part of the system:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s a tough question for which we don’t have a comprehensive answer at the moment. Our fee model does not match up well with the funding model of grant projects–our costs are ongoing, our accounting rules prohibit us from carrying funds across fiscal years, yet our customers may have funding for only a limited time period.  We want to partner with organizations committed to the long-term preservation of their digital content. We would work with our partners to find new organizational homes if their commitment changed, due to changes in funding and priorities. This isn’t really an answer, other than to say that we view the relationship as a partnership, and we want to help our partners succeed in their goals for long-term preservation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;–Merritt helpdesk.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Future of Data: Plans for a UC Davis workshop</title>
   <link href="/2011/02/01/the-future-of-data-preparing-researchers-to-face-new-funding-and-publishing-policies.html"/>
   <updated>2011-02-01T09:40:59+00:00</updated>
   <id>/2011/02/01/the-future-of-data-preparing-researchers-to-face-new-funding-and-publishing-policies</id>
   <content type="html">&lt;p&gt;I have been working with several of the faculty of UC Davis to prepare a workshop for our department to (1) help make our researchers &lt;strong&gt;aware&lt;/strong&gt; of the new requirements for NSF proposals and journal submissions with regard to data archiving, (2) To explore what &lt;strong&gt;new opportunities&lt;/strong&gt; this creates and what &lt;strong&gt;tools&lt;/strong&gt; exist that will help our researchers take maximum advantage of it, and (3) to engage our research community in the discussion of &lt;strong&gt;future standards&lt;/strong&gt; for sharing, standardizing, rewarding and funding the archiving of data.&lt;/p&gt;
&lt;p&gt;I’m very pleased that we have an excellent panel of supportive faculty:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://strong.ucdavis.edu/&quot;&gt;Don Strong&lt;/a&gt;, Editor-in-Chief of &lt;strong&gt;Ecology&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.des.ucdavis.edu/faculty/holyoak/&quot;&gt;Marcel Holyoak&lt;/a&gt;, Editor-in-Chief, &lt;strong&gt;Ecology Letters&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://phylogenomics.blogspot.com&quot;&gt;Jonathan Eisen,&lt;/a&gt; Editor-in-Chief, &lt;strong&gt;PLoS Biology&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://two.ucdavis.edu/~me/&quot;&gt;Alan Hastings&lt;/a&gt;, founding Editor-in-Chief, &lt;strong&gt;Theoretical Ecology&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.des.ucdavis.edu/FacultyInfo.aspx?ID_Number=83&quot;&gt;Robert Hijmans&lt;/a&gt;, Asst Professor of Environmental Science &amp;amp; Policy and expert in software infrastructure for reproducible research.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cdlib.org/cdlinfo/2010/05/19/patricia-cruse-digital-preservation-pioneer/&quot;&gt;Trisha Cruse&lt;/a&gt; Director of UC Curation Center at the University of California Digital Library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are currently working out the format of the workshop.  The current draft consists of three 1.5hr sessions, progressing through the three main goals above in an interactive format.&lt;/p&gt;
&lt;h3 id=&quot;why-archive&quot;&gt;1) Why Archive?&lt;/h3&gt;
&lt;h5 id=&quot;weds-march-2nd-230-4p-lsa-1022&quot;&gt;(Weds March 2nd 2:30-4p, LSA 1022)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;New &lt;a href=&quot;http://www.cdlib.org/services/uc3/datamanagement/funding.html&quot;&gt;requirements from funders&lt;/a&gt; and publishers.&lt;/li&gt;
&lt;li&gt;Recommended practice: What’s ideal, what’s acceptable, what isn’t?&lt;/li&gt;
&lt;li&gt;Types/levels of archiving.  Why repositories?&lt;/li&gt;
&lt;li&gt;Funding data repositories.&lt;/li&gt;
&lt;li&gt;Access embargoes.  Citing data/getting credit.&lt;/li&gt;
&lt;li&gt;Panel Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;leveraging-data-archives-logistics&quot;&gt;2) Leveraging data archives: logistics&lt;/h3&gt;
&lt;h5 id=&quot;thurs-march-3nd-230-4p-pes-3001&quot;&gt;(Thurs March 3nd 2:30-4p, PES 3001)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;The major repositories.&lt;/li&gt;
&lt;li&gt;Discovering data.&lt;/li&gt;
&lt;li&gt;Citing data.  Metadata, DOIs, formats, persistent data&lt;/li&gt;
&lt;li&gt;Sharing/archiving code?&lt;/li&gt;
&lt;li&gt;Linked data, ontologies &amp;amp; Semantic web.&lt;/li&gt;
&lt;li&gt;Tools for reproducible research.&lt;/li&gt;
&lt;li&gt;Panel Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;looking-ahead-new-scale-new-science&quot;&gt;3) Looking ahead: new scale, new science?&lt;/h3&gt;
&lt;h5 id=&quot;fri-march-4nd-230-4p-pes-3001&quot;&gt;(Fri March 4nd 2:30-4p, PES 3001)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Challenges and Opportunities (&lt;em&gt;i.e.&lt;/em&gt; &lt;a href=&quot;http://www.sciencemag.org/content/331/6018/692.short&quot;&gt;&lt;em&gt;Science&lt;/em&gt; issue&lt;/a&gt;,&lt;a href=&quot;http://www.sciencemag.org/content/331/6018/703.short&quot;&gt;&lt;em&gt;Science&lt;/em&gt; re: Ecology&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Integrated databases, standards for data&lt;/li&gt;
&lt;li&gt;Dryad “publication model” vs DataONE model, etc&lt;/li&gt;
&lt;li&gt;Reproducible research&lt;/li&gt;
&lt;li&gt;UC3’s “data publication” concept?&lt;/li&gt;
&lt;li&gt;Future of credit: Will you be scooped or will you be famous?&lt;/li&gt;
&lt;li&gt;What roll we can/should have in shaping future standards, expectations &amp;amp; requirements&lt;/li&gt;
&lt;li&gt;Panel Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;p&gt;See the recent discussion of these issues in &lt;a href=&quot;http://www.sciencemag.org/cgi/doi/10.1126/science.331.6018.692&quot;&gt;Science&lt;/a&gt;, &lt;a href=&quot;http://www.nature.com/doifinder/10.1038/nj7333-295a&quot;&gt;Nature&lt;/a&gt;, &lt;a href=&quot;http://linkinghub.elsevier.com/retrieve/pii/S0169534710002697&quot;&gt;TREE&lt;/a&gt;, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20073990&quot;&gt;Am Nat&lt;/a&gt;, and elsewhere.   I have started assembling a list of resources in my December notes: &lt;em&gt;&lt;a href=&quot;http://www.carlboettiger.info/archives/502&quot;&gt;Thoughts on the New Policies for Data Archiving at NSF and in Common Journals&lt;/a&gt;. &lt;/em&gt;In addition, there’s more detail available on the &lt;a href=&quot;https://www.nescent.org/wg_dryad/Main_Page&quot;&gt;Dryad Wiki&lt;/a&gt; and from Dryad here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.datadryad.org/depositing&quot;&gt;Depositing Data&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.datadryad.org/using&quot;&gt;Using Data&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.datadryad.org/partners&quot;&gt;Dryad Partners (Publishers)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.datadryad.org/jdap&quot;&gt;Archiving Policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some great information also available from UC3:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cdlib.org/services/uc3/datamanagement&quot;&gt;UC3 Data Management Guidelines.&lt;/a&gt; In addition to summarizing the requirements, UC3 is also developing a “TurboTax” tool for data management:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In the next month or so we are going to roll out a rule based tool that will ask the researcher a series of questions based on the funding organization that are applying to and will generate a Data Management Plan.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://merritt.cdlib.org/&quot;&gt;Merritt Repository services&lt;/a&gt;,  launched in late summer 2010.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.cdlib.org/services/uc3/ezid/&quot;&gt;EZID Identifier services&lt;/a&gt;:   obtain and manage long-term identifiers (DOI, ARK) for their digital content.  Can assign identifiers to anything: scientific datasets, technical reports, audio files, digital photographs, and non-digital objects as well.  This spring the objects with DOIs  will appear in Web of Science. (Annual charge unlimited number of DOIs: $75/user, $375/group, or $1,125 for whole university)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consultation services: Individual consultation, also more workshops  on data citation, data sharing, and data rights this Fall.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;We have been actively thinking about data and building services that support the management, preservation, and publication of data.  We are just about to embark on a project that will create an Excel add-in that will make it easy to archive, share, and publish data embedded in Excel.  We are also actively involved in the DataONE project.  We recently presented &lt;a href=&quot;http://www.cdlib.org/services/uc3/docs/dax.pdf&quot;&gt;a paper&lt;/a&gt; at the Beyond the PDF workshop at UCSD that outlines some of our ideas surrounding data publication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;your-input&quot;&gt;Your Input?&lt;/h2&gt;
&lt;p&gt;What would you like to see?  Structure, organization, topics?&lt;/p&gt;
&lt;p&gt;[iframe http://friendfeed.com/science-2-0/e68945dc/i-m-planning-workshop-at-ucdavis-on-future-of?embed=1 480 400]&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reflections from #scio11: Saturday's Open Science Track</title>
   <link href="/2011/01/15/reflections-from-scio11-saturdays-open-science-track.html"/>
   <updated>2011-01-15T23:12:45+00:00</updated>
   <id>/2011/01/15/reflections-from-scio11-saturdays-open-science-track</id>
   <content type="html">&lt;p&gt;Have just returned from the amazing and energetic science online 2011 conference, where I will now, like the rest of the attendees, turn to catching up on sleep, following new online acquaintances, struggling not to append #scio11 to everything I write, and composing that nearly obligatory post-event blog entry we all use like a glass of water after a party; with the hope in may help me metabolize the experience.&lt;/p&gt;
&lt;p&gt;Never mind that I don’t actually blog. I just have this lab notebook, normally filled with equations, graphs and code. But it is recordings of my research experience, even if it is written only for me most of the time, so let’s just call this part of research and forgive the conversational tone. Meanwhile I will use the excuse to write more at length for myself the notes I wish to remember than would be seemly in writing in a blog with an audience.  In the spirit of this resembling my daily notebook and also in actually finishing this post, here are my notes and thoughts on Saturday’s amazing blur. &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;saturday&quot;&gt;Saturday&lt;/h3&gt;
&lt;p&gt;I began the day in Kay’s (@&lt;a href=&quot;http://twitter.com/#!/kaythaney&quot;&gt;kaythaney&lt;/a&gt;) Digital Toolbox session, room C, where I would return for each of my sessions that day. It was one of the live-streamed rooms, so all the following sessions should have &lt;a href=&quot;http://scienceonline2011.com/watch-2/&quot;&gt;archived recordings&lt;/a&gt;.  The sessions that followed sequentially in this room fit together like a perfectly flowing conversation, as if each one responding to the former and anticipating the next.  Just further evidence that Bora (&lt;span class=&quot;citation&quot; data-cites=&quot;Boraz&quot;&gt;[@Boraz]&lt;/span&gt;(http://twitter.com/#!/boraz)) and Anton (&lt;span class=&quot;citation&quot; data-cites=&quot;mistersugar&quot;&gt;[@mistersugar]&lt;/span&gt;(http://twitter.com/#!/mistersugar)) must come from magic lamps.&lt;/p&gt;
&lt;p&gt;Next up brought one of my existing luminaries, &lt;a href=&quot;http://twitter.com/#!/skoch3&quot;&gt;Steve Koch&lt;/a&gt;),&lt;a href=&quot;#fn2&quot; class=&quot;footnoteRef&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, with  &lt;a href=&quot;http://libraryadventures.com/&quot;&gt;Kiyomi Deards&lt;/a&gt; and &lt;a href=&quot;http://cloud.lib.wfu.edu/blog/pd/&quot;&gt;Molly Keener&lt;/a&gt; to the panel on data discover-ability and archiving in Open Notebook Science. The session erupted in lively debate on the relative merits of commercial hosting solutions which may offer great discoverability and ease of use until they suddenly go the way of Del.icio.us, or institutional repositories, which are long lived but often unintegrated even amongst themselves. The good-spirited solutions all pointed to doing both and trying to make each better.&lt;/p&gt;
&lt;p&gt;Not that we wouldn’t also question that optimism too, for the next session asked us: “What’s keeping us from open science?” Covering many things, at the heart of this session was the crux of lacking incentives. The promise of altmetrics loomed large, but &lt;a href=&quot;http://friendfeed.com/stevekoch/a0823501/scio11-good-question-from-carl-should-emphasis&quot;&gt;I wondered&lt;/a&gt; if we needed to do focus more on making impact with open science than merely measuring it. For instance, we can surely try to measure the impact of open notebooks and science tweets, but tools for better discovery, standardization, or more semantic open content may go much father to having unmistakable impact.&lt;/p&gt;
&lt;p&gt;The schedule indicates &lt;a href=&quot;#fn3&quot; class=&quot;footnoteRef&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; that lunch happened next. though he whirling discussions left me with lots of ideas and names than memories of food or thoughts to my own upcoming panel. But no matter, I would be sitting with the heroes Jean-Claude Bradley (&lt;span class=&quot;citation&quot; data-cites=&quot;jcbradley&quot;&gt;@jcbradley&lt;/span&gt;) and Anthony Williams (&lt;span class=&quot;citation&quot; data-cites=&quot;ChemConnector&quot;&gt;@ChemConnector&lt;/span&gt;), so why worry?&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://m.friendfeed-media.com/a9192c4f24f3b1cc96c306e48bad1c498cab0c23&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Our session on Open Notebook Science quickly gave us reason to worry when the projector which had done fine thus far suddenly refused to work. Nevermind, we all work with technology too frequently to be unfamiliar with its failure - we would do without. If you were there and you’re wondering what you missed, here are my slides.  See &lt;a href=&quot;http://usefulchem.blogspot.com/2011/01/science-online-2011-thoughts.html&quot;&gt;Jean-Claude’s post&lt;/a&gt; for more on our session.&lt;/p&gt;
&lt;iframe src=&quot;http://www.slideshare.net/slideshow/embed_code/6606229&quot; width=&quot;427&quot; height=&quot;356&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px&quot; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; 
&lt;strong&gt; &lt;a href=&quot;http://www.slideshare.net/cboettig/scioslides&quot; title=&quot;Scioslides&quot; target=&quot;_blank&quot;&gt;Scioslides&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;http://www.slideshare.net/cboettig&quot; target=&quot;_blank&quot;&gt;Carl Boettiger&lt;/a&gt;&lt;/strong&gt;
&lt;/div&gt;


&lt;p&gt;I described the tools I use to make my notebook more automated and complete and discussed my vision of a social lab notebook.  In face of the observation that many open database efforts face the tragedy of commons effect (many researchers may use the resources but few contribute) I discussed some of the many immediate benefits the practice has had for me.  I must say what an immense treat it is to be able to finish a session and see the immediate feedback and reactions it has generated through the online streams.&lt;/p&gt;
&lt;p&gt;The video recording of our session has now been posted:&lt;/p&gt;
&lt;iframe src=&quot;http://player.vimeo.com/video/20100632&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitAllowFullScreen mozallowfullscreen allowFullScreen&gt;&lt;/iframe&gt; &lt;p&gt;
&lt;a href=&quot;http://vimeo.com/20100632&quot;&gt;Open Notebook Science: Pushing Data from Bench to Web Service&lt;/a&gt; from &lt;a href=&quot;http://vimeo.com/user2419982&quot;&gt;Smartley-Dunn&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;http://scienceonline2011.com/watch-2/&quot;&gt;recordings of the other sessions&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Having broached the idea of &lt;a href=&quot;http://altmetrics.org/manifesto/&quot;&gt;altmetrics&lt;/a&gt; on several occasions already, our next session dove into the details of how we might construct these, what they might measure, and how they would be received. Or as it was more concisely and provocatively phrased by Paul Groth (&lt;span class=&quot;citation&quot; data-cites=&quot;pgroth&quot;&gt;[@pgroth]&lt;/span&gt;(http://twitter.com/#!/pgroth)):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Would you list the number of twitter followers you have on your CV?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The last session dived further into tackling and improving the most common metric: citations. From citations that map to specific parts of articles to ones measuring the sentiment (supports/refutes) or even anticipating the social-cultural context of citations, the potential here is amazing. Jason Hoyt’s (&lt;span class=&quot;citation&quot; data-cites=&quot;jasonHoyt&quot;&gt;[@jasonHoyt]&lt;/span&gt;(http://twitter.com/#!/jasonhoyt)) 140 characters may have captured this session best:&lt;img src=&quot;http://m.friendfeed-media.com/f22753e4ef38225cbad94b61e22a9e5fdb701f23&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;if I do my job right, no one will care about publishing in Nature, Science or Cell…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indicating that better article discovery and citation tracking through tools such as Mendeley would make the impact factor obsolete.&lt;/p&gt;
&lt;p&gt;The evening banquet featured many highlights, including seeing one of my fellow graduate students in the video competition, the very moving story of Meg Lowman’s (&lt;span class=&quot;citation&quot; data-cites=&quot;canopymeg&quot;&gt;[@canopymeg]&lt;/span&gt;(http://twitter.com/#!/canopymeg)) work researching and inspiring rain-forest conservation, laughing tears at Brian Marlow’s (&lt;span class=&quot;citation&quot; data-cites=&quot;sciencecomedian&quot;&gt;[@sciencecomedian]&lt;/span&gt;(http://twitter.com/#!/sciencecomedian)) humorist routine, and talks deep into the night about &lt;a href=&quot;http://drexel-coas-elearning.blogspot.com/&quot;&gt;innovated ideas in open education&lt;/a&gt; with JC and then the future of publishing with Jason, Mark and Tony (&lt;span class=&quot;citation&quot; data-cites=&quot;jasonPriem&quot;&gt;[@jasonPriem]&lt;/span&gt;(http://twitter.com/#!/jasonpriem), &lt;span class=&quot;citation&quot; data-cites=&quot;science3point0&quot;&gt;[@science3point0]&lt;/span&gt;(http://twitter.com/#!/science3point0), and &lt;span class=&quot;citation&quot; data-cites=&quot;ChemConnector&quot;&gt;[@ChemConnector]&lt;/span&gt;(http://twitter.com/#!/ChemConnector)).&lt;/p&gt;
&lt;p&gt;Many other great discussions throughout the day on and off line that I cannot begin to capture here.  But after all, they are just beginning.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Note this was written up on Jan 17 upon returning to reality with enough time to write, but published under the event date for notebook consistency. Touched up for the Jekyll notebook 2013-07-09, see history in sidebar.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;pronounced cook, I learned, and all of a sudden understood the &lt;a href=&quot;http://openwetware.org/wiki/User:Skoch3&quot;&gt;chef’s hat symbol&lt;/a&gt; for their lab that has puzzled me for a year!&lt;a href=&quot;#fnref2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Also, the schedule lists the full topics and presenters for these sessions, for which reason I should  &lt;a href=&quot;http://scio11.wikispaces.com/Program&quot;&gt;work in a link  to it somewhere.&lt;/a&gt;&lt;a href=&quot;#fnref3&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts on the New Policies for Data Archiving at NSF and in Common Journals</title>
   <link href="/2010/12/03/nsf-and-journal-data-archiving.html"/>
   <updated>2010-12-03T19:08:53+00:00</updated>
   <id>/2010/12/03/nsf-and-journal-data-archiving</id>
   <content type="html">&lt;p&gt;This post is a work in progress, a scratch pad for me to start assembling what I’ve been learning about and resources pertaining to the new policies emerging from NSF and journals relevant to ecology and evolution.  Hoping to highlight not only the policies, but the issues, opportunities, and concerns around them.&lt;/p&gt;
&lt;p&gt;I am hoping to help organize a workshop to discuss these issues in my department this Winter.  The Davis Open Science group, is planning a series of these workshops, hoping to work with departments, the libraries and the UC Davis Office of Research and its &lt;a href=&quot;http://www.research.ucdavis.edu/home.cfm?id=OVC,10,1622,2065&quot;&gt;Responsible Conduct of Research&lt;/a&gt; program (in compliance with NIH/NSF ethics requirements), as well as resident faculty and editors.  Suggestions on what you would like to see in such a workshop much appreciated.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Association of Research Libraries has just released a &lt;a href=&quot;http://www.arl.org/rtl/eresearch/escien/nsf/index.shtml&quot;&gt;nice guide on the new polices&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-are-the-new-nsf-policies&quot;&gt;What are the new NSF policies?&lt;/h2&gt;
&lt;p&gt;Each discipline has (or will have, &lt;a href=&quot;http://researchremix.wordpress.com/2010/11/15/nsf-where&quot;&gt;see notes and refs  by Heather Piwowar&lt;/a&gt; it’s own guidelines, but the basic gist is perhaps best summarized in excerpt from &lt;a href=&quot;http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp&quot;&gt;this NSF statement&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;This supplement should describe how the proposal will conform to NSF policy on the dissemination and sharing of research results (see AAG Chapter VI.D.4 ), and may include:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;The types of data, samples, physical collections, software, curriculum materials, and other materials to be produced in the course of the project;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the standards to be used for data and metadata format and content (where existing standards are absent or deemed inadequate, this should be documented along with any proposed solutions or remedies);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;policies for access and sharing including provisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;policies and provisions for re-use, re-distribution, and the production of derivatives; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;plans for archiving data, samples, and other research products, and for preservation of access to them.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Data management requirements and plans specific to the Directorate, Office, Division, Program, or other NSF unit, relevant to a proposal are available at: &lt;a href=&quot;http://www.nsf.gov/bfa/dias/policy/dmp.jsp&quot;&gt;http://www.nsf.gov/bfa/dias/policy/dmp.jsp&lt;/a&gt;. If guidance specific to the program is not available, then the requirements established in this section apply. [….] The Data Management Plan will be reviewed as an integral part of the proposal, coming under Intellectual Merit or Broader Impacts or both, as appropriate for the scientific community of relevance.&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-new-requirements-being-set-by-journals&quot;&gt;What are the new requirements being set by journals?&lt;/h2&gt;
&lt;p&gt;The American Naturalist&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This journal requires, as a condition for publication, that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, TreeBASE, Dryad, or the Knowledge Network for Biocomplexity. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Whitlock, M.C. et al. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20073990&quot;&gt;Data archiving&lt;/a&gt;. The American Naturalist 175, 145-6(2010).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Evolution&lt;/em&gt; has a similar policy, outlined in:&lt;/p&gt;
&lt;p&gt;Fairbairn, D.J. &lt;a href=&quot;http://doi.wiley.com/10.1111/j.1558-5646.2010.01182.x&quot;&gt;the Advent of Mandatory Data Archiving&lt;/a&gt;. Evolution (2010) (preprint)&lt;/p&gt;
&lt;p&gt;Journals with policy in place (links and editions coming)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The American Naturalist&lt;/strong&gt; (see Whitlock, 2010; above)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Evolution&lt;/strong&gt; (see Fairbarin, 2010; above)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Journal of Evolutionary Biology&lt;/strong&gt; (as mentioned in Whitlock)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Molecular Ecology&lt;/strong&gt; (as mentioned in Whitlock)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Heredity&lt;/strong&gt; (as mentioned in Whitlock)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ESA journals: &lt;strong&gt;Ecology&lt;/strong&gt;, &lt;strong&gt;Ecological&lt;/strong&gt; &lt;strong&gt;Monographs&lt;/strong&gt;, &lt;strong&gt;Ecological&lt;/strong&gt; &lt;strong&gt;Applications&lt;/strong&gt;, and ** Frontiers**.  Currently have a position in Dryad board and a soft data deposition requirement.  Also maintains the Ecological Archive. (pers. comm.)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-are-the-available-archives&quot;&gt;What are the available archives?&lt;/h2&gt;
&lt;p&gt;There’s a variety of potential archives.  Identifying the best archive for the type of data involved may be complicated.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://esapubs.org/archive/default.htm&quot;&gt;ESA’s Ecological Archive&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://datadryad.org/&quot;&gt;Dryad&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.treebase.org/treebase-web/home.html&quot;&gt;TreeBASE&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/genbank/&quot;&gt;GenBank&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://knb.ecoinformatics.org/index.jsp&quot;&gt;Knowledge Network for Biocomplexity&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;funding-models-concerns&quot;&gt;Funding Models &amp;amp; Concerns&lt;/h2&gt;
&lt;p&gt;Cost is a key concern: good archiving takes resources, and does little good if repositories aren’t maintained or persistent.  While the repositories are supported by their own grants (for the moment), it seems the repositories will generally charge the  journals where the publication accompanying the data will be submitted.  The journals may in turn charge the authors, who will have to write these costs into their NSF grants?&lt;/p&gt;
&lt;p&gt;Like the policies themselves, these details are still being actively worked out [ref] Todd Vision, associate director of informatics at NESCENT which oversees the Dryad project currently, discusses this in &lt;a href=&quot;http://ff.im/uGFbn&quot;&gt;this FF forum&lt;/a&gt;.  [/ref].&lt;/p&gt;
&lt;p&gt;Good references on funding models include these papers by Beagerie and others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://bit.ly/fPX0Nm&quot;&gt;Dryad cost model potential&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.jisc.ac.uk/media/documents/publications/keepingresearchdatasafe0408.pdf&quot;&gt;a cost model for universities in the UK&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;why-archive-whats-the-added-value-of-repositories&quot;&gt;Why Archive?  What’s the added value of Repositories?&lt;/h2&gt;
&lt;p&gt;Dryad has a good explanation of &lt;a href=&quot;http://blog.datadryad.org/2010/12/02/what-happens-after-you-submit-your-data-to-dryad/&quot;&gt;what happens when data is deposited&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The need for data archiving in ecology and evolutionary biology has been persuasively argued in articles in a variety of our journals for some time.    See:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Moore, A.J. et al. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20149022&quot;&gt;The need for archiving data in evolutionary biology.&lt;/a&gt;Journal of evolutionary biology 23, 659-60(2010).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Piwowar, H. a, Day, R.S. &amp;amp; Fridsma, D.B. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/17375194&quot;&gt;Sharing detailed research data is associated with increased citation rate&lt;/a&gt;. PloS one 2, e308(2007).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;impact-of-sharing-and-citing-data&quot;&gt;Impact of Sharing and Citing Data&lt;/h2&gt;
&lt;p&gt;In addition to funding, primary concerns involve how data will be shared and cited.  Done well, researchers are appropriately accredited and rewarded when there data is re-used, making research faster, supported by larger data sets, more reproducible, and carrying a broader impact.  A set of articles highlights many of these issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;﻿﻿﻿Thorisson, G.A. &lt;a href=&quot;http://dx.doi.org/10.1038/nbt1109-984b&quot;&gt;Accreditation and attribution in data sharing&lt;/a&gt;. Nature Biotechnology 27, 984-985(2009).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sieber, J.E. &amp;amp; Trumbo, B.E. &lt;a href=&quot;http://www.springerlink.com/index/10.1007/BF02628694&quot;&gt;(Not) giving credit where credit is due: Citation of data sets&lt;/a&gt;. Science and Engineering Ethics 1, 11-20(1995).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stodden, V. &lt;a href=&quot;http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4720221&quot;&gt;The Legal Framework for Reproducible Scientific Research: Licensing and Copyright&lt;/a&gt;. Computing in Science &amp;amp; Engineering 11, 35-40(2009).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gleditsch, N.P. &amp;amp; Strand, H. &lt;a href=&quot;http://www.prio.no/Research-and-Publications/Publication/?oid=55406&quot;&gt;Posting your data: will you be scooped or will you be famous?&lt;/a&gt;International Studies Perspectives 4, 89-97(2003).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;﻿Vision, T.J. &lt;a href=&quot;http://caliber.ucpress.net/doi/abs/10.1525/bio.2010.60.5.2&quot;&gt;Open Data and the Social Contract of Scientific Publishing&lt;/a&gt;. BioScience 60, 330-331(2010).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;﻿Costello, M.J. &lt;a href=&quot;http://caliber.ucpress.net/doi/abs/10.1525/bio.2009.59.5.9&quot;&gt;Motivating Online Publication of Data.&lt;/a&gt; BioScience 59, 418-427(2009).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data citation is closely tied to the discussion of data licenses, such as CC licenses that allow conditional reuse, vs content in the public domain that can be used unconditionally. It seems data is not generally viewed by the law as a creative work but as fact, and thus not under the jurisdiction of intellectual property and copyright licenses. Acknowledging this, repositories such as Dryad identify data as Creative Commons zero, which is essentially an intentional decision to put data into the public domain. See &lt;a href=&quot;http://friendfeed.com/opensci-info/3dfde7f6/where-do-you-point-to-for-explanation-of-why-data&quot;&gt;more discussion&lt;/a&gt; of when this applies and when data can be cc-by, etc, and &lt;a href=&quot;http://sciencecommons.org/weblog/archives/2009/11/19/remembering-babel-open-data-sharing-integration/&quot;&gt;Wilbanks’ explanation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;following-the-developments-sources&quot;&gt;Following the Developments; Sources&lt;/h2&gt;
&lt;p&gt;The Data Citation collection on Mendeley provides an actively maintained collection of articles discussing practices for depositing, citing, incentives, and tools for data management and publication.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.mendeley.com/groups/544621/data-citation/&quot;&gt;Visit the group list&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.mendeley.com/groups/544621/data-citation/feed/rss&quot;&gt;Follow the RSS&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussion continues on a few FF forum threads have helped assemble this information:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://friendfeed.com/science-2-0/8b143647/who-pays-for-archiving-costs-to-comply-with-new&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;http://friendfeed.com/science-2-0/98efbfa5/where-best-place-to-start-learn-about-new-nsf&quot;&gt;2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dryad maintains an &lt;a href=&quot;http://datadryad.org/pages/jdap&quot;&gt;excellent list&lt;/a&gt; of the growing number of Ecology and Evolution partner journals that mandate archiving &lt;em&gt; &lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
